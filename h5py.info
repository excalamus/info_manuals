This is h5py.info, produced by makeinfo version 6.5 from h5py.texi.

     h5py 3.1.0, Dec 04, 2020

     Andrew Collette and contributors

     Copyright © 2014, Andrew Collette and contributors

INFO-DIR-SECTION Miscellaneous
START-INFO-DIR-ENTRY
* h5py: (h5py.info). One line description of project.
END-INFO-DIR-ENTRY


   Generated by Sphinx 2.3.1.


File: h5py.info,  Node: Top,  Next: Where to start,  Up: (dir)

h5py Documentation
******************

     h5py 3.1.0, Dec 04, 2020

     Andrew Collette and contributors

     Copyright © 2014, Andrew Collette and contributors

The h5py package is a Pythonic interface to the HDF5 binary data format.

HDF5(1) lets you store huge amounts of numerical data, and easily
manipulate that data from NumPy.  For example, you can slice into
multi-terabyte datasets stored on disk, as if they were real NumPy
arrays.  Thousands of datasets can be stored in a single file,
categorized and tagged however you want.

* Menu:

* Where to start::
* Other resources::
* Introductory info::
* High-level API reference::
* Advanced topics::
* Meta-info about the h5py project::
* Index::

   ---------- Footnotes ----------

   (1) https://hdfgroup.org


File: h5py.info,  Node: Where to start,  Next: Other resources,  Prev: Top,  Up: Top

1 Where to start
****************

   * *note Quick-start guide: 3.

   * *note Installation: 4.


File: h5py.info,  Node: Other resources,  Next: Introductory info,  Prev: Where to start,  Up: Top

2 Other resources
*****************

   * Python and HDF5 O’Reilly book(1)

   * Ask questions on the mailing list at Google Groups(2)

   * GitHub project(3)

   ---------- Footnotes ----------

   (1) https://shop.oreilly.com/product/0636920030249.do

   (2) http://groups.google.com/d/forum/h5py

   (3) https://github.com/h5py/h5py


File: h5py.info,  Node: Introductory info,  Next: High-level API reference,  Prev: Other resources,  Up: Top

3 Introductory info
*******************

* Menu:

* Quick Start Guide::
* Installation::


File: h5py.info,  Node: Quick Start Guide,  Next: Installation,  Up: Introductory info

3.1 Quick Start Guide
=====================

* Menu:

* Install::
* Core concepts::
* Groups and hierarchical organization::
* Attributes::


File: h5py.info,  Node: Install,  Next: Core concepts,  Up: Quick Start Guide

3.1.1 Install
-------------

With Anaconda(1) or Miniconda(2):

     conda install h5py

If there are wheels for your platform (mac, linux, windows on x86) and
you do not need MPI you can install ‘h5py’ via pip:

     pip install h5py

With Enthought Canopy(3), use the GUI package manager or:

     enpkg h5py

To install from source see *note Installation: 4.

   ---------- Footnotes ----------

   (1) http://continuum.io/downloads

   (2) http://conda.pydata.org/miniconda.html

   (3) https://www.enthought.com/products/canopy/


File: h5py.info,  Node: Core concepts,  Next: Groups and hierarchical organization,  Prev: Install,  Up: Quick Start Guide

3.1.2 Core concepts
-------------------

An HDF5 file is a container for two kinds of objects: ‘datasets’, which
are array-like collections of data, and ‘groups’, which are folder-like
containers that hold datasets and other groups.  The most fundamental
thing to remember when using h5py is:

     `Groups work like dictionaries, and datasets work like NumPy
     arrays'

Suppose someone has sent you a HDF5 file, ‘mytestfile.hdf5’.  (To create
this file, read *note Appendix; Creating a file: b.)  The very first
thing you’ll need to do is to open the file for reading:

     >>> import h5py
     >>> f = h5py.File('mytestfile.hdf5', 'r')

The *note File object: c. is your starting point.  What is stored in
this file?  Remember *note h5py.File: d. acts like a Python dictionary,
thus we can check the keys,

     >>> list(f.keys())
     ['mydataset']

Based on our observation, there is one data set, ‘mydataset’ in the
file.  Let us examine the data set as a *note Dataset: e. object

     >>> dset = f['mydataset']

The object we obtained isn’t an array, but *note an HDF5 dataset: e.
Like NumPy arrays, datasets have both a shape and a data type:

     >>> dset.shape
     (100,)
     >>> dset.dtype
     dtype('int32')

They also support array-style slicing.  This is how you read and write
data from a dataset in the file:

     >>> dset[...] = np.arange(100)
     >>> dset[0]
     0
     >>> dset[10]
     10
     >>> dset[0:100:10]
     array([ 0, 10, 20, 30, 40, 50, 60, 70, 80, 90])

For more, see *note File Objects: c. and *note Datasets: e.

* Menu:

* Appendix; Creating a file: Appendix Creating a file.


File: h5py.info,  Node: Appendix Creating a file,  Up: Core concepts

3.1.2.1 Appendix: Creating a file
.................................

At this point, you may wonder how ‘mytestdata.hdf5’ is created.  We can
create a file by setting the ‘mode’ to ‘w’ when the File object is
initialized.  Some other modes are ‘a’ (for read/write/create access),
and ‘r+’ (for read/write access).  A full list of file access modes and
their meanings is at *note File Objects: c.

     >>> import h5py
     >>> import numpy as np
     >>> f = h5py.File("mytestfile.hdf5", "w")

The *note File object: c. has a couple of methods which look
interesting.  One of them is ‘create_dataset’, which as the name
suggests, creates a data set of given shape and dtype

     >>> dset = f.create_dataset("mydataset", (100,), dtype='i')

The File object is a context manager; so the following code works too

     >>> import h5py
     >>> import numpy as np
     >>> with h5py.File("mytestfile.hdf5", "w") as f:
     >>>     dset = f.create_dataset("mydataset", (100,), dtype='i')


File: h5py.info,  Node: Groups and hierarchical organization,  Next: Attributes,  Prev: Core concepts,  Up: Quick Start Guide

3.1.3 Groups and hierarchical organization
------------------------------------------

“HDF” stands for “Hierarchical Data Format”.  Every object in an HDF5
file has a name, and they’re arranged in a POSIX-style hierarchy with
‘/’-separators:

     >>> dset.name
     '/mydataset'

The “folders” in this system are called *note groups: 10.  The ‘File’
object we created is itself a group, in this case the ‘root group’,
named ‘/’:

     >>> f.name
     '/'

Creating a subgroup is accomplished via the aptly-named ‘create_group’.
But we need to open the file in the “append” mode first (Read/write if
exists, create otherwise)

     >>> f = h5py.File('mydataset.hdf5', 'a')
     >>> grp = f.create_group("subgroup")

All ‘Group’ objects also have the ‘create_*’ methods like File:

     >>> dset2 = grp.create_dataset("another_dataset", (50,), dtype='f')
     >>> dset2.name
     '/subgroup/another_dataset'

By the way, you don’t have to create all the intermediate groups
manually.  Specifying a full path works just fine:

     >>> dset3 = f.create_dataset('subgroup2/dataset_three', (10,), dtype='i')
     >>> dset3.name
     '/subgroup2/dataset_three'

Groups support most of the Python dictionary-style interface.  You
retrieve objects in the file using the item-retrieval syntax:

     >>> dataset_three = f['subgroup2/dataset_three']

Iterating over a group provides the names of its members:

     >>> for name in f:
     ...     print(name)
     mydataset
     subgroup
     subgroup2

Membership testing also uses names:

     >>> "mydataset" in f
     True
     >>> "somethingelse" in f
     False

You can even use full path names:

     >>> "subgroup/another_dataset" in f
     True

There are also the familiar ‘keys()’, ‘values()’, ‘items()’ and ‘iter()’
methods, as well as ‘get()’.

Since iterating over a group only yields its directly-attached members,
iterating over an entire file is accomplished with the ‘Group’ methods
‘visit()’ and ‘visititems()’, which take a callable:

     >>> def printname(name):
     ...     print(name)
     >>> f.visit(printname)
     mydataset
     subgroup
     subgroup/another_dataset
     subgroup2
     subgroup2/dataset_three

For more, see *note Groups: 10.


File: h5py.info,  Node: Attributes,  Prev: Groups and hierarchical organization,  Up: Quick Start Guide

3.1.4 Attributes
----------------

One of the best features of HDF5 is that you can store metadata right
next to the data it describes.  All groups and datasets support attached
named bits of data called ‘attributes’.

Attributes are accessed through the ‘attrs’ proxy object, which again
implements the dictionary interface:

     >>> dset.attrs['temperature'] = 99.5
     >>> dset.attrs['temperature']
     99.5
     >>> 'temperature' in dset.attrs
     True

For more, see *note Attributes: 12.


File: h5py.info,  Node: Installation,  Prev: Quick Start Guide,  Up: Introductory info

3.2 Installation
================

It is highly recommended that you use a pre-built version of h5py,
either from a Python Distribution, an OS-specific package manager, or a
pre-built wheel from PyPI.

Be aware however that most pre-built versions lack MPI support, and that
they are built against a specific version of HDF5.  If you require MPI
support, or newer HDF5 features, you will need to build from source.

After installing h5py, you should run the tests to be sure that
everything was installed correctly.  This can be done in the python
interpreter via:

     import h5py
     h5py.run_tests()

* Menu:

* Pre-built installation (recommended): Pre-built installation recommended.
* Source installation::
* Custom installation::
* Building against Parallel HDF5::


File: h5py.info,  Node: Pre-built installation recommended,  Next: Source installation,  Up: Installation

3.2.1 Pre-built installation (recommended)
------------------------------------------

Pre-build h5py can be installed via many Python Distributions,
OS-specific package managers, or via h5py wheels.

* Menu:

* Python Distributions::
* Wheels::
* OS-Specific Package Managers::


File: h5py.info,  Node: Python Distributions,  Next: Wheels,  Up: Pre-built installation recommended

3.2.1.1 Python Distributions
............................

If you do not already use a Python Distribution, we recommend either
Anaconda(1)/Miniconda(2) or Enthought Canopy(3), both of which support
most versions of Microsoft Windows, OSX/MacOS, and a variety of Linux
Distributions.  Installation of h5py can be done on the command line
via:

     $ conda install h5py

for Anaconda/MiniConda, and via:

     $ enpkg h5py

for Canopy.

   ---------- Footnotes ----------

   (1) http://continuum.io/downloads

   (2) http://conda.pydata.org/miniconda.html

   (3) https://www.enthought.com/products/canopy/


File: h5py.info,  Node: Wheels,  Next: OS-Specific Package Managers,  Prev: Python Distributions,  Up: Pre-built installation recommended

3.2.1.2 Wheels
..............

If you have an existing Python installation (e.g.  a python.org
download, or one that comes with your OS), then on Windows, MacOS/OSX,
and Linux on Intel computers, pre-built h5py wheels can be installed via
pip from PyPI:

     $ pip install h5py

Additionally, for Windows users, Chris Gohlke provides third-party
wheels which use Intel’s MKL(1).

   ---------- Footnotes ----------

   (1) http://www.lfd.uci.edu/~gohlke/pythonlibs/


File: h5py.info,  Node: OS-Specific Package Managers,  Prev: Wheels,  Up: Pre-built installation recommended

3.2.1.3 OS-Specific Package Managers
....................................

On OSX/MacOS, h5py can be installed via Homebrew(1), Macports(2), or
Fink(3).

The current state of h5py in various Linux Distributions can be seen at
‘https://pkgs.org/download/python-h5py’, and can be installed via the
package manager.

As far as the h5py developers know, none of the Windows package managers
(e.g.  Chocolatey(4), nuget(5)) have h5py included, however they may
assist in installing h5py’s requirements when building from source.

   ---------- Footnotes ----------

   (1) https://brew.sh/

   (2) https://www.macports.org/

   (3) http://finkproject.org/

   (4) https://chocolatey.org/

   (5) https://www.nuget.org/


File: h5py.info,  Node: Source installation,  Next: Custom installation,  Prev: Pre-built installation recommended,  Up: Installation

3.2.2 Source installation
-------------------------

To install h5py from source, you need three things installed:

   * A supported Python version with development headers

   * Cython >=0.29

   * HDF5 1.8.4 or newer with development headers

   * A C compiler

On Unix platforms, you also need ‘pkg-config’ unless you explicitly
specify a path for HDF5 as described in *note Custom installation: 1d.

OS-specific instructions for installing HDF5, Python and a C compiler
are in the next few sections.

Additional Python-level requirements should be installed automatically
(which will require an internet connection).

The actual installation of h5py should be done via:

     $ pip install --no-binary=h5py h5py

or, from a tarball or git *note checkout: 1e.

     $ pip install -v .

If you are working on a development version and the underlying cython
files change it may be necessary to force a full rebuild.  The easiest
way to achieve this is

     $ git clean -xfd

from the top of your clone and then rebuilding.

* Menu:

* Source installation on OSX/MacOS::
* Source installation on Linux/Other Unix::
* Source installation on Windows::


File: h5py.info,  Node: Source installation on OSX/MacOS,  Next: Source installation on Linux/Other Unix,  Up: Source installation

3.2.2.1 Source installation on OSX/MacOS
........................................

HDF5 and Python are most likely in your package manager (e.g.
Homebrew(1), Macports(2), or Fink(3)).  Be sure to install the
development headers, as sometimes they are not included in the main
package.

XCode comes with a C compiler (clang), and your package manager will
likely have other C compilers for you to install.

   ---------- Footnotes ----------

   (1) https://brew.sh/

   (2) https://www.macports.org/

   (3) http://finkproject.org/


File: h5py.info,  Node: Source installation on Linux/Other Unix,  Next: Source installation on Windows,  Prev: Source installation on OSX/MacOS,  Up: Source installation

3.2.2.2 Source installation on Linux/Other Unix
...............................................

HDF5 and Python are most likely in your package manager.  A C compiler
almost definitely is, usually there is some kind of metapackage to
install the default build tools, e.g.  ‘build-essential’, which should
be sufficient for our needs.  Make sure that that you have the
development headers, as they are usually not installed by default.  They
can usually be found in ‘python-dev’ or similar and ‘libhdf5-dev’ or
similar.


File: h5py.info,  Node: Source installation on Windows,  Prev: Source installation on Linux/Other Unix,  Up: Source installation

3.2.2.3 Source installation on Windows
......................................

Installing from source on Windows is a much more difficult prospect than
installing from source on other OSs, as not only are you likely to need
to compile HDF5 from source, everything must be built with the correct
version of Visual Studio.  Additional patches are also needed to HDF5 to
get HDF5 and Python to work together.

We recommend examining the appveyor build scripts, and using those to
build and install HDF5 and h5py.


File: h5py.info,  Node: Custom installation,  Next: Building against Parallel HDF5,  Prev: Source installation,  Up: Installation

3.2.3 Custom installation
-------------------------

     Important: Remember that pip installs wheels by default.  To
     perform a custom installation with pip, you should use:

          $ pip install --no-binary=h5py h5py

     or build from a git checkout or downloaded tarball to avoid getting
     a pre-built version of h5py.

You can specify build options for h5py as environment variables when you
build it from source:

     $ HDF5_DIR=/path/to/hdf5 pip install --no-binary=h5py h5py
     $ HDF5_VERSION=X.Y.Z pip install --no-binary=h5py h5py
     $ CC="mpicc" HDF5_MPI="ON" HDF5_DIR=/path/to/parallel-hdf5 pip install --no-binary=h5py h5py

The supported build options are:

   - To specify where to find HDF5, use one of these options:

        - ‘HDF5_LIBDIR’ and ‘HDF5_INCLUDEDIR’: the directory containing
          the compiled HDF5 libraries and the directory containing the C
          header files, respectively.

        - ‘HDF5_DIR’: a shortcut for common installations, a directory
          with ‘lib’ and ‘include’ subdirectories containing compiled
          libraries and C headers.

        - ‘HDF5_PKGCONFIG_NAME’: A name to query ‘pkg-config’ for.  If
          none of these options are specified, h5py will query
          ‘pkg-config’ by default for ‘hdf5’, or ‘hdf5-openmpi’ if
          building with MPI support.

   - ‘HDF5_MPI=ON’ to build with MPI integration - see *note Building
     against Parallel HDF5: 23.

   - ‘HDF5_VERSION’ to force a specified HDF5 version.  In most cases,
     you don’t need to set this; the version number will be detected
     from the HDF5 library.

   - ‘H5PY_SYSTEM_LZF=1’ to build the bundled LZF compression filter
     (see *note Filter pipeline: 24.) against an external LZF library,
     rather than using the bundled LZF C code.


File: h5py.info,  Node: Building against Parallel HDF5,  Prev: Custom installation,  Up: Installation

3.2.4 Building against Parallel HDF5
------------------------------------

If you just want to build with ‘mpicc’, and don’t care about using
Parallel HDF5 features in h5py itself:

     $ export CC=mpicc
     $ pip install --no-binary=h5py h5py

If you want access to the full Parallel HDF5 feature set in h5py (*note
Parallel HDF5: 26.), you will further have to build in MPI mode.  This
can be done by setting the ‘HDF5_MPI’ environment variable:

     $ export CC=mpicc
     $ export HDF5_MPI="ON"
     $ pip install --no-binary=h5py h5py

You will need a shared-library build of Parallel HDF5 as well, i.e.
built with ‘./configure --enable-shared --enable-parallel’.


File: h5py.info,  Node: High-level API reference,  Next: Advanced topics,  Prev: Introductory info,  Up: Top

4 High-level API reference
**************************

* Menu:

* File Objects::
* Groups::
* Datasets::
* Attributes: Attributes<2>.
* Dimension Scales::
* Low-Level API::


File: h5py.info,  Node: File Objects,  Next: Groups,  Up: High-level API reference

4.1 File Objects
================

File objects serve as your entry point into the world of HDF5.  In
addition to the File-specific capabilities listed here, every File
instance is also an *note HDF5 group: 10. representing the ‘root group’
of the file.

* Menu:

* Opening & creating files::
* File drivers::
* Python file-like objects::
* Version bounding::
* Closing files::
* User block::
* Filenames on different systems::
* Chunk cache::
* Reference::


File: h5py.info,  Node: Opening & creating files,  Next: File drivers,  Up: File Objects

4.1.1 Opening & creating files
------------------------------

HDF5 files work generally like standard Python file objects.  They
support standard modes like r/w/a, and should be closed when they are no
longer in use.  However, there is obviously no concept of “text” vs
“binary” mode.

     >>> f = h5py.File('myfile.hdf5','r')

The file name may be a byte string or unicode string.  Valid modes are:

     r            Readonly, file must exist (default)
                  
                  
     r+           Read/write, file must exist
                  
                  
     w            Create file, truncate if exists
                  
                  
     w- or x      Create file, fail if exists
                  
                  
     a            Read/write if exists, create otherwise
                  

Changed in version 3.0: Files are now opened read-only by default.
Earlier versions of h5py would pick different modes depending on the
presence and permissions of the file.


File: h5py.info,  Node: File drivers,  Next: Python file-like objects,  Prev: Opening & creating files,  Up: File Objects

4.1.2 File drivers
------------------

HDF5 ships with a variety of different low-level drivers, which map the
logical HDF5 address space to different storage mechanisms.  You can
specify which driver you want to use when the file is opened:

     >>> f = h5py.File('myfile.hdf5', driver=<driver name>, <driver_kwds>)

For example, the HDF5 “core” driver can be used to create a purely
in-memory HDF5 file, optionally written out to disk when it is closed.
Here’s a list of supported drivers and their options:

     None

          `Strongly recommended.'  Use the standard HDF5 driver
          appropriate for the current platform.  On UNIX, this is the
          H5FD_SEC2 driver; on Windows, it is H5FD_WINDOWS.

     ‘sec2’

          Unbuffered, optimized I/O using standard POSIX functions.

     ‘stdio’

          Buffered I/O using functions from stdio.h.

     ‘core’

          Store and manipulate the data in memory, and optionally write
          it back out when the file is closed.  Using this with an
          existing file and a reading mode will read the entire file
          into memory.  Keywords:

          backing_store:

               If True (default), save changes to the real file at the
               specified path on *note close(): 2e. or *note flush():
               2f.  If False, any changes are discarded when the file is
               closed.

          block_size:

               Increment (in bytes) by which memory is extended.
               Default is 64k.

     ‘family’

          Store the file on disk as a series of fixed-length chunks.
          Useful if the file system doesn’t allow large files.  Note:
          the filename you provide `must' contain a printf-style integer
          format code (e.g.  %d”), which will be replaced by the file
          sequence number.  Keywords:

          memb_size: Maximum file size (default is 2**31-1).

     ‘fileobj’

          Store the data in a Python file-like object; see below.  This
          is the default if a file-like object is passed to *note File:
          d.

     ‘split’

          Splits the meta data and raw data into separate files.
          Keywords:

          meta_ext:

               Metadata filename extension.  Default is ‘-m.h5’.

          raw_ext:

               Raw data filename extension.  Default is ‘-r.h5’.


File: h5py.info,  Node: Python file-like objects,  Next: Version bounding,  Prev: File drivers,  Up: File Objects

4.1.3 Python file-like objects
------------------------------

New in version 2.9.

The first argument to *note File: d. may be a Python file-like object,
such as an ‘io.BytesIO’ or ‘tempfile.TemporaryFile’ instance.  This is a
convenient way to create temporary HDF5 files, e.g.  for testing or to
send over the network.

The file-like object must be open for binary I/O, and must have these
methods: ‘read()’ (or ‘readinto()’), ‘write()’, ‘seek()’, ‘tell()’,
‘truncate()’ and ‘flush()’.

     >>> tf = tempfile.TemporaryFile()
     >>> f = h5py.File(tf, 'w')

Accessing the *note File: d. instance after the underlying file object
has been closed will result in undefined behaviour.

When using an in-memory object such as ‘io.BytesIO’, the data written
will take up space in memory.  If you want to write large amounts of
data, a better option may be to store temporary data on disk using the
functions in ‘tempfile’.

     """Create an HDF5 file in memory and retrieve the raw bytes

     This could be used, for instance, in a server producing small HDF5
     files on demand.
     """
     import io
     import h5py

     bio = io.BytesIO()
     with h5py.File(bio, 'w') as f:
         f['dataset'] = range(10)

     data = bio.getvalue() # data is a regular Python bytes object.
     print("Total size:", len(data))
     print("First bytes:", data[:10])

     Warning: When using a Python file-like object for an HDF5 file,
     make sure to close the HDF5 file before closing the file object
     it’s wrapping.  If there is an error while trying to close the HDF5
     file, segfaults may occur.

     Note: Using a Python file-like object for HDF5 is internally more
     complex, as the HDF5 C code calls back into Python to access it.
     It inevitably has more ways to go wrong, and the failures may be
     less clear when it does.  For some common use cases, you can easily
     avoid it:

        - To create a file in memory and never write it to disk, use the
          ‘'core'’ driver with ‘mode='w', backing_store=False’ (see
          *note File drivers: 2c.).

        - To use a temporary file securely, make a temporary directory
          and *note open a file path: 2a. inside it.


File: h5py.info,  Node: Version bounding,  Next: Closing files,  Prev: Python file-like objects,  Up: File Objects

4.1.4 Version bounding
----------------------

HDF5 has been evolving for many years now.  By default, the library will
write objects in the most compatible fashion possible, so that older
versions will still be able to read files generated by modern programs.
However, there can be feature or performance advantages if you are
willing to forgo a certain level of backwards compatibility.  By using
the “libver” option to *note File: d, you can specify the minimum and
maximum sophistication of these structures:

     >>> f = h5py.File('name.hdf5', libver='earliest') # most compatible
     >>> f = h5py.File('name.hdf5', libver='latest')   # most modern

Here “latest” means that HDF5 will always use the newest version of
these structures without particular concern for backwards compatibility.
The “earliest” option means that HDF5 will make a `best effort' to be
backwards compatible.

The default is “earliest”.

Specifying version bounds has changed from HDF5 version 1.10.2.  There
are two new compatibility levels: ‘v108’ (for HDF5 1.8) and ‘v110’ (for
HDF5 1.10).  This change enables, for example, something like this:

     >>> f = h5py.File('name.hdf5', libver=('earliest', 'v108'))

which enforces full backward compatibility up to HDF5 1.8.  Using any
HDF5 feature that requires a newer format will raise an error.

‘latest’ is now an alias to another bound label that represents the
latest version.  Because of this, the ‘File.libver’ property will not
use ‘latest’ in its output for HDF5 1.10.2 or later.


File: h5py.info,  Node: Closing files,  Next: User block,  Prev: Version bounding,  Up: File Objects

4.1.5 Closing files
-------------------

If you call *note File.close(): 2e, or leave a ‘with h5py.File(...)’
block, the file will be closed and any objects (such as groups or
datasets) you have from that file will become unusable.  This is
equivalent to what HDF5 calls ‘strong’ closing.

If a file object goes out of scope in your Python code, the file will
only be closed when there are no remaining objects belonging to it.
This is what HDF5 calls ‘weak’ closing.

     with h5py.File('f1.h5', 'r') as f1:
         ds = f1['dataset']

     # ERROR - can't access dataset, because f1 is closed:
     ds[0]

     def get_dataset():
         f2 = h5py.File('f2.h5', 'r')
         return f2['dataset']
     ds = get_dataset()

     # OK - f2 is out of scope, but the dataset reference keeps it open:
     ds[0]

     del ds  # Now f2.h5 will be closed


File: h5py.info,  Node: User block,  Next: Filenames on different systems,  Prev: Closing files,  Up: File Objects

4.1.6 User block
----------------

HDF5 allows the user to insert arbitrary data at the beginning of the
file, in a reserved space called the ‘user block’.  The length of the
user block must be specified when the file is created.  It can be either
zero (the default) or a power of two greater than or equal to 512.  You
can specify the size of the user block when creating a new file, via the
‘userblock_size’ keyword to File; the userblock size of an open file can
likewise be queried through the ‘File.userblock_size’ property.

Modifying the user block on an open file is not supported; this is a
limitation of the HDF5 library.  However, once the file is closed you
are free to read and write data at the start of the file, provided your
modifications don’t leave the user block region.


File: h5py.info,  Node: Filenames on different systems,  Next: Chunk cache,  Prev: User block,  Up: File Objects

4.1.7 Filenames on different systems
------------------------------------

Different operating systems (and different file systems) store filenames
with different encodings.  Additionally, in Python there are at least
two different representations of filenames, as encoded bytes (via str on
Python 2, bytes on Python 3) or as a unicode string (via unicode on
Python 2 and str on Python 3).  The safest bet when creating a new file
is to use unicode strings on all systems.

* Menu:

* macOS (OSX): macOS OSX.
* Linux (and non-macOS Unix): Linux and non-macOS Unix.
* Windows::


File: h5py.info,  Node: macOS OSX,  Next: Linux and non-macOS Unix,  Up: Filenames on different systems

4.1.7.1 macOS (OSX)
...................

macOS is the simplest system to deal with, it only accepts UTF-8, so
using unicode paths will just work (and should be preferred).


File: h5py.info,  Node: Linux and non-macOS Unix,  Next: Windows,  Prev: macOS OSX,  Up: Filenames on different systems

4.1.7.2 Linux (and non-macOS Unix)
..................................

Unix-like systems use locale settings to determine the correct encoding
to use.  These are set via a number of different environment variables,
of which ‘LANG’ and ‘LC_ALL’ are the ones of most interest.  Of special
interest is the ‘C’ locale, which Python will interpret as only allowing
ASCII, meaning unicode paths should be pre-encoded.  This will likely
change in Python 3.7 with ‘https://www.python.org/dev/peps/pep-0538/’,
but this will likely be backported by distributions to earlier versions.

To summarise, use unicode strings where possible, but be aware that
sometimes using encoded bytes may be necessary to read incorrectly
encoded filenames.


File: h5py.info,  Node: Windows,  Prev: Linux and non-macOS Unix,  Up: Filenames on different systems

4.1.7.3 Windows
...............

Windows systems have two different APIs to perform file-related
operations, a ANSI (char, legacy) interface and a unicode (wchar)
interface.  HDF5 currently only supports the ANSI interface, which is
limited in what it can encode.  This means that it may not be possible
to open certain files, and because *note External links: 3d. do not
specify their encoding, it is possible that opening an external link may
not work.  There is work being done to fix this (see
‘https://github.com/h5py/h5py/issues/839’), but it is likely there will
need to be breaking changes make to allow Windows to have the same level
of support for unicode filenames as other operating systems.

The best suggestion is to use unicode strings, but to keep to ASCII for
filenames to avoid possible breakage.


File: h5py.info,  Node: Chunk cache,  Next: Reference,  Prev: Filenames on different systems,  Up: File Objects

4.1.8 Chunk cache
-----------------

*note Chunked storage: 40. allows datasets to be stored on disk in
separate pieces.  When a part of any one of these pieces is needed, the
entire chunk is read into memory before the requested part is copied to
the user’s buffer.  To the extent possible those chunks are cached in
memory, so that if the user requests a different part of a chunk that
has already been read, the data can be copied directly from memory
rather than reading the file again.  The details of a given dataset’s
chunks are controlled when creating the dataset, but it is possible to
adjust the behavior of the chunk `cache' when opening the file.

The parameters controlling this behavior are prefixed by ‘rdcc’, for
`raw data chunk cache'.

   * ‘rdcc_nbytes’ sets the total size (measured in bytes) of the raw
     data chunk cache for each dataset.  The default size is 1 MB. This
     should be set to the size of each chunk times the number of chunks
     that are likely to be needed in cache.

   * ‘rdcc_w0’ sets the policy for chunks to be removed from the cache
     when more space is needed.  If the value is set to 0, then the
     library will always evict the least recently used chunk in cache.
     If the value is set to 1, the library will always evict the least
     recently used chunk which has been fully read or written, and if
     none have been fully read or written, it will evict the least
     recently used chunk.  If the value is between 0 and 1, the behavior
     will be a blend of the two.  Therefore, if the application will
     access the same data more than once, the value should be set closer
     to 0, and if the application does not, the value should be set
     closer to 1.

   * ‘rdcc_nslots’ is the number of chunk slots in the cache for this
     entire file.  In order to allow the chunks to be looked up quickly
     in cache, each chunk is assigned a unique hash value that is used
     to look up the chunk.  The cache contains a simple array of
     pointers to chunks, which is called a hash table.  A chunk’s hash
     value is simply the index into the hash table of the pointer to
     that chunk.  While the pointer at this location might instead point
     to a different chunk or to nothing at all, no other locations in
     the hash table can contain a pointer to the chunk in question.
     Therefore, the library only has to check this one location in the
     hash table to tell if a chunk is in cache or not.  This also means
     that if two or more chunks share the same hash value, then only one
     of those chunks can be in the cache at the same time.  When a chunk
     is brought into cache and another chunk with the same hash value is
     already in cache, the second chunk must be evicted first.
     Therefore it is very important to make sure that the size of the
     hash table (which is determined by the ‘rdcc_nslots’ parameter) is
     large enough to minimize the number of hash value collisions.  Due
     to the hashing strategy, this value should ideally be a prime
     number.  As a rule of thumb, this value should be at least 10 times
     the number of chunks that can fit in ‘rdcc_nbytes’ bytes.  For
     maximum performance, this value should be set approximately 100
     times that number of chunks.  The default value is 521.

Chunks and caching are described in greater detail in the HDF5
documentation(1).

   ---------- Footnotes ----------

   (1) https://portal.hdfgroup.org/display/HDF5/Chunking+in+HDF5


File: h5py.info,  Node: Reference,  Prev: Chunk cache,  Up: File Objects

4.1.9 Reference
---------------

     Note: Unlike Python file objects, the attribute ‘File.name’ gives
     the HDF5 name of the root group, “‘/’”.  To access the on-disk
     name, use *note File.filename: 42.

 -- Class: h5py.File (name, mode=None, driver=None, libver=None,
          userblock_size=None, swmr=False, rdcc_nslots=None,
          rdcc_nbytes=None, rdcc_w0=None, track_order=None,
          fs_strategy=None, fs_persist=False, fs_threshold=1, **kwds)

     Open or create a new file.

     Note that in addition to the File-specific methods and properties
     listed below, File objects inherit the full interface of *note
     Group: 43.


     Parameters:

        * ‘name’ – Name of file (‘bytes’ or ‘str’), or an instance of
          ‘h5f.FileID’ to bind to an existing file identifier, or a
          file-like object (see *note Python file-like objects: 30.).

        * ‘mode’ – Mode in which to open file; one of (“w”, “r”, “r+”,
          “a”, “w-“).  See *note Opening & creating files: 2a.

        * ‘driver’ – File driver to use; see *note File drivers: 2c.

        * ‘libver’ – Compatibility bounds; see *note Version bounding:
          32.

        * ‘userblock_size’ – Size (in bytes) of the user block.  If
          nonzero, must be a power of 2 and at least 512.  See *note
          User block: 36.

        * ‘swmr’ – If ‘True’ open the file in
          single-writer-multiple-reader mode.  Only used when mode=”r”.

        * ‘rdcc_nbytes’ – Total size of the raw data chunk cache in
          bytes.  The default size is 1024^2 (1 MiB) per dataset.

        * ‘rdcc_w0’ – Chunk preemption policy for all datasets.  Default
          value is 0.75.

        * ‘rdcc_nslots’ – Number of chunk slots in the raw data chunk
          cache for this file.  Default value is 521.

        * ‘track_order’ – Track dataset/group/attribute creation order
          under root group if ‘True’.  Default is
          ‘h5.get_config().track_order’.

        * ‘fs_strategy’ – The file space handling strategy to be used.
          Only allowed when creating a new file.  One of “fsm”, “page”,
          “aggregate”, “none”, or None (to use the HDF5 default).

        * ‘fs_persist’ – A boolean to indicate whether free space should
          be persistent or not.  Only allowed when creating a new file.
          The default is False.

        * ‘fs_threshold’ – The smallest free-space section size that the
          free space manager will track.  Only allowed when creating a
          new file.  The default is 1.

        * ‘kwds’ – Driver-specific keywords; see *note File drivers: 2c.

      -- Method: __bool__ ()

          Check that the file descriptor is valid and the file open:

               >>> f = h5py.File(filename)
               >>> f.close()
               >>> if f:
               ...     print("file is open")
               ... else:
               ...     print("file is closed")
               file is closed

      -- Method: close ()

          Close this file.  All open objects will become invalid.

      -- Method: flush ()

          Request that the HDF5 library flush its buffers to disk.

      -- Attribute: id

          Low-level identifier (an instance of FileID(1)).

      -- Attribute: filename

          Name of this file on disk.  Generally a Unicode string; a byte
          string will be used if HDF5 returns a non-UTF-8 encoded
          string.

      -- Attribute: mode

          String indicating if the file is open readonly (“r”) or
          read-write (“r+”).  Will always be one of these two values,
          regardless of the mode used to open the file.

      -- Attribute: swmr_mode

          True if the file access is using *note Single Writer Multiple
          Reader (SWMR): 48.  Use *note mode: 46. to distinguish SWMR
          read from write.

      -- Attribute: driver

          String giving the driver used to open the file.  Refer to
          *note File drivers: 2c. for a list of drivers.

      -- Attribute: libver

          2-tuple with library version settings.  See *note Version
          bounding: 32.

      -- Attribute: userblock_size

          Size of user block (in bytes).  Generally 0.  See *note User
          block: 36.

   ---------- Footnotes ----------

   (1) https://api.h5py.org/h5f.html#h5py.h5f.FileID


File: h5py.info,  Node: Groups,  Next: Datasets,  Prev: File Objects,  Up: High-level API reference

4.2 Groups
==========

Groups are the container mechanism by which HDF5 files are organized.
From a Python perspective, they operate somewhat like dictionaries.  In
this case the “keys” are the names of group members, and the “values”
are the members themselves (*note Group: 43. and *note Dataset: 4e.)
objects.

Group objects also contain most of the machinery which makes HDF5
useful.  The *note File object: c. does double duty as the HDF5 `root
group', and serves as your entry point into the file:

     >>> f = h5py.File('foo.hdf5','w')
     >>> f.name
     '/'
     >>> list(f.keys())
     []

Names of all objects in the file are all text strings (‘str’).  These
will be encoded with the HDF5-approved UTF-8 encoding before being
passed to the HDF5 C library.  Objects may also be retrieved using byte
strings, which will be passed on to HDF5 as-is.

* Menu:

* Creating groups::
* Dict interface and links::
* Reference: Reference<2>.
* Link classes::


File: h5py.info,  Node: Creating groups,  Next: Dict interface and links,  Up: Groups

4.2.1 Creating groups
---------------------

New groups are easy to create:

     >>> grp = f.create_group("bar")
     >>> grp.name
     '/bar'
     >>> subgrp = grp.create_group("baz")
     >>> subgrp.name
     '/bar/baz'

Multiple intermediate groups can also be created implicitly:

     >>> grp2 = f.create_group("/some/long/path")
     >>> grp2.name
     '/some/long/path'
     >>> grp3 = f['/some/long']
     >>> grp3.name
     '/some/long'


File: h5py.info,  Node: Dict interface and links,  Next: Reference<2>,  Prev: Creating groups,  Up: Groups

4.2.2 Dict interface and links
------------------------------

Groups implement a subset of the Python dictionary convention.  They
have methods like ‘keys()’, ‘values()’ and support iteration.  Most
importantly, they support the indexing syntax, and standard exceptions:

     >>> myds = subgrp["MyDS"]
     >>> missing = subgrp["missing"]
     KeyError: "Name doesn't exist (Symbol table: Object not found)"

Objects can be deleted from the file using the standard syntax:

     >>> del subgroup["MyDataset"]

     Note: When using h5py from Python 3, the keys(), values() and
     items() methods will return view-like objects instead of lists.
     These objects support membership testing and iteration, but can’t
     be sliced like lists.

By default, objects inside group are iterated in alphanumeric order.
However, if group is created with ‘track_order=True’, the insertion
order for the group is remembered (tracked) in HDF5 file, and group
contents are iterated in that order.  The latter is consistent with
Python 3.7+ dictionaries.

The default ‘track_order’ for all new groups can be specified globally
with ‘h5.get_config().track_order’.

* Menu:

* Hard links::
* Soft links::
* External links::


File: h5py.info,  Node: Hard links,  Next: Soft links,  Up: Dict interface and links

4.2.2.1 Hard links
..................

What happens when assigning an object to a name in the group?  It
depends on the type of object being assigned.  For NumPy arrays or other
data, the default is to create an *note HDF5 datasets: e.:

     >>> grp["name"] = 42
     >>> out = grp["name"]
     >>> out
     <HDF5 dataset "name": shape (), type "<i8">

When the object being stored is an existing Group or Dataset, a new link
is made to the object:

     >>> grp["other name"] = out
     >>> grp["other name"]
     <HDF5 dataset "other name": shape (), type "<i8">

Note that this is ‘not’ a copy of the dataset!  Like hard links in a
UNIX file system, objects in an HDF5 file can be stored in multiple
groups:

     >>> f["other name"] == f["name"]
     True


File: h5py.info,  Node: Soft links,  Next: External links,  Prev: Hard links,  Up: Dict interface and links

4.2.2.2 Soft links
..................

Also like a UNIX filesystem, HDF5 groups can contain “soft” or symbolic
links, which contain a text path instead of a pointer to the object
itself.  You can easily create these in h5py by using ‘h5py.SoftLink’:

     >>> myfile = h5py.File('foo.hdf5','w')
     >>> group = myfile.create_group("somegroup")
     >>> myfile["alias"] = h5py.SoftLink('/somegroup')

If the target is removed, they will “dangle”:

     >>> del myfile['somegroup']
     >>> print(myfile['alias'])
     KeyError: 'Component not found (Symbol table: Object not found)'


File: h5py.info,  Node: External links,  Prev: Soft links,  Up: Dict interface and links

4.2.2.3 External links
......................

New in HDF5 1.8, external links are “soft links plus”, which allow you
to specify the name of the file as well as the path to the desired
object.  You can refer to objects in any file you wish.  Use similar
syntax as for soft links:

     >>> myfile = h5py.File('foo.hdf5','w')
     >>> myfile['ext link'] = h5py.ExternalLink("otherfile.hdf5", "/path/to/resource")

When the link is accessed, the file “otherfile.hdf5” is opened, and
object at “/path/to/resource” is returned.

Since the object retrieved is in a different file, its “.file” and
“.parent” properties will refer to objects in that file, `not' the file
in which the link resides.

     Note: Currently, you can’t access an external link if the file it
     points to is already open.  This is related to how HDF5 manages
     file permissions internally.

     Note: How the filename is processed is operating system dependent,
     it is recommended to read *note Filenames on different systems: 38.
     to understand potential limitations on filenames on your operating
     system.  Note especially that Windows is particularly susceptible
     to problems with external links, due to possible encoding errors
     and how filenames are structured.


File: h5py.info,  Node: Reference<2>,  Next: Link classes,  Prev: Dict interface and links,  Up: Groups

4.2.3 Reference
---------------

 -- Class: h5py.Group (identifier)

     Generally Group objects are created by opening objects in the file,
     or by the method *note Group.create_group(): 59.  Call the
     constructor with a GroupID(1) instance to create a new Group bound
     to an existing low-level identifier.

      -- Method: __iter__ ()

          Iterate over the names of objects directly attached to the
          group.  Use *note Group.visit(): 5b. or *note
          Group.visititems(): 5c. for recursive access to group members.

      -- Method: __contains__ (name)

          Dict-like membership testing.  ‘name’ may be a relative or
          absolute path.

      -- Method: __getitem__ (name)

          Retrieve an object.  ‘name’ may be a relative or absolute
          path, or an *note object or region reference: 5f.  See *note
          Dict interface and links: 52.

      -- Method: __setitem__ (name, value)

          Create a new link, or automatically create a dataset.  See
          *note Dict interface and links: 52.

      -- Method: __bool__ ()

          Check that the group is accessible.  A group could be
          inaccessible for several reasons.  For instance, the group, or
          the file it belongs to, may have been closed elsewhere.

               >>> f = h5py.open(filename)
               >>> group = f["MyGroup"]
               >>> f.close()
               >>> if group:
               ...     print("group is accessible")
               ... else:
               ...     print("group is inaccessible")
               group is inaccessible

      -- Method: keys ()

               Get the names of directly attached group members.  Use
               *note Group.visit(): 5b. or *note Group.visititems(): 5c.
               for recursive access to group members.


          Returns: set-like object.

      -- Method: values ()

          Get the objects contained in the group (Group and Dataset
          instances).  Broken soft or external links show up as None.


          Returns: a collection or bag-like object.

      -- Method: items ()

          Get ‘(name, value)’ pairs for object directly attached to this
          group.  Values for broken soft or external links show up as
          None.


          Returns: a set-like object.

      -- Method: get (name, default=None, getclass=False, getlink=False)

          Retrieve an item, or information about an item.  ‘name’ and
          ‘default’ work like the standard Python ‘dict.get’.


          Parameters:

             * ‘name’ – Name of the object to retrieve.  May be a
               relative or absolute path.

             * ‘default’ – If the object isn’t found, return this
               instead.

             * ‘getclass’ – If True, return the class of object instead;
               *note Group: 43. or *note Dataset: 4e.

             * ‘getlink’ – If true, return the type of link via a *note
               HardLink: 66, *note SoftLink: 67. or *note ExternalLink:
               68. instance.  If ‘getclass’ is also True, returns the
               corresponding Link class without instantiating it.

      -- Method: visit (callable)

          Recursively visit all objects in this group and subgroups.
          You supply a callable with the signature:

               callable(name) -> None or return value

          ‘name’ will be the name of the object relative to the current
          group.  Return None to continue visiting until all objects are
          exhausted.  Returning anything else will immediately stop
          visiting and return that value from ‘visit’:

               >>> def find_foo(name):
               ...     """ Find first object with 'foo' anywhere in the name """
               ...     if 'foo' in name:
               ...         return name
               >>> group.visit(find_foo)
               'some/subgroup/foo'

      -- Method: visititems (callable)

          Recursively visit all objects in this group and subgroups.
          Like *note Group.visit(): 5b, except your callable should have
          the signature:

               callable(name, object) -> None or return value

          In this case ‘object’ will be a *note Group: 43. or *note
          Dataset: 4e. instance.

      -- Method: move (source, dest)

          Move an object or link in the file.  If ‘source’ is a hard
          link, this effectively renames the object.  If a soft or
          external link, the link itself is moved.


          Parameters:

             * ‘source’ (‘String’) – Name of object or link to move.

             * ‘dest’ (‘String’) – New location for object or link.

      -- Method: copy (source, dest, name=None, shallow=False,
               expand_soft=False, expand_external=False,
               expand_refs=False, without_attrs=False)

          Copy an object or group.  The source and destination need not
          be in the same file.  If the source is a Group object, by
          default all objects within that group will be copied
          recursively.


          Parameters:

             * ‘source’ – What to copy.  May be a path in the file or a
               Group/Dataset object.

             * ‘dest’ – Where to copy it.  May be a path or Group
               object.

             * ‘name’ – If the destination is a Group object, use this
               for the name of the copied object (default is basename).

             * ‘shallow’ – Only copy immediate members of a group.

             * ‘expand_soft’ – Expand soft links into new objects.

             * ‘expand_external’ – Expand external links into new
               objects.

             * ‘expand_refs’ – Copy objects which are pointed to by
               references.

             * ‘without_attrs’ – Copy object(s) without copying HDF5
               attributes.

      -- Method: create_group (name, track_order=None)

          Create and return a new group in the file.


          Parameters:

             * ‘name’ (‘String’‘ or ’‘None’) – Name of group to create.
               May be an absolute or relative path.  Provide None to
               create an anonymous group, to be linked into the file
               later.

             * ‘track_order’ – Track dataset/group/attribute creation
               order under this group if ‘True’.  Default is
               ‘h5.get_config().track_order’.


          Returns: The new *note Group: 43. object.

      -- Method: require_group (name)

          Open a group in the file, creating it if it doesn’t exist.
          TypeError is raised if a conflicting object already exists.
          Parameters as in *note Group.create_group(): 59.

      -- Method: create_dataset (name, shape=None, dtype=None,
               data=None, **kwds)

          Create a new dataset.  Options are explained in *note Creating
          datasets: 6d.


          Parameters:

             * ‘name’ – Name of dataset to create.  May be an absolute
               or relative path.  Provide None to create an anonymous
               dataset, to be linked into the file later.

             * ‘shape’ – Shape of new dataset (Tuple).

             * ‘dtype’ – Data type for new dataset

             * ‘data’ – Initialize dataset to this (NumPy array).

             * ‘chunks’ – Chunk shape, or True to enable auto-chunking.

             * ‘maxshape’ – Dataset will be resizable up to this shape
               (Tuple).  Automatically enables chunking.  Use None for
               the axes you want to be unlimited.

             * ‘compression’ – Compression strategy.  See *note Filter
               pipeline: 24.

             * ‘compression_opts’ – Parameters for compression filter.

             * ‘scaleoffset’ – See *note Scale-Offset filter: 6e.

             * ‘shuffle’ – Enable shuffle filter (T/`F'). See *note
               Shuffle filter: 6f.

             * ‘fletcher32’ – Enable Fletcher32 checksum (T/`F'). See
               *note Fletcher32 filter: 70.

             * ‘fillvalue’ – This value will be used when reading
               uninitialized parts of the dataset.

             * ‘track_times’ – Enable dataset creation timestamps
               (`T'/F).

             * ‘track_order’ – Track attribute creation order if ‘True’.
               Default is ‘h5.get_config().track_order’.

             * ‘external’ – Store the dataset in one or more external,
               non-HDF5 files.  This should be an iterable (such as a
               list) of tuples of ‘(name, offset, size)’ to store data
               from ‘offset’ to ‘offset + size’ in the named file.  Each
               name must be a str, bytes, or os.PathLike; each offset
               and size, an integer.  The last file in the sequence may
               have size ‘h5py.h5f.UNLIMITED’ to let it grow as needed.
               If only a name is given instead of an iterable of tuples,
               it is equivalent to ‘[(name, 0, h5py.h5f.UNLIMITED)]’.

             * ‘allow_unknown_filter’ – Do not check that the requested
               filter is available for use (T/F). This should only be
               set if you will write any data with ‘write_direct_chunk’,
               compressing the data before passing it to h5py.

      -- Method: require_dataset (name, shape=None, dtype=None,
               exact=None, **kwds)

          Open a dataset, creating it if it doesn’t exist.

          If keyword “exact” is False (default), an existing dataset
          must have the same shape and a conversion-compatible dtype to
          be returned.  If True, the shape and dtype must match exactly.

          Other dataset keywords (see create_dataset) may be provided,
          but are only used if a new dataset is to be created.

          Raises TypeError if an incompatible object already exists, or
          if the shape or dtype don’t match according to the above
          rules.


          Parameters: ‘exact’ – Require shape and type to match exactly
          (T/`F')

      -- Method: create_dataset_like (name, other, **kwds)

          Create a dataset similar to ‘other’, much like numpy’s ‘_like’
          functions.


          Parameters:

             * ‘name’ – Name of the dataset (absolute or relative).
               Provide None to make an anonymous dataset.

             * ‘other’ – The dataset whom the new dataset should mimic.
               All properties, such as shape, dtype, chunking, … will be
               taken from it, but no data or attributes are being
               copied.

          Any dataset keywords (see create_dataset) may be provided,
          including shape and dtype, in which case the provided values
          take precedence over those from ‘other’.

      -- Method: create_virtual_dataset (name, layout, fillvalue=None)

          Create a new virtual dataset in this group.  See *note Virtual
          Datasets (VDS): 74. for more details.


          Parameters:

             * ‘name’ (‘str’) – Name of the dataset (absolute or
               relative).

             * ‘layout’ (*note VirtualLayout: 75.) – Defines what source
               data fills which parts of the virtual dataset.

             * ‘fillvalue’ – The value to use where there is no data.

      -- Attribute: attrs

          *note Attributes: 12. for this group.

      -- Attribute: id

          The groups’s low-level identifier; an instance of GroupID(2).

      -- Attribute: ref

          An HDF5 object reference pointing to this group.  See *note
          Using object references: 79.

      -- Attribute: regionref

          A proxy object allowing you to interrogate region references.
          See *note Using region references: 7b.

      -- Attribute: name

          String giving the full path to this group.

      -- Attribute: file

          *note File: d. instance in which this group resides.

      -- Attribute: parent

          *note Group: 43. instance containing this group.

   ---------- Footnotes ----------

   (1) https://api.h5py.org/h5g.html#h5py.h5g.GroupID

   (2) https://api.h5py.org/h5g.html#h5py.h5g.GroupID


File: h5py.info,  Node: Link classes,  Prev: Reference<2>,  Up: Groups

4.2.4 Link classes
------------------

 -- Class: h5py.HardLink

     Exists only to support *note Group.get(): 65.  Has no state and
     provides no properties or methods.

 -- Class: h5py.SoftLink (path)

     Exists to allow creation of soft links in the file.  See *note Soft
     links: 55.  These only serve as containers for a path; they are not
     related in any way to a particular file.


     Parameters: ‘path’ (‘String’) – Value of the soft link.

      -- Attribute: path

          Value of the soft link

 -- Class: h5py.ExternalLink (filename, path)

     Like *note SoftLink: 67, only they specify a filename in addition
     to a path.  See *note External links: 3d.


     Parameters:

        * ‘filename’ (‘String’) – Name of the file to which the link
          points

        * ‘path’ (‘String’) – Path to the object in the external file.

      -- Attribute: filename

          Name of the external file

      -- Attribute: path

          Path to the object in the external file


File: h5py.info,  Node: Datasets,  Next: Attributes<2>,  Prev: Groups,  Up: High-level API reference

4.3 Datasets
============

Datasets are very similar to NumPy arrays.  They are homogeneous
collections of data elements, with an immutable datatype and
(hyper)rectangular shape.  Unlike NumPy arrays, they support a variety
of transparent storage features such as compression, error-detection,
and chunked I/O.

They are represented in h5py by a thin proxy class which supports
familiar NumPy operations like slicing, along with a variety of
descriptive attributes:

        - `shape' attribute

        - `size' attribute

        - `ndim' attribute

        - `dtype' attribute

        - `nbytes' attribute

h5py supports most NumPy dtypes, and uses the same character codes (e.g.
‘'f'’, ‘'i8'’) and dtype machinery as Numpy(1).  See *note FAQ: 85. for
the list of dtypes h5py supports.

* Menu:

* Creating datasets::
* Reading & writing data::
* Chunked storage::
* Resizable datasets::
* Filter pipeline::
* Multi-Block Selection::
* Fancy indexing::
* Creating and Reading Empty (or Null) datasets and attributes: Creating and Reading Empty or Null datasets and attributes.
* Reference: Reference<3>.

   ---------- Footnotes ----------

   (1) https://docs.scipy.org/doc/numpy/reference/arrays.dtypes.html


File: h5py.info,  Node: Creating datasets,  Next: Reading & writing data,  Up: Datasets

4.3.1 Creating datasets
-----------------------

New datasets are created using either *note Group.create_dataset(): 6c.
or *note Group.require_dataset(): 71.  Existing datasets should be
retrieved using the group indexing syntax (‘dset = group["name"]’).

To initialise a dataset, all you have to do is specify a name, shape,
and optionally the data type (defaults to ‘'f'’):

     >>> dset = f.create_dataset("default", (100,))
     >>> dset = f.create_dataset("ints", (100,), dtype='i8')

     Note: This is not the same as creating an *note Empty dataset: 87.

You may also initialize the dataset to an existing NumPy array by
providing the ‘data’ parameter:

     >>> arr = np.arange(100)
     >>> dset = f.create_dataset("init", data=arr)

Keywords ‘shape’ and ‘dtype’ may be specified along with ‘data’; if so,
they will override ‘data.shape’ and ‘data.dtype’.  It’s required that
(1) the total number of points in ‘shape’ match the total number of
points in ‘data.shape’, and that (2) it’s possible to cast ‘data.dtype’
to the requested ‘dtype’.


File: h5py.info,  Node: Reading & writing data,  Next: Chunked storage,  Prev: Creating datasets,  Up: Datasets

4.3.2 Reading & writing data
----------------------------

HDF5 datasets re-use the NumPy slicing syntax to read and write to the
file.  Slice specifications are translated directly to HDF5 “hyperslab”
selections, and are a fast and efficient way to access data in the file.
The following slicing arguments are recognized:

        * Indices: anything that can be converted to a Python long

        * Slices (i.e.  ‘[:]’ or ‘[0:10]’)

        * Field names, in the case of compound data

        * At most one ‘Ellipsis’ (‘...’) object

        * An empty tuple (‘()’) to retrieve all data or ‘scalar’ data

Here are a few examples (output omitted).

     >>> dset = f.create_dataset("MyDataset", (10,10,10), 'f')
     >>> dset[0,0,0]
     >>> dset[0,2:10,1:9:3]
     >>> dset[:,::2,5]
     >>> dset[0]
     >>> dset[1,5]
     >>> dset[0,...]
     >>> dset[...,6]
     >>> dset[()]

There’s more documentation on what parts of numpy’s *note fancy
indexing: 8a. are available in h5py.

For compound data, it is advised to separate field names from the
numeric slices:

     >>> dset.fields("FieldA")[:10]   # Read a single field
     >>> dset[:10]["FieldA"]          # Read all fields, select in NumPy

It is also possible to mix indexing and field names (‘dset[:10,
"FieldA"]’), but this might be removed in a future version of h5py.

To retrieve the contents of a ‘scalar’ dataset, you can use the same
syntax as in NumPy: ‘result = dset[()]’.  In other words, index into the
dataset using an empty tuple.

For simple slicing, broadcasting is supported:

     >>> dset[0,:,:] = np.arange(10)  # Broadcasts to (10,10)

Broadcasting is implemented using repeated hyperslab selections, and is
safe to use with very large target selections.  It is supported for the
above “simple” (integer, slice and ellipsis) slicing only.

     Warning: Currently h5py does not support nested compound types, see
     GH1197(1) for more information.

* Menu:

* Multiple indexing::
* Length and iteration::

   ---------- Footnotes ----------

   (1) https://github.com/h5py/h5py/issues/1197


File: h5py.info,  Node: Multiple indexing,  Next: Length and iteration,  Up: Reading & writing data

4.3.2.1 Multiple indexing
.........................

Indexing a dataset once loads a numpy array into memory.  If you try to
index it twice to write data, you may be surprised that nothing seems to
have happened:

     >>> f = h5py.File('my_hdf5_file.h5', 'w')
     >>> dset = f.create_dataset("test", (2, 2))
     >>> dset[0][1] = 3.0  # No effect!
     >>> print(dset[0][1])
     0.0

The assignment above only modifies the loaded array.  It’s equivalent to
this:

     >>> new_array = dset[0]
     >>> new_array[1] = 3.0
     >>> print(new_array[1])
     3.0
     >>> print(dset[0][1])
     0.0

To write to the dataset, combine the indexes in a single step:

     >>> dset[0, 1] = 3.0
     >>> print(dset[0, 1])
     3.0


File: h5py.info,  Node: Length and iteration,  Prev: Multiple indexing,  Up: Reading & writing data

4.3.2.2 Length and iteration
............................

As with NumPy arrays, the ‘len()’ of a dataset is the length of the
first axis, and iterating over a dataset iterates over the first axis.
However, modifications to the yielded data are not recorded in the file.
Resizing a dataset while iterating has undefined results.

On 32-bit platforms, ‘len(dataset)’ will fail if the first axis is
bigger than 2**32.  It’s recommended to use *note Dataset.len(): 8e. for
large datasets.


File: h5py.info,  Node: Chunked storage,  Next: Resizable datasets,  Prev: Reading & writing data,  Up: Datasets

4.3.3 Chunked storage
---------------------

An HDF5 dataset created with the default settings will be ‘contiguous’;
in other words, laid out on disk in traditional C order.  Datasets may
also be created using HDF5’s ‘chunked’ storage layout.  This means the
dataset is divided up into regularly-sized pieces which are stored
haphazardly on disk, and indexed using a B-tree.

Chunked storage makes it possible to resize datasets, and because the
data is stored in fixed-size chunks, to use compression filters.

To enable chunked storage, set the keyword ‘chunks’ to a tuple
indicating the chunk shape:

     >>> dset = f.create_dataset("chunked", (1000, 1000), chunks=(100, 100))

Data will be read and written in blocks with shape (100,100); for
example, the data in ‘dset[0:100,0:100]’ will be stored together in the
file, as will the data points in range ‘dset[400:500, 100:200]’.

Chunking has performance implications.  It’s recommended to keep the
total size of your chunks between 10 KiB and 1 MiB, larger for larger
datasets.  Also keep in mind that when any element in a chunk is
accessed, the entire chunk is read from disk.

Since picking a chunk shape can be confusing, you can have h5py guess a
chunk shape for you:

     >>> dset = f.create_dataset("autochunk", (1000, 1000), chunks=True)

Auto-chunking is also enabled when using compression or ‘maxshape’,
etc., if a chunk shape is not manually specified.

The chunk_iter method returns an iterator that can be used to perform
chunk by chunk reads or writes:

     >>> for s in dset.chunk_iter():
     >>>     arr = dset[s]  # get numpy array for chunk


File: h5py.info,  Node: Resizable datasets,  Next: Filter pipeline,  Prev: Chunked storage,  Up: Datasets

4.3.4 Resizable datasets
------------------------

In HDF5, datasets can be resized once created up to a maximum size, by
calling *note Dataset.resize(): 92.  You specify this maximum size when
creating the dataset, via the keyword ‘maxshape’:

     >>> dset = f.create_dataset("resizable", (10,10), maxshape=(500, 20))

Any (or all) axes may also be marked as “unlimited”, in which case they
may be increased up to the HDF5 per-axis limit of 2**64 elements.
Indicate these axes using ‘None’:

     >>> dset = f.create_dataset("unlimited", (10, 10), maxshape=(None, 10))

     Note: Resizing an array with existing data works differently than
     in NumPy; if any axis shrinks, the data in the missing region is
     discarded.  Data does not “rearrange” itself as it does when
     resizing a NumPy array.


File: h5py.info,  Node: Filter pipeline,  Next: Multi-Block Selection,  Prev: Resizable datasets,  Up: Datasets

4.3.5 Filter pipeline
---------------------

Chunked data may be transformed by the HDF5 ‘filter pipeline’.  The most
common use is applying transparent compression.  Data is compressed on
the way to disk, and automatically decompressed when read.  Once the
dataset is created with a particular compression filter applied, data
may be read and written as normal with no special steps required.

Enable compression with the ‘compression’ keyword to *note
Group.create_dataset(): 6c.:

     >>> dset = f.create_dataset("zipped", (100, 100), compression="gzip")

Options for each filter may be specified with ‘compression_opts’:

     >>> dset = f.create_dataset("zipped_max", (100, 100), compression="gzip", compression_opts=9)

* Menu:

* Lossless compression filters::
* Custom compression filters::
* Scale-Offset filter::
* Shuffle filter::
* Fletcher32 filter::


File: h5py.info,  Node: Lossless compression filters,  Next: Custom compression filters,  Up: Filter pipeline

4.3.5.1 Lossless compression filters
....................................

GZIP filter (‘"gzip"’)

     Available with every installation of HDF5, so it’s best where
     portability is required.  Good compression, moderate speed.
     ‘compression_opts’ sets the compression level and may be an integer
     from 0 to 9, default is 4.

LZF filter (‘"lzf"’)

     Available with every installation of h5py (C source code also
     available).  Low to moderate compression, very fast.  No options.

SZIP filter (‘"szip"’)

     Patent-encumbered filter used in the NASA community.  Not available
     with all installations of HDF5 due to legal reasons.  Consult the
     HDF5 docs for filter options.


File: h5py.info,  Node: Custom compression filters,  Next: Scale-Offset filter,  Prev: Lossless compression filters,  Up: Filter pipeline

4.3.5.2 Custom compression filters
..................................

In addition to the compression filters listed above, compression filters
can be dynamically loaded by the underlying HDF5 library.  This is done
by passing a filter number to *note Group.create_dataset(): 6c. as the
‘compression’ parameter.  The ‘compression_opts’ parameter will then be
passed to this filter.

     Note: The underlying implementation of the compression filter will
     have the ‘H5Z_FLAG_OPTIONAL’ flag set.  This indicates that if the
     compression filter doesn’t compress a block while writing, no error
     will be thrown.  The filter will then be skipped when subsequently
     reading the block.


File: h5py.info,  Node: Scale-Offset filter,  Next: Shuffle filter,  Prev: Custom compression filters,  Up: Filter pipeline

4.3.5.3 Scale-Offset filter
...........................

Filters enabled with the ‘compression’ keywords are `lossless'; what
comes out of the dataset is exactly what you put in.  HDF5 also includes
a lossy filter which trades precision for storage space.

Works with integer and floating-point data only.  Enable the
scale-offset filter by setting *note Group.create_dataset(): 6c. keyword
‘scaleoffset’ to an integer.

For integer data, this specifies the number of bits to retain.  Set to 0
to have HDF5 automatically compute the number of bits required for
lossless compression of the chunk.  For floating-point data, indicates
the number of digits after the decimal point to retain.

     Warning: Currently the scale-offset filter does not preserve
     special float values (i.e.  NaN, inf), see
     ‘https://forum.hdfgroup.org/t/scale-offset-filter-and-special-float-values-nan-infinity/3379’
     for more information and follow-up.


File: h5py.info,  Node: Shuffle filter,  Next: Fletcher32 filter,  Prev: Scale-Offset filter,  Up: Filter pipeline

4.3.5.4 Shuffle filter
......................

Block-oriented compressors like GZIP or LZF work better when presented
with runs of similar values.  Enabling the shuffle filter rearranges the
bytes in the chunk and may improve compression ratio.  No significant
speed penalty, lossless.

Enable by setting *note Group.create_dataset(): 6c. keyword ‘shuffle’ to
True.


File: h5py.info,  Node: Fletcher32 filter,  Prev: Shuffle filter,  Up: Filter pipeline

4.3.5.5 Fletcher32 filter
.........................

Adds a checksum to each chunk to detect data corruption.  Attempts to
read corrupted chunks will fail with an error.  No significant speed
penalty.  Obviously shouldn’t be used with lossy compression filters.

Enable by setting *note Group.create_dataset(): 6c. keyword ‘fletcher32’
to True.


File: h5py.info,  Node: Multi-Block Selection,  Next: Fancy indexing,  Prev: Filter pipeline,  Up: Datasets

4.3.6 Multi-Block Selection
---------------------------

The full H5Sselect_hyperslab API is exposed via the MultiBlockSlice
object.  This takes four elements to define the selection (start, count,
stride and block) in contrast to the built-in slice object, which takes
three elements.  A MultiBlockSlice can be used in place of a slice to
select a number of (count) blocks of multiple elements separated by a
stride, rather than a set of single elements separated by a step.

For an explanation of how this slicing works, see the HDF5
documentation(1).

For example:

     >>> dset[...]
     array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10])
     >>> dset[MultiBlockSlice(start=1, count=3, stride=4, block=2)]
     array([ 1,  2,  5,  6,  9, 10])

They can be used in multi-dimensional slices alongside any slicing
object, including other MultiBlockSlices.  For a more complete example
of this, see the multiblockslice_interleave.py example script.

   ---------- Footnotes ----------

   (1) https://support.hdfgroup.org/HDF5/Tutor/selectsimple.html


File: h5py.info,  Node: Fancy indexing,  Next: Creating and Reading Empty or Null datasets and attributes,  Prev: Multi-Block Selection,  Up: Datasets

4.3.7 Fancy indexing
--------------------

A subset of the NumPy fancy-indexing syntax is supported.  Use this with
caution, as the underlying HDF5 mechanisms may have different
performance than you expect.

For any axis, you can provide an explicit list of points you want; for a
dataset with shape (10, 10):

     >>> dset.shape
     (10, 10)
     >>> result = dset[0, [1,3,8]]
     >>> result.shape
     (3,)
     >>> result = dset[1:6, [5,8,9]]
     >>> result.shape
     (5, 3)

The following restrictions exist:

   * Selection coordinates must be given in increasing order

   * Duplicate selections are ignored

   * Very long lists (> 1000 elements) may produce poor performance

NumPy boolean “mask” arrays can also be used to specify a selection.
The result of this operation is a 1-D array with elements arranged in
the standard NumPy (C-style) order.  Behind the scenes, this generates a
laundry list of points to select, so be careful when using it with large
masks:

     >>> arr = numpy.arange(100).reshape((10,10))
     >>> dset = f.create_dataset("MyDataset", data=arr)
     >>> result = dset[arr > 50]
     >>> result.shape
     (49,)

Changed in version 2.10: Selecting using an empty list is now allowed.
This returns an array with length 0 in the relevant dimension.


File: h5py.info,  Node: Creating and Reading Empty or Null datasets and attributes,  Next: Reference<3>,  Prev: Fancy indexing,  Up: Datasets

4.3.8 Creating and Reading Empty (or Null) datasets and attributes
------------------------------------------------------------------

HDF5 has the concept of Empty or Null datasets and attributes.  These
are not the same as an array with a shape of (), or a scalar dataspace
in HDF5 terms.  Instead, it is a dataset with an associated type, no
data, and no shape.  In h5py, we represent this as either a dataset with
shape ‘None’, or an instance of ‘h5py.Empty’.  Empty datasets and
attributes cannot be sliced.

To create an empty attribute, use ‘h5py.Empty’ as per *note Attributes:
12.:

     >>> obj.attrs["EmptyAttr"] = h5py.Empty("f")

Similarly, reading an empty attribute returns ‘h5py.Empty’:

     >>> obj.attrs["EmptyAttr"]
     h5py.Empty(dtype="f")

Empty datasets can be created either by defining a ‘dtype’ but no
‘shape’ in ‘create_dataset’:

     >>> grp.create_dataset("EmptyDataset", dtype="f")

or by ‘data’ to an instance of ‘h5py.Empty’:

     >>> grp.create_dataset("EmptyDataset", data=h5py.Empty("f"))

An empty dataset has shape defined as ‘None’, which is the best way of
determining whether a dataset is empty or not.  An empty dataset can be
“read” in a similar way to scalar datasets, i.e.  if ‘empty_dataset’ is
an empty dataset:

     >>> empty_dataset[()]
     h5py.Empty(dtype="f")

The dtype of the dataset can be accessed via ‘<dset>.dtype’ as per
normal.  As empty datasets cannot be sliced, some methods of datasets
such as ‘read_direct’ will raise a ‘TypeError’ exception if used on a
empty dataset.


File: h5py.info,  Node: Reference<3>,  Prev: Creating and Reading Empty or Null datasets and attributes,  Up: Datasets

4.3.9 Reference
---------------

 -- Class: h5py.Dataset (identifier)

     Dataset objects are typically created via *note
     Group.create_dataset(): 6c, or by retrieving existing datasets from
     a file.  Call this constructor to create a new Dataset bound to an
     existing DatasetID(1) identifier.

      -- Method: __getitem__ (args)

          NumPy-style slicing to retrieve data.  See *note Reading &
          writing data: 88.

      -- Method: __setitem__ (args)

          NumPy-style slicing to write data.  See *note Reading &
          writing data: 88.

      -- Method: __bool__ ()

          Check that the dataset is accessible.  A dataset could be
          inaccessible for several reasons.  For instance, the dataset,
          or the file it belongs to, may have been closed elsewhere.

               >>> f = h5py.open(filename)
               >>> dset = f["MyDS"]
               >>> f.close()
               >>> if dset:
               ...     print("datset accessible")
               ... else:
               ...     print("dataset inaccessible")
               dataset inaccessible

      -- Method: read_direct (array, source_sel=None, dest_sel=None)

          Read from an HDF5 dataset directly into a NumPy array, which
          can avoid making an intermediate copy as happens with slicing.
          The destination array must be C-contiguous and writable, and
          must have a datatype to which the source data may be cast.
          Data type conversion will be carried out on the fly by HDF5.

          ‘source_sel’ and ‘dest_sel’ indicate the range of points in
          the dataset and destination array respectively.  Use the
          output of ‘numpy.s_[args]’:

               >>> dset = f.create_dataset("dset", (100,), dtype='int64')
               >>> arr = np.zeros((100,), dtype='int32')
               >>> dset.read_direct(arr, np.s_[0:10], np.s_[50:60])

      -- Method: write_direct (source, source_sel=None, dest_sel=None)

          Write data directly to HDF5 from a NumPy array.  The source
          array must be C-contiguous.  Selections must be the output of
          numpy.s_[<args>].  Broadcasting is supported for simple
          indexing.

      -- Method: astype (dtype)

          Return a wrapper allowing you to read data as a particular
          type.  Conversion is handled by HDF5 directly, on the fly:

               >>> dset = f.create_dataset("bigint", (1000,), dtype='int64')
               >>> out = dset.astype('int16')[:]
               >>> out.dtype
               dtype('int16')

          Changed in version 3.0: Allowed reading through the wrapper
          object.  In earlier versions, *note astype(): a3. had to be
          used as a context manager:

               >>> with dset.astype('int16'):
               ...     out = dset[:]

      -- Method: asstr (encoding=None, errors='strict')

          Only for string datasets.  Returns a wrapper to read data as
          Python string objects:

               >>> s = dataset.asstr()[0]

          encoding and errors work like ‘bytes.decode()’, but the
          default encoding is defined by the datatype - ASCII or UTF-8.
          This is not guaranteed to be correct.

          New in version 3.0.

      -- Method: fields (names)

          Get a wrapper to read a subset of fields from a compound data
          type:

               >>> 2d_coords = dataset.fields(['x', 'y'])[:]

          If names is a string, a single field is extracted, and the
          resulting arrays will have that dtype.  Otherwise, it should
          be an iterable, and the read data will have a compound dtype.

          New in version 3.0.

      -- Method: iter_chunks ()

          Iterate over chunks in a chunked dataset.  The optional ‘sel’
          argument is a slice or tuple of slices that defines the region
          to be used.  If not set, the entire dataspace will be used for
          the iterator.

          For each chunk within the given region, the iterator yields a
          tuple of slices that gives the intersection of the given chunk
          with the selection area.  This can be used to *note read or
          write data in that chunk: 88.

          A TypeError will be raised if the dataset is not chunked.

          A ValueError will be raised if the selection region is
          invalid.

          New in version 3.0.

      -- Method: resize (size, axis=None)

          Change the shape of a dataset.  ‘size’ may be a tuple giving
          the new dataset shape, or an integer giving the new length of
          the specified ‘axis’.

          Datasets may be resized only up to *note Dataset.maxshape: a7.

      -- Method: len ()

          Return the size of the first axis.

      -- Method: make_scale (name='')

          Make this dataset an HDF5 *note dimension scale: a9.

          You can then attach it to dimensions of other datasets like
          this:

               other_ds.dims[0].attach_scale(ds)

          You can optionally pass a name to associate with this scale.

      -- Method: virtual_sources ()

          If this dataset is a *note virtual dataset: 74, return a list
          of named tuples: ‘(vspace, file_name, dset_name, src_space)’,
          describing which parts of the dataset map to which source
          datasets.  The two ‘space’ members are low-level SpaceID(2)
          objects.

      -- Attribute: shape

          NumPy-style shape tuple giving dataset dimensions.

      -- Attribute: dtype

          NumPy dtype object giving the dataset’s type.

      -- Attribute: size

          Integer giving the total number of elements in the dataset.

      -- Attribute: nbytes

          Integer giving the total number of bytes required to load the
          full dataset into RAM (i.e.  ‘dset[()]’).  This may not be the
          amount of disk space occupied by the dataset, as datasets may
          be compressed when written or only partly filled with data.
          This value also does not include the array overhead, as it
          only describes the size of the data itself.  Thus the real
          amount of RAM occupied by this dataset may be slightly
          greater.

          New in version 3.0.

      -- Attribute: ndim

          Integer giving the total number of dimensions in the dataset.

      -- Attribute: maxshape

          NumPy-style shape tuple indicating the maximum dimensions up
          to which the dataset may be resized.  Axes with ‘None’ are
          unlimited.

      -- Attribute: chunks

          Tuple giving the chunk shape, or None if chunked storage is
          not used.  See *note Chunked storage: 40.

      -- Attribute: compression

          String with the currently applied compression filter, or None
          if compression is not enabled for this dataset.  See *note
          Filter pipeline: 24.

      -- Attribute: compression_opts

          Options for the compression filter.  See *note Filter
          pipeline: 24.

      -- Attribute: scaleoffset

          Setting for the HDF5 scale-offset filter (integer), or None if
          scale-offset compression is not used for this dataset.  See
          *note Scale-Offset filter: 6e.

      -- Attribute: shuffle

          Whether the shuffle filter is applied (T/F). See *note Shuffle
          filter: 6f.

      -- Attribute: fletcher32

          Whether Fletcher32 checksumming is enabled (T/F). See *note
          Fletcher32 filter: 70.

      -- Attribute: fillvalue

          Value used when reading uninitialized portions of the dataset,
          or None if no fill value has been defined, in which case HDF5
          will use a type-appropriate default value.  Can’t be changed
          after the dataset is created.

      -- Attribute: external

          If this dataset is stored in one or more external files, this
          is a list of 3-tuples, like the ‘external=’ parameter to *note
          Group.create_dataset(): 6c.  Otherwise, it is ‘None’.

      -- Attribute: is_virtual

          True if this dataset is a *note virtual dataset: 74, otherwise
          False.

      -- Attribute: dims

          Access to *note Dimension Scales: a9.

      -- Attribute: attrs

          *note Attributes: 12. for this dataset.

      -- Attribute: id

          The dataset’s low-level identifier; an instance of
          DatasetID(3).

      -- Attribute: ref

          An HDF5 object reference pointing to this dataset.  See *note
          Using object references: 79.

      -- Attribute: regionref

          Proxy object for creating HDF5 region references.  See *note
          Using region references: 7b.

      -- Attribute: name

          String giving the full path to this dataset.

      -- Attribute: file

          *note File: d. instance in which this dataset resides

      -- Attribute: parent

          *note Group: 43. instance containing this dataset.

   ---------- Footnotes ----------

   (1) https://api.h5py.org/h5d.html#h5py.h5d.DatasetID

   (2) https://api.h5py.org/h5s.html#h5py.h5s.SpaceID

   (3) https://api.h5py.org/h5d.html#h5py.h5d.DatasetID


File: h5py.info,  Node: Attributes<2>,  Next: Dimension Scales,  Prev: Datasets,  Up: High-level API reference

4.4 Attributes
==============

Attributes are a critical part of what makes HDF5 a “self-describing”
format.  They are small named pieces of data attached directly to *note
Group: 43. and *note Dataset: 4e. objects.  This is the official way to
store metadata in HDF5.

Each Group or Dataset has a small proxy object attached to it, at
‘<obj>.attrs’.  Attributes have the following properties:

   - They may be created from any scalar or NumPy array

   - Each attribute should be small (generally < 64k)

   - There is no partial I/O (i.e.  slicing); the entire attribute must
     be read.

The ‘.attrs’ proxy objects are of class *note AttributeManager: c3,
below.  This class supports a dictionary-style interface.

By default, attributes are iterated in alphanumeric order.  However, if
group or dataset is created with ‘track_order=True’, the attribute
insertion order is remembered (tracked) in HDF5 file, and iteration uses
that order.  The latter is consistent with Python 3.7+ dictionaries.

The default ‘track_order’ for all new groups and datasets can be
specified globally with ‘h5.get_config().track_order’.

* Menu:

* Reference: Reference<4>.


File: h5py.info,  Node: Reference<4>,  Up: Attributes<2>

4.4.1 Reference
---------------

 -- Class: h5py.AttributeManager (parent)

     AttributeManager objects are created directly by h5py.  You should
     access instances by ‘group.attrs’ or ‘dataset.attrs’, not by
     manually creating them.

      -- Method: __iter__ ()

          Get an iterator over attribute names.

      -- Method: __contains__ (name)

          Determine if attribute ‘name’ is attached to this object.

      -- Method: __getitem__ (name)

          Retrieve an attribute.

      -- Method: __setitem__ (name, val)

          Create an attribute, overwriting any existing attribute.  The
          type and shape of the attribute are determined automatically
          by h5py.

      -- Method: __delitem__ (name)

          Delete an attribute.  KeyError if it doesn’t exist.

      -- Method: keys ()

          Get the names of all attributes attached to this object.


          Returns: set-like object.

      -- Method: values ()

          Get the values of all attributes attached to this object.


          Returns: collection or bag-like object.

      -- Method: items ()

          Get ‘(name, value)’ tuples for all attributes attached to this
          object.


          Returns: collection or set-like object.

      -- Method: get (name, default=None)

          Retrieve ‘name’, or ‘default’ if no such attribute exists.

      -- Method: get_id (name)

          Get the low-level AttrID(1) for the named attribute.

      -- Method: create (name, data, shape=None, dtype=None)

          Create a new attribute, with control over the shape and type.
          Any existing attribute will be overwritten.


          Parameters:

             * ‘name’ (‘String’) – Name of the new attribute

             * ‘data’ – Value of the attribute; will be put through
               ‘numpy.array(data)’.

             * ‘shape’ (‘Tuple’) – Shape of the attribute.  Overrides
               ‘data.shape’ if both are given, in which case the total
               number of points must be unchanged.

             * ‘dtype’ (‘NumPy dtype’) – Data type for the attribute.
               Overrides ‘data.dtype’ if both are given.

      -- Method: modify (name, value)

          Change the value of an attribute while preserving its type and
          shape.  Unlike *note AttributeManager.__setitem__(): c8, if
          the attribute already exists, only its value will be changed.
          This can be useful for interacting with externally generated
          files, where the type and shape must not be altered.

          If the attribute doesn’t exist, it will be created with a
          default shape and type.


          Parameters:

             * ‘name’ (‘String’) – Name of attribute to modify.

             * ‘value’ – New value.  Will be put through
               ‘numpy.array(value)’.

   ---------- Footnotes ----------

   (1) https://api.h5py.org/h5a.html#h5py.h5a.AttrID


File: h5py.info,  Node: Dimension Scales,  Next: Low-Level API,  Prev: Attributes<2>,  Up: High-level API reference

4.5 Dimension Scales
====================

Datasets are multidimensional arrays.  HDF5 provides support for
labeling the dimensions and associating one or more “dimension scales”
with each dimension.  A dimension scale is simply another HDF5 dataset.
In principle, the length of the multidimensional array along the
dimension of interest should be equal to the length of the dimension
scale, but HDF5 does not enforce this property.

The HDF5 library provides the H5DS API for working with dimension
scales.  H5py provides low-level bindings to this API in h5py.h5ds(1).
These low-level bindings are in turn used to provide a high-level
interface through the ‘Dataset.dims’ property.  Suppose we have the
following data file:

     f = File('foo.h5', 'w')
     f['data'] = np.ones((4, 3, 2), 'f')

HDF5 allows the dimensions of ‘data’ to be labeled, for example:

     f['data'].dims[0].label = 'z'
     f['data'].dims[2].label = 'x'

Note that the first dimension, which has a length of 4, has been labeled
“z”, the third dimension (in this case the fastest varying dimension),
has been labeled “x”, and the second dimension was given no label at
all.

We can also use HDF5 datasets as dimension scales.  For example, if we
have:

     f['x1'] = [1, 2]
     f['x2'] = [1, 1.1]
     f['y1'] = [0, 1, 2]
     f['z1'] = [0, 1, 4, 9]

We are going to treat the ‘x1’, ‘x2’, ‘y1’, and ‘z1’ datasets as
dimension scales:

     f['x1'].make_scale()
     f['x2'].make_scale('x2 name')
     f['y1'].make_scale('y1 name')
     f['z1'].make_scale('z1 name')

When you create a dimension scale, you may provide a name for that
scale.  In this case, the ‘x1’ scale was not given a name, but the
others were.  Now we can associate these dimension scales with the
primary dataset:

     f['data'].dims[0].attach_scale(f['z1'])
     f['data'].dims[1].attach_scale(f['y1'])
     f['data'].dims[2].attach_scale(f['x1'])
     f['data'].dims[2].attach_scale(f['x2'])

Note that two dimension scales were associated with the third dimension
of ‘data’.  You can also detach a dimension scale:

     f['data'].dims[2].detach_scale(f['x2'])

but for now, lets assume that we have both ‘x1’ and ‘x2’ still
associated with the third dimension of ‘data’.  You can attach a
dimension scale to any number of HDF5 datasets, you can even attach it
to multiple dimensions of a single HDF5 dataset.

Now that the dimensions of ‘data’ have been labeled, and the dimension
scales for the various axes have been specified, we have provided much
more context with which ‘data’ can be interpreted.  For example, if you
want to know the labels for the various dimensions of ‘data’:

     >>> [dim.label for dim in f['data'].dims]
     ['z', '', 'x']

If you want the names of the dimension scales associated with the “x”
axis:

     >>> f['data'].dims[2].keys()
     ['', 'x2 name']

‘items()’ and ‘values()’ methods are also provided.  The dimension
scales themselves can also be accessed with:

     f['data'].dims[2][1]

or:

     f['data'].dims[2]['x2 name']

such that:

     >>> f['data'].dims[2][1] == f['x2']
     True

though, beware that if you attempt to index the dimension scales with a
string, the first dimension scale whose name matches the string is the
one that will be returned.  There is no guarantee that the name of the
dimension scale is unique.

Nested dimension scales are not permitted: if a dataset has a dimension
scale attached to it, converting the dataset to a dimension scale will
fail, since the HDF5 specification doesn’t allow this(2).

     >>> f['data'].make_scale()
     RuntimeError: Unspecified error in H5DSset_scale (return value <0)

   ---------- Footnotes ----------

   (1) https://api.h5py.org/h5ds.html#module-h5py.h5ds

   (2) https://confluence.hdfgroup.org/display/HDF5/H5DS_SET_SCALE


File: h5py.info,  Node: Low-Level API,  Prev: Dimension Scales,  Up: High-level API reference

4.6 Low-Level API
=================

This documentation mostly describes the h5py high-level API, which
offers the main features of HDF5 in an interface modelled on
dictionaries and NumPy arrays.  h5py also provides a low-level API,
which more closely follows the HDF5 C API.

See also
........

   - h5py Low-Level API Reference(1)

   - HDF5 C/Fortran Reference Manual(2)

You can easily switch between the two levels in your code:

   - `To the low-level': High-level *note File: d, *note Group: 43. and
     *note Dataset: 4e. objects all have a ‘.id’ attribute exposing the
     corresponding low-level objects—FileID(3), GroupID(4) and
     DatasetID(5):

          dsid = dset.id
          dsid.get_offset()  # Low-level method

     Although there is no high-level object for a single attribute,
     *note AttributeManager.get_id(): ce. will get the low-level
     AttrID(6) object:

          aid = dset.attrs.get_id('timestamp')
          aid.get_storage_size()  # Low-level method

   - `To the high-level': Low-level FileID(7), GroupID(8) and
     DatasetID(9) objects can be passed to the constructors of *note
     File: d, *note Group: 43. and *note Dataset: 4e, respectively.

   ---------- Footnotes ----------

   (1) https://api.h5py.org/

   (2) https://confluence.hdfgroup.org/display/HDF5/Core+Library

   (3) https://api.h5py.org/h5f.html#h5py.h5f.FileID

   (4) https://api.h5py.org/h5g.html#h5py.h5g.GroupID

   (5) https://api.h5py.org/h5d.html#h5py.h5d.DatasetID

   (6) https://api.h5py.org/h5a.html#h5py.h5a.AttrID

   (7) https://api.h5py.org/h5f.html#h5py.h5f.FileID

   (8) https://api.h5py.org/h5g.html#h5py.h5g.GroupID

   (9) https://api.h5py.org/h5d.html#h5py.h5d.DatasetID


File: h5py.info,  Node: Advanced topics,  Next: Meta-info about the h5py project,  Prev: High-level API reference,  Up: Top

5 Advanced topics
*****************

* Menu:

* Configuring h5py::
* Special types::
* Strings in HDF5::
* Object and Region References::
* Parallel HDF5::
* Single Writer Multiple Reader (SWMR): Single Writer Multiple Reader SWMR.
* Virtual Datasets (VDS): Virtual Datasets VDS.


File: h5py.info,  Node: Configuring h5py,  Next: Special types,  Up: Advanced topics

5.1 Configuring h5py
====================

* Menu:

* Library configuration::
* IPython::


File: h5py.info,  Node: Library configuration,  Next: IPython,  Up: Configuring h5py

5.1.1 Library configuration
---------------------------

A few library options are available to change the behavior of the
library.  You can get a reference to the global library configuration
object via the function ‘h5py.get_config()’.  This object supports the
following attributes:

     `complex_names'

          Set to a 2-tuple of strings (real, imag) to control how
          complex numbers are saved.  The default is (‘r’,’i’).

     `bool_names'

          Booleans are saved as HDF5 enums.  Set this to a 2-tuple of
          strings (false, true) to control the names used in the enum.
          The default is (“FALSE”, “TRUE”).

     `track_order'

          Whether to track dataset/group/attribute creation order.  If
          container creation order is tracked, its links and attributes
          are iterated in ascending creation order (consistent with
          ‘dict’ in Python 3.7+); otherwise in ascending alphanumeric
          order.  Global configuration value can be overridden for
          particular container by specifying ‘track_order’ argument to
          *note h5py.File: d, *note h5py.Group.create_group(): 59, *note
          h5py.Group.create_dataset(): 6c.  The default is ‘False’.


File: h5py.info,  Node: IPython,  Prev: Library configuration,  Up: Configuring h5py

5.1.2 IPython
-------------

H5py ships with a custom ipython completer, which provides object
introspection and tab completion for h5py objects in an ipython session.
For example, if a file contains 3 groups, “foo”, “bar”, and “baz”:

     In [4]: f['b<TAB>
     bar   baz

     In [4]: f['f<TAB>
     # Completes to:
     In [4]: f['foo'

     In [4]: f['foo'].<TAB>
     f['foo'].attrs            f['foo'].items            f['foo'].ref
     f['foo'].copy             f['foo'].iteritems        f['foo'].require_dataset
     f['foo'].create_dataset   f['foo'].iterkeys         f['foo'].require_group
     f['foo'].create_group     f['foo'].itervalues       f['foo'].values
     f['foo'].file             f['foo'].keys             f['foo'].visit
     f['foo'].get              f['foo'].name             f['foo'].visititems
     f['foo'].id               f['foo'].parent

The easiest way to enable the custom completer is to do the following in
an IPython session:

     In  [1]: import h5py

     In [2]: h5py.enable_ipython_completer()

It is also possible to configure IPython to enable the completer every
time you start a new session.  For >=ipython-0.11, “h5py.ipy_completer”
just needs to be added to the list of extensions in your ipython config
file, for example ‘~/.config/ipython/profile_default/ipython_config.py’
(if this file does not exist, you can create it by invoking ‘ipython
profile create’):

     c = get_config()
     c.InteractiveShellApp.extensions = ['h5py.ipy_completer']

For <ipython-0.11, the completer can be enabled by adding the following
lines to the ‘main()’ in ‘.ipython/ipy_user_conf.py’:

     def main():
         ip.ex('from h5py import ipy_completer')
         ip.ex('ipy_completer.load_ipython_extension()')


File: h5py.info,  Node: Special types,  Next: Strings in HDF5,  Prev: Configuring h5py,  Up: Advanced topics

5.2 Special types
=================

HDF5 supports a few types which have no direct NumPy equivalent.  Among
the most useful and widely used are `variable-length' (VL) types, and
enumerated types.  As of version 2.3, h5py fully supports HDF5 enums and
VL types.

* Menu:

* How special types are represented::
* Variable-length strings::
* Arbitrary vlen data::
* Enumerated types::
* Object and region references::
* Storing other types as opaque data::
* Older API::


File: h5py.info,  Node: How special types are represented,  Next: Variable-length strings,  Up: Special types

5.2.1 How special types are represented
---------------------------------------

Since there is no direct NumPy dtype for variable-length strings, enums
or references, h5py extends the dtype system slightly to let HDF5 know
how to store these types.  Each type is represented by a native NumPy
dtype, with a small amount of metadata attached.  NumPy routines ignore
the metadata, but h5py can use it to determine how to store the data.

The metadata h5py attaches to dtypes is not part of the public API, so
it may change between versions.  Use the functions described below to
create and check for these types.


File: h5py.info,  Node: Variable-length strings,  Next: Arbitrary vlen data,  Prev: How special types are represented,  Up: Special types

5.2.2 Variable-length strings
-----------------------------

See also
........

*note Strings in HDF5: df.

In HDF5, data in VL format is stored as arbitrary-length vectors of a
base type.  In particular, strings are stored C-style in null-terminated
buffers.  NumPy has no native mechanism to support this.  Unfortunately,
this is the de facto standard for representing strings in the HDF5 C
API, and in many HDF5 applications.

Thankfully, NumPy has a generic pointer type in the form of the “object”
(“O”) dtype.  In h5py, variable-length strings are mapped to object
arrays.  A small amount of metadata attached to an “O” dtype tells h5py
that its contents should be converted to VL strings when stored in the
file.

Existing VL strings can be read and written to with no additional
effort; Python strings and fixed-length NumPy strings can be
auto-converted to VL data and stored.

Here’s an example showing how to create a VL array of strings:

     >>> f = h5py.File('foo.hdf5')
     >>> dt = h5py.string_dtype(encoding='utf-8')
     >>> ds = f.create_dataset('VLDS', (100,100), dtype=dt)
     >>> ds.dtype.kind
     'O'
     >>> h5py.check_string_dtype(ds.dtype)
     string_info(encoding='utf-8', length=None)

 -- Function: h5py.string_dtype (encoding='utf-8', length=None)

     Make a numpy dtype for HDF5 strings


     Parameters:

        * ‘encoding’ – ‘'utf-8'’ or ‘'ascii'’.

        * ‘length’ – ‘None’ for variable-length, or an integer for
          fixed-length string data, giving the length in bytes.

 -- Function: h5py.check_string_dtype (dt)

     Check if ‘dt’ is a string dtype.  Returns a `string_info' object if
     it is, or ‘None’ if not.

 -- Class: h5py.string_info

     A named tuple type holding string encoding and length.

      -- Attribute: encoding

          The character encoding associated with the string dtype, which
          can be ‘'utf-8'’ or ‘'ascii'’.

      -- Attribute: length

          For fixed-length string dtypes, the length in bytes.  ‘None’
          for variable-length strings.


File: h5py.info,  Node: Arbitrary vlen data,  Next: Enumerated types,  Prev: Variable-length strings,  Up: Special types

5.2.3 Arbitrary vlen data
-------------------------

Starting with h5py 2.3, variable-length types are not restricted to
strings.  For example, you can create a “ragged” array of integers:

     >>> dt = h5py.vlen_dtype(np.dtype('int32'))
     >>> dset = f.create_dataset('vlen_int', (100,), dtype=dt)
     >>> dset[0] = [1,2,3]
     >>> dset[1] = [1,2,3,4,5]

Single elements are read as NumPy arrays:

     >>> dset[0]
     array([1, 2, 3], dtype=int32)

Multidimensional selections produce an object array whose members are
integer arrays:

     >>> dset[0:2]
     array([array([1, 2, 3], dtype=int32), array([1, 2, 3, 4, 5], dtype=int32)], dtype=object)

 -- Function: h5py.vlen_dtype (basetype)

     Make a numpy dtype for an HDF5 variable-length datatype.


     Parameters: ‘basetype’ – The dtype of each element in the array.

 -- Function: h5py.check_vlen_dtype (dt)

     Check if ‘dt’ is a variable-length dtype.  Returns the base type if
     it is, or ‘None’ if not.


File: h5py.info,  Node: Enumerated types,  Next: Object and region references,  Prev: Arbitrary vlen data,  Up: Special types

5.2.4 Enumerated types
----------------------

HDF5 has the concept of an `enumerated type', which is an integer
datatype with a restriction to certain named values.  Since NumPy has no
such datatype, HDF5 ENUM types are read and written as integers.

Here’s an example of creating an enumerated type:

     >>> dt = h5py.enum_dtype({"RED": 0, "GREEN": 1, "BLUE": 42}, basetype='i')
     >>> h5py.check_enum_dtype(dt)
     {'BLUE': 42, 'GREEN': 1, 'RED': 0}
     >>> f = h5py.File('foo.hdf5','w')
     >>> ds = f.create_dataset("EnumDS", (100,100), dtype=dt)
     >>> ds.dtype.kind
     'i'
     >>> ds[0,:] = 42
     >>> ds[0,0]
     42
     >>> ds[1,0]
     0

 -- Function: h5py.enum_dtype (values_dict, basetype=np.uint8)

     Create a NumPy representation of an HDF5 enumerated type


     Parameters:

        * ‘values_dict’ – Mapping of string names to integer values.

        * ‘basetype’ – An appropriate integer base dtype large enough to
          hold the possible options.

 -- Function: h5py.check_enum_dtype (dt)

     Check if ‘dt’ represents an enumerated type.  Returns the values
     dict if it is, or ‘None’ if not.


File: h5py.info,  Node: Object and region references,  Next: Storing other types as opaque data,  Prev: Enumerated types,  Up: Special types

5.2.5 Object and region references
----------------------------------

References have their *note own section: 5f.


File: h5py.info,  Node: Storing other types as opaque data,  Next: Older API,  Prev: Object and region references,  Up: Special types

5.2.6 Storing other types as opaque data
----------------------------------------

New in version 3.0.

Numpy datetime64 and timedelta64 dtypes have no equivalent in HDF5 (the
HDF5 time type is broken and deprecated).  h5py allows you to store such
data with an HDF5 opaque type; it can be read back correctly by h5py,
but won’t be interoperable with other tools.

Here’s an example of storing and reading a datetime array:

     >>> arr = np.array([np.datetime64('2019-09-22T17:38:30')])
     >>> f['data'] = arr.astype(h5py.opaque_dtype(arr.dtype))
     >>> print(f['data'][:])
     ['2019-09-22T17:38:30']

 -- Function: h5py.opaque_dtype (dt)

     Return a dtype like the input, tagged to be stored as HDF5 opaque
     type.

 -- Function: h5py.check_opaque_dtype (dt)

     Return True if the dtype given is tagged to be stored as HDF5
     opaque data.

     Note: With some exceptions, you can use *note opaque_dtype(): ef.
     with any numpy dtype.  While this may seem like a convenient way to
     get arbitrary data into HDF5, remember that it’s not a standard
     format.  It’s better to fit your data into HDF5’s native
     structures, or use a file format better suited to your data.


File: h5py.info,  Node: Older API,  Prev: Storing other types as opaque data,  Up: Special types

5.2.7 Older API
---------------

Before h5py 2.10, a single pair of functions was used to create and
check for all of these special dtypes.  These are still available for
backwards compatibility, but are deprecated in favour of the functions
listed above.

 -- Function: h5py.special_dtype (**kwds)

     Create a NumPy dtype object containing type hints.  Only one
     keyword may be specified.


     Parameters:

        * ‘vlen’ – Base type for HDF5 variable-length datatype.

        * ‘enum’ – 2-tuple ‘(basetype, values_dict)’.  ‘basetype’ must
          be an integer dtype; ‘values_dict’ is a dictionary mapping
          string names to integer values.

        * ‘ref’ – Provide class ‘h5py.Reference’ or
          ‘h5py.RegionReference’ to create a type representing object or
          region references respectively.

 -- Function: h5py.check_dtype (**kwds)

     Determine if the given dtype object is a special type.  Example:

          >>> out = h5py.check_dtype(vlen=mydtype)
          >>> if out is not None:
          ...     print("Vlen of type %s" % out)
          str


     Parameters:

        * ‘vlen’ – Check for an HDF5 variable-length type; returns base
          class

        * ‘enum’ – Check for an enumerated type; returns 2-tuple
          ‘(basetype, values_dict)’.

        * ‘ref’ – Check for an HDF5 object or region reference; returns
          either ‘h5py.Reference’ or ‘h5py.RegionReference’.


File: h5py.info,  Node: Strings in HDF5,  Next: Object and Region References,  Prev: Special types,  Up: Advanced topics

5.3 Strings in HDF5
===================

     Note: The rules around reading & writing string data were
     redesigned for h5py 3.0.  Refer to the h5py 2.10 docs(1) for how to
     store strings in older versions.

* Menu:

* Reading strings::
* Storing strings::
* How to store raw binary data::
* Object names::
* Encodings::

   ---------- Footnotes ----------

   (1) https://docs.h5py.org/en/2.10.0/strings.html


File: h5py.info,  Node: Reading strings,  Next: Storing strings,  Up: Strings in HDF5

5.3.1 Reading strings
---------------------

String data in HDF5 datasets is read as bytes by default: ‘bytes’
objects for variable-length strings, or numpy bytes arrays (‘'S'’
dtypes) for fixed-length strings.  Use *note Dataset.asstr(): a4. to
retrieve ‘str’ objects.

Variable-length strings in attributes are read as ‘str’ objects.  These
are decoded as UTF-8 with surrogate escaping for unrecognised bytes.


File: h5py.info,  Node: Storing strings,  Next: How to store raw binary data,  Prev: Reading strings,  Up: Strings in HDF5

5.3.2 Storing strings
---------------------

When creating a new dataset or attribute, Python ‘str’ or ‘bytes’
objects will be treated as variable-length strings, marked as UTF-8 and
ASCII respectively.  Numpy bytes arrays (‘'S'’ dtypes) make fixed-length
strings.  You can use *note string_dtype(): e0. to explictly specify any
HDF5 string datatype.

When writing data to an existing dataset or attribute, data passed as
bytes is written without checking the encoding.  Data passed as Python
‘str’ objects is encoded as either ASCII or UTF-8, based on the HDF5
datatype.  In either case, null bytes (‘'\x00'’) in the data will cause
an error.

     Warning: Fixed-length string datasets will silently truncate longer
     strings which are written to them.  Numpy byte string arrays do the
     same thing.

     Fixed-length strings in HDF5 hold a set number of bytes.  It may
     take multiple bytes to store one character.

* Menu:

* What about NumPy’s U type?::


File: h5py.info,  Node: What about NumPy’s U type?,  Up: Storing strings

5.3.2.1 What about NumPy’s ‘U’ type?
....................................

NumPy also has a Unicode type, a UTF-32 fixed-width format (4-byte
characters).  HDF5 has no support for wide characters.  Rather than
trying to hack around this and “pretend” to support it, h5py will raise
an error if you try to store data of this type.


File: h5py.info,  Node: How to store raw binary data,  Next: Object names,  Prev: Storing strings,  Up: Strings in HDF5

5.3.3 How to store raw binary data
----------------------------------

If you have a non-text blob in a Python byte string (as opposed to ASCII
or UTF-8 encoded text, which is fine), you should wrap it in a ‘void’
type for storage.  This will map to the HDF5 OPAQUE datatype, and will
prevent your blob from getting mangled by the string machinery.

Here’s an example of how to store binary data in an attribute, and then
recover it:

     >>> binary_blob = b"Hello\x00Hello\x00"
     >>> dset.attrs["attribute_name"] = np.void(binary_blob)
     >>> out = dset.attrs["attribute_name"]
     >>> binary_blob = out.tobytes()


File: h5py.info,  Node: Object names,  Next: Encodings,  Prev: How to store raw binary data,  Up: Strings in HDF5

5.3.4 Object names
------------------

Unicode strings are used exclusively for object names in the file:

     >>> f.name
     '/'

You can supply either byte or unicode strings when creating or
retrieving objects.  If a byte string is supplied, it will be used
as-is; Unicode strings will be encoded as UTF-8.

In the file, h5py uses the most-compatible representation;
H5T_CSET_ASCII for characters in the ASCII range; H5T_CSET_UTF8
otherwise.

     >>> grp = f.create_dataset(b"name")
     >>> grp2 = f.create_dataset("name2")


File: h5py.info,  Node: Encodings,  Prev: Object names,  Up: Strings in HDF5

5.3.5 Encodings
---------------

HDF5 supports two string encodings: ASCII and UTF-8.  We recommend using
UTF-8 when creating HDF5 files, and this is what h5py does by default
with Python ‘str’ objects.  If you need to write ASCII for compatibility
reasons, you should ensure you only write pure ASCII characters (this
can be done by ‘your_string.encode("ascii")’), as otherwise your text
may turn into mojibake(1).  You can use *note string_dtype(): e0. to
specify the encoding for string data.

See also
........

Joel Spolsky’s introduction to Unicode & character sets(2)

     If this section looks like gibberish, try this.

For reading, as long as the encoding metadata is correct, the defaults
for *note Dataset.asstr(): a4. will always work.  However, HDF5 does not
enforce the string encoding, and there are files where the encoding
metadata doesn’t match what’s really stored.  Most commonly, data marked
as ASCII may be in one of the many “Extended ASCII” encodings such as
Latin-1.  If you know what encoding your data is in, you can specify
this using *note Dataset.asstr(): a4.  If you have data in an unknown
encoding, you can also use any of the builtin python error handlers(3).

Variable-length strings in attributes are read as ‘str’ objects, decoded
as UTF-8 with the ‘'surrogateescape'’ error handler.  If an attribute is
incorrectly encoded, you’ll see ‘surrogate’ characters such as
‘'\udcb1'’ when reading it:

     >>> s = "2.0±0.1"
     >>> f.attrs["string_good"] = s  # Good - h5py uses UTF-8
     >>> f.attrs["string_bad"] = s.encode("latin-1")  # Bad!
     >>> f.attrs["string_bad"]
     '2.0\udcb10.1'

To recover the original string, you’ll need to `encode' it with UTF-8,
and then decode it with the correct encoding:

     >>> f.attrs["string_bad"].encode('utf-8', 'surrogateescape').decode('latin-1')
     '2.0±0.1'

Fixed length strings are different; h5py doesn’t try to decode them:

     >>> s = "2.0±0.1"
     >>> utf8_type = h5py.string_dtype('utf-8', 30)
     >>> ascii_type = h5py.string_dtype('ascii', 30)
     >>> f.attrs["fixed_good"] = np.array(s.encode("utf-8"), dtype=utf8_type)
     >>> f.attrs["fixed_bad"] = np.array(s.encode("latin-1"), dtype=ascii_type)
     >>> f.attrs["fixed_bad"]
     b'2.0\xb10.1'
     >>> f.attrs["fixed_bad"].decode("utf-8")
     Traceback (most recent call last):
       File "<input>", line 1, in <module>
         f.attrs["fixed_bad"].decode("utf-8")
     UnicodeDecodeError: 'utf-8' codec can't decode byte 0xb1 in position 3: invalid start byte
     >>> f.attrs["fixed_bad"].decode("latin-1")
     '2.0±0.1'

As we get bytes back, we only need to decode them with the correct
encoding.

   ---------- Footnotes ----------

   (1) https://en.wikipedia.org/wiki/Mojibake

   (2) 
https://www.joelonsoftware.com/2003/10/08/the-absolute-minimum-every-software-developer-absolutely-positively-must-know-about-unicode-and-character-sets-no-excuses/

   (3) https://docs.python.org/3/library/codecs.html#error-handlers


File: h5py.info,  Node: Object and Region References,  Next: Parallel HDF5,  Prev: Strings in HDF5,  Up: Advanced topics

5.4 Object and Region References
================================

In addition to soft and external links, HDF5 supplies one more mechanism
to refer to objects and data in a file.  HDF5 `references' are low-level
pointers to other objects.  The great advantage of references is that
they can be stored and retrieved as data; you can create an attribute or
an entire dataset of reference type.

References come in two flavors, object references and region references.
As the name suggests, object references point to a particular object in
a file, either a dataset, group or named datatype.  Region references
always point to a dataset, and additionally contain information about a
certain selection (`dataset region') on that dataset.  For example, if
you have a dataset representing an image, you could specify a region of
interest, and store it as an attribute on the dataset.

* Menu:

* Using object references::
* Using region references::
* Storing references in a dataset::
* Storing references in an attribute::
* Null references::


File: h5py.info,  Node: Using object references,  Next: Using region references,  Up: Object and Region References

5.4.1 Using object references
-----------------------------

It’s trivial to create a new object reference; every high-level object
in h5py has a read-only property “ref”, which when accessed returns a
new object reference:

     >>> myfile = h5py.File('myfile.hdf5')
     >>> mygroup = myfile['/some/group']
     >>> ref = mygroup.ref
     >>> print(ref)
     <HDF5 object reference>

“Dereferencing” these objects is straightforward; use the same syntax as
when opening any other object:

     >>> mygroup2 = myfile[ref]
     >>> print(mygroup2)
     <HDF5 group "/some/group" (0 members)>


File: h5py.info,  Node: Using region references,  Next: Storing references in a dataset,  Prev: Using object references,  Up: Object and Region References

5.4.2 Using region references
-----------------------------

Region references always contain a selection.  You create them using the
dataset property “regionref” and standard NumPy slicing syntax:

     >>> myds = myfile.create_dataset('dset', (200,200))
     >>> regref = myds.regionref[0:10, 0:5]
     >>> print(regref)
     <HDF5 region reference>

The reference itself can now be used in place of slicing arguments to
the dataset:

     >>> subset = myds[regref]

For selections which don’t conform to a regular grid, h5py copies the
behavior of NumPy’s fancy indexing, which returns a 1D array.  Note that
for h5py release before 2.2, h5py always returns a 1D array.

In addition to storing a selection, region references inherit from
object references, and can be used anywhere an object reference is
accepted.  In this case the object they point to is the dataset used to
create them.


File: h5py.info,  Node: Storing references in a dataset,  Next: Storing references in an attribute,  Prev: Using region references,  Up: Object and Region References

5.4.3 Storing references in a dataset
-------------------------------------

HDF5 treats object and region references as data.  Consequently, there
is a special HDF5 type to represent them.  However, NumPy has no
equivalent type.  Rather than implement a special “reference type” for
NumPy, references are handled at the Python layer as plain, ordinary
python objects.  To NumPy they are represented with the “object” dtype
(kind ‘O’).  A small amount of metadata attached to the dtype tells h5py
to interpret the data as containing reference objects.

These dtypes are available from h5py for references and region
references:

   * ‘h5py.ref_dtype’ - for object references

   * ‘h5py.regionref_dtype’ - for region references

To store an array of references, use the appropriate dtype when creating
the dataset:

     >>> ref_dataset = myfile.create_dataset("MyRefs", (100,), dtype=h5py.ref_dtype)

You can read from and write to the array as normal:

     >>> ref_dataset[0] = myfile.ref
     >>> print(ref_dataset[0])
     <HDF5 object reference>


File: h5py.info,  Node: Storing references in an attribute,  Next: Null references,  Prev: Storing references in a dataset,  Up: Object and Region References

5.4.4 Storing references in an attribute
----------------------------------------

Simply assign the reference to a name; h5py will figure it out and store
it with the correct type:

     >>> myref = myfile.ref
     >>> myfile.attrs["Root group reference"] = myref


File: h5py.info,  Node: Null references,  Prev: Storing references in an attribute,  Up: Object and Region References

5.4.5 Null references
---------------------

When you create a dataset of reference type, the uninitialized elements
are “null” references.  H5py uses the truth value of a reference object
to indicate whether or not it is null:

     >>> print(bool(myfile.ref))
     True
     >>> nullref = ref_dataset[50]
     >>> print(bool(nullref))
     False


File: h5py.info,  Node: Parallel HDF5,  Next: Single Writer Multiple Reader SWMR,  Prev: Object and Region References,  Up: Advanced topics

5.5 Parallel HDF5
=================

Read-only parallel access to HDF5 files works with no special
preparation: each process should open the file independently and read
data normally (avoid opening the file and then forking).

Parallel HDF5(1) is a feature built on MPI which also supports `writing'
an HDF5 file in parallel.  To use this, both HDF5 and h5py must be
compiled with MPI support turned on, as described below.

* Menu:

* How does Parallel HDF5 work?::
* Building against Parallel HDF5: Building against Parallel HDF5<2>.
* Using Parallel HDF5 from h5py::
* Collective versus independent operations::
* MPI atomic mode::
* More information::

   ---------- Footnotes ----------

   (1) https://portal.hdfgroup.org/display/HDF5/Parallel+HDF5


File: h5py.info,  Node: How does Parallel HDF5 work?,  Next: Building against Parallel HDF5<2>,  Up: Parallel HDF5

5.5.1 How does Parallel HDF5 work?
----------------------------------

Parallel HDF5 is a configuration of the HDF5 library which lets you
share open files across multiple parallel processes.  It uses the MPI
(Message Passing Interface) standard for interprocess communication.
Consequently, when using Parallel HDF5 from Python, your application
will also have to use the MPI library.

This is accomplished through the mpi4py(1) Python package, which
provides excellent, complete Python bindings for MPI. Here’s an example
“Hello World” using ‘mpi4py’:

     >>> from mpi4py import MPI
     >>> print("Hello World (from process %d)" % MPI.COMM_WORLD.Get_rank())

To run an MPI-based parallel program, use the ‘mpiexec’ program to
launch several parallel instances of Python:

     $ mpiexec -n 4 python demo.py
     Hello World (from process 1)
     Hello World (from process 2)
     Hello World (from process 3)
     Hello World (from process 0)

The ‘mpi4py’ package includes all kinds of mechanisms to share data
between processes, synchronize, etc.  It’s a different flavor of
parallelism than, say, threads or ‘multiprocessing’, but easy to get
used to.

Check out the mpi4py web site(2) for more information and a great
tutorial.

   ---------- Footnotes ----------

   (1) http://mpi4py.scipy.org/

   (2) http://mpi4py.scipy.org/


File: h5py.info,  Node: Building against Parallel HDF5<2>,  Next: Using Parallel HDF5 from h5py,  Prev: How does Parallel HDF5 work?,  Up: Parallel HDF5

5.5.2 Building against Parallel HDF5
------------------------------------

HDF5 must be built with at least the following options:

     $./configure --enable-parallel --enable-shared

Note that ‘--enable-shared’ is required.

Often, a “parallel” version of HDF5 will be available through your
package manager.  You can check to see what build options were used by
using the program ‘h5cc’:

     $ h5cc -showconfig

Once you’ve got a Parallel-enabled build of HDF5, h5py has to be
compiled in “MPI mode”.  Set your default compiler to the ‘mpicc’
wrapper and build h5py with the ‘HDF5_MPI’ environment variable:

     $ export CC=mpicc
     $ export HDF5_MPI="ON"
     $ export HDF5_DIR="/path/to/parallel/hdf5"  # If this isn't found by default
     $ pip install .


File: h5py.info,  Node: Using Parallel HDF5 from h5py,  Next: Collective versus independent operations,  Prev: Building against Parallel HDF5<2>,  Up: Parallel HDF5

5.5.3 Using Parallel HDF5 from h5py
-----------------------------------

The parallel features of HDF5 are mostly transparent.  To open a file
shared across multiple processes, use the ‘mpio’ file driver.  Here’s an
example program which opens a file, creates a single dataset and fills
it with the process ID:

     from mpi4py import MPI
     import h5py

     rank = MPI.COMM_WORLD.rank  # The process ID (integer 0-3 for 4-process run)

     f = h5py.File('parallel_test.hdf5', 'w', driver='mpio', comm=MPI.COMM_WORLD)

     dset = f.create_dataset('test', (4,), dtype='i')
     dset[rank] = rank

     f.close()

Run the program:

     $ mpiexec -n 4 python demo2.py

Looking at the file with ‘h5dump’:

     $ h5dump parallel_test.hdf5
     HDF5 "parallel_test.hdf5" {
     GROUP "/" {
        DATASET "test" {
           DATATYPE  H5T_STD_I32LE
           DATASPACE  SIMPLE { ( 4 ) / ( 4 ) }
           DATA {
           (0): 0, 1, 2, 3
           }
        }
     }
     }


File: h5py.info,  Node: Collective versus independent operations,  Next: MPI atomic mode,  Prev: Using Parallel HDF5 from h5py,  Up: Parallel HDF5

5.5.4 Collective versus independent operations
----------------------------------------------

MPI-based programs work by launching many instances of the Python
interpreter, each of which runs your script.  There are certain
requirements imposed on what each process can do.  Certain operations in
HDF5, for example, anything which modifies the file metadata, must be
performed by all processes.  Other operations, for example, writing data
to a dataset, can be performed by some processes and not others.

These two classes are called `collective' and `independent' operations.
Anything which modifies the `structure' or metadata of a file must be
done collectively.  For example, when creating a group, each process
must participate:

     >>> grp = f.create_group('x')  # right

     >>> if rank == 1:
     ...     grp = f.create_group('x')   # wrong; all processes must do this

On the other hand, writing data to a dataset can be done independently:

     >>> if rank > 2:
     ...     dset[rank] = 42   # this is fine


File: h5py.info,  Node: MPI atomic mode,  Next: More information,  Prev: Collective versus independent operations,  Up: Parallel HDF5

5.5.5 MPI atomic mode
---------------------

HDF5 versions 1.8.9+ support the MPI “atomic” file access mode, which
trades speed for more stringent consistency requirements.  Once you’ve
opened a file with the ‘mpio’ driver, you can place it in atomic mode
using the settable ‘atomic’ property:

     >>> f = h5py.File('parallel_test.hdf5', 'w', driver='mpio', comm=MPI.COMM_WORLD)
     >>> f.atomic = True


File: h5py.info,  Node: More information,  Prev: MPI atomic mode,  Up: Parallel HDF5

5.5.6 More information
----------------------

Parallel HDF5 is a new feature in h5py.  If you have any questions, feel
free to ask on the mailing list (h5py at google groups).  We welcome bug
reports, enhancements and general inquiries.


File: h5py.info,  Node: Single Writer Multiple Reader SWMR,  Next: Virtual Datasets VDS,  Prev: Parallel HDF5,  Up: Advanced topics

5.6 Single Writer Multiple Reader (SWMR)
========================================

Starting with version 2.5.0, h5py includes support for the HDF5 SWMR
features.

* Menu:

* What is SWMR?::
* Using the SWMR feature from h5py::
* Examples::


File: h5py.info,  Node: What is SWMR?,  Next: Using the SWMR feature from h5py,  Up: Single Writer Multiple Reader SWMR

5.6.1 What is SWMR?
-------------------

The SWMR features allow simple concurrent reading of a HDF5 file while
it is being written from another process.  Prior to this feature
addition it was not possible to do this as the file data and meta-data
would not be synchronised and attempts to read a file which was open for
writing would fail or result in garbage data.

A file which is being written to in SWMR mode is guaranteed to always be
in a valid (non-corrupt) state for reading.  This has the added benefit
of leaving a file in a valid state even if the writing application
crashes before closing the file properly.

This feature has been implemented to work with independent writer and
reader processes.  No synchronisation is required between processes and
it is up to the user to implement either a file polling mechanism,
inotify or any other IPC mechanism to notify when data has been written.

The SWMR functionality requires use of the latest HDF5 file format:
v110.  In practice this implies using at least HDF5 1.10 (this can be
checked via ‘h5py.version.info’) and setting the libver bounding to
“latest” when opening or creating the file.

     Warning: New v110 format files are `not' compatible with v18
     format.  So, files written in SWMR mode with libver=’latest’ cannot
     be opened with older versions of the HDF5 library (basically any
     version older than the SWMR feature).

The HDF Group has documented the SWMR features in details on the
website: Single-Writer/Multiple-Reader (SWMR) Documentation(1).  This is
highly recommended reading for anyone intending to use the SWMR feature
even through h5py.  For production systems in particular pay attention
to the file system requirements regarding POSIX I/O semantics.

   ---------- Footnotes ----------

   (1) 
https://support.hdfgroup.org/HDF5/docNewFeatures/NewFeaturesSwmrDocs.html


File: h5py.info,  Node: Using the SWMR feature from h5py,  Next: Examples,  Prev: What is SWMR?,  Up: Single Writer Multiple Reader SWMR

5.6.2 Using the SWMR feature from h5py
--------------------------------------

The following basic steps are typically required by writer and reader
processes:

   - Writer process creates the target file and all groups, datasets and
     attributes.

   - Writer process switches file into SWMR mode.

   - Reader process can open the file with swmr=True.

   - Writer writes and/or appends data to existing datasets (new groups
     and datasets `cannot' be created when in SWMR mode).

   - Writer regularly flushes the target dataset to make it visible to
     reader processes.

   - Reader refreshes target dataset before reading new meta-data and/or
     main data.

   - Writer eventually completes and close the file as normal.

   - Reader can finish and close file as normal whenever it is
     convenient.

The following snippet demonstrate a SWMR writer appending to a single
dataset:

     f = h5py.File("swmr.h5", 'w', libver='latest')
     arr = np.array([1,2,3,4])
     dset = f.create_dataset("data", chunks=(2,), maxshape=(None,), data=arr)
     f.swmr_mode = True
     # Now it is safe for the reader to open the swmr.h5 file
     for i in range(5):
         new_shape = ((i+1) * len(arr), )
         dset.resize( new_shape )
         dset[i*len(arr):] = arr
         dset.flush()
         # Notify the reader process that new data has been written

The following snippet demonstrate how to monitor a dataset as a SWMR
reader:

     f = h5py.File("swmr.h5", 'r', libver='latest', swmr=True)
     dset = f["data"]
     while True:
         dset.id.refresh()
         shape = dset.shape
         print( shape )


File: h5py.info,  Node: Examples,  Prev: Using the SWMR feature from h5py,  Up: Single Writer Multiple Reader SWMR

5.6.3 Examples
--------------

In addition to the above example snippets, a few more complete examples
can be found in the examples folder.  These examples are described in
the following sections.

* Menu:

* Dataset monitor with inotify::
* Multiprocess concurrent write and read::


File: h5py.info,  Node: Dataset monitor with inotify,  Next: Multiprocess concurrent write and read,  Up: Examples

5.6.3.1 Dataset monitor with inotify
....................................

The inotify example demonstrates how to use SWMR in a reading
application which monitors live progress as a dataset is being written
by another process.  This example uses the the linux inotify
(pyinotify(1) python bindings) to receive a signal each time the target
file has been updated.


     """
         Demonstrate the use of h5py in SWMR mode to monitor the growth of a dataset
         on notification of file modifications.

         This demo uses pyinotify as a wrapper of Linux inotify.
         https://pypi.python.org/pypi/pyinotify

         Usage:
                 swmr_inotify_example.py [FILENAME [DATASETNAME]]

                   FILENAME:    name of file to monitor. Default: swmr.h5
                   DATASETNAME: name of dataset to monitor in DATAFILE. Default: data

         This script will open the file in SWMR mode and monitor the shape of the
         dataset on every write event (from inotify). If another application is
         concurrently writing data to the file, the writer must have have switched
         the file into SWMR mode before this script can open the file.
     """
     import asyncore
     import pyinotify
     import sys
     import h5py
     import logging

     #assert h5py.version.hdf5_version_tuple >= (1,9,178), "SWMR requires HDF5 version >= 1.9.178"

     class EventHandler(pyinotify.ProcessEvent):

         def monitor_dataset(self, filename, datasetname):
             logging.info("Opening file %s", filename)
             self.f = h5py.File(filename, 'r', libver='latest', swmr=True)
             logging.debug("Looking up dataset %s"%datasetname)
             self.dset = self.f[datasetname]

             self.get_dset_shape()

         def get_dset_shape(self):
             logging.debug("Refreshing dataset")
             self.dset.refresh()

             logging.debug("Getting shape")
             shape = self.dset.shape
             logging.info("Read data shape: %s"%str(shape))
             return shape

         def read_dataset(self, latest):
             logging.info("Reading out dataset [%d]"%latest)
             self.dset[latest:]

         def process_IN_MODIFY(self, event):
             logging.debug("File modified!")
             shape = self.get_dset_shape()
             self.read_dataset(shape[0])

         def process_IN_CLOSE_WRITE(self, event):
             logging.info("File writer closed file")
             self.get_dset_shape()
             logging.debug("Good bye!")
             sys.exit(0)


     if __name__ == "__main__":
         logging.basicConfig(format='%(asctime)s  %(levelname)s\t%(message)s',level=logging.INFO)

         file_name = "swmr.h5"
         if len(sys.argv) > 1:
             file_name = sys.argv[1]
         dataset_name = "data"
         if len(sys.argv) > 2:
             dataset_name = sys.argv[2]


         wm = pyinotify.WatchManager()  # Watch Manager
         mask = pyinotify.IN_MODIFY | pyinotify.IN_CLOSE_WRITE
         evh = EventHandler()
         evh.monitor_dataset( file_name, dataset_name )

         notifier = pyinotify.AsyncNotifier(wm, evh)
         wdd = wm.add_watch(file_name, mask, rec=False)

         # Sit in this loop() until the file writer closes the file
         # or the user hits ctrl-c
         asyncore.loop()

   ---------- Footnotes ----------

   (1) https://pypi.python.org/pypi/pyinotify


File: h5py.info,  Node: Multiprocess concurrent write and read,  Prev: Dataset monitor with inotify,  Up: Examples

5.6.3.2 Multiprocess concurrent write and read
..............................................

The SWMR multiprocess example starts two concurrent child processes: a
writer and a reader.  The writer process first creates the target file
and dataset.  Then it switches the file into SWMR mode and the reader
process is notified (with a multiprocessing.Event) that it is safe to
open the file for reading.

The writer process then continue to append chunks to the dataset.  After
each write it notifies the reader that new data has been written.
Whether the new data is visible in the file at this point is subject to
OS and file system latencies.

The reader first waits for the initial “SWMR mode” notification from the
writer, upon which it goes into a loop where it waits for further
notifications from the writer.  The reader may drop some notifications,
but for each one received it will refresh the dataset and read the
dimensions.  After a time-out it will drop out of the loop and exit.

     """
         Demonstrate the use of h5py in SWMR mode to write to a dataset (appending)
         from one process while monitoring the growing dataset from another process.

         Usage:
                 swmr_multiprocess.py [FILENAME [DATASETNAME]]

                   FILENAME:    name of file to monitor. Default: swmrmp.h5
                   DATASETNAME: name of dataset to monitor in DATAFILE. Default: data

         This script will start up two processes: a writer and a reader. The writer
         will open/create the file (FILENAME) in SWMR mode, create a dataset and start
         appending data to it. After each append the dataset is flushed and an event
         sent to the reader process. Meanwhile the reader process will wait for events
         from the writer and when triggered it will refresh the dataset and read the
         current shape of it.
     """

     import sys
     import h5py
     import numpy as np
     import logging
     from multiprocessing import Process, Event

     class SwmrReader(Process):
         def __init__(self, event, fname, dsetname, timeout = 2.0):
             super(SwmrReader, self).__init__()
             self._event = event
             self._fname = fname
             self._dsetname = dsetname
             self._timeout = timeout

         def run(self):
             self.log = logging.getLogger('reader')
             self.log.info("Waiting for initial event")
             assert self._event.wait( self._timeout )
             self._event.clear()

             self.log.info("Opening file %s", self._fname)
             f = h5py.File(self._fname, 'r', libver='latest', swmr=True)
             assert f.swmr_mode
             dset = f[self._dsetname]
             try:
                 # monitor and read loop
                 while self._event.wait( self._timeout ):
                     self._event.clear()
                     self.log.debug("Refreshing dataset")
                     dset.refresh()

                     shape = dset.shape
                     self.log.info("Read dset shape: %s"%str(shape))
             finally:
                 f.close()

     class SwmrWriter(Process):
         def __init__(self, event, fname, dsetname):
             super(SwmrWriter, self).__init__()
             self._event = event
             self._fname = fname
             self._dsetname = dsetname

         def run(self):
             self.log = logging.getLogger('writer')
             self.log.info("Creating file %s", self._fname)
             f = h5py.File(self._fname, 'w', libver='latest')
             try:
                 arr = np.array([1,2,3,4])
                 dset = f.create_dataset(self._dsetname, chunks=(2,), maxshape=(None,), data=arr)
                 assert not f.swmr_mode

                 self.log.info("SWMR mode")
                 f.swmr_mode = True
                 assert f.swmr_mode
                 self.log.debug("Sending initial event")
                 self._event.set()

                 # Write loop
                 for i in range(5):
                     new_shape = ((i+1) * len(arr), )
                     self.log.info("Resizing dset shape: %s"%str(new_shape))
                     dset.resize( new_shape )
                     self.log.debug("Writing data")
                     dset[i*len(arr):] = arr
                     #dset.write_direct( arr, np.s_[:], np.s_[i*len(arr):] )
                     self.log.debug("Flushing data")
                     dset.flush()
                     self.log.info("Sending event")
                     self._event.set()
             finally:
                 f.close()


     if __name__ == "__main__":
         logging.basicConfig(format='%(levelname)10s  %(asctime)s  %(name)10s  %(message)s',level=logging.INFO)
         fname = 'swmrmp.h5'
         dsetname = 'data'
         if len(sys.argv) > 1:
             fname = sys.argv[1]
         if len(sys.argv) > 2:
             dsetname = sys.argv[2]

         event = Event()
         reader = SwmrReader(event, fname, dsetname)
         writer = SwmrWriter(event, fname, dsetname)

         logging.info("Starting reader")
         reader.start()
         logging.info("Starting reader")
         writer.start()

         logging.info("Waiting for writer to finish")
         writer.join()
         logging.info("Waiting for reader to finish")
         reader.join()

The example output below (from a virtual Ubuntu machine) illustrate some
latency between the writer and reader:

     python examples/swmr_multiprocess.py
       INFO  2015-02-26 18:05:03,195        root  Starting reader
       INFO  2015-02-26 18:05:03,196        root  Starting reader
       INFO  2015-02-26 18:05:03,197      reader  Waiting for initial event
       INFO  2015-02-26 18:05:03,197        root  Waiting for writer to finish
       INFO  2015-02-26 18:05:03,198      writer  Creating file swmrmp.h5
       INFO  2015-02-26 18:05:03,203      writer  SWMR mode
       INFO  2015-02-26 18:05:03,205      reader  Opening file swmrmp.h5
       INFO  2015-02-26 18:05:03,210      writer  Resizing dset shape: (4,)
       INFO  2015-02-26 18:05:03,212      writer  Sending event
       INFO  2015-02-26 18:05:03,213      reader  Read dset shape: (4,)
       INFO  2015-02-26 18:05:03,214      writer  Resizing dset shape: (8,)
       INFO  2015-02-26 18:05:03,214      writer  Sending event
       INFO  2015-02-26 18:05:03,215      writer  Resizing dset shape: (12,)
       INFO  2015-02-26 18:05:03,215      writer  Sending event
       INFO  2015-02-26 18:05:03,215      writer  Resizing dset shape: (16,)
       INFO  2015-02-26 18:05:03,215      reader  Read dset shape: (12,)
       INFO  2015-02-26 18:05:03,216      writer  Sending event
       INFO  2015-02-26 18:05:03,216      writer  Resizing dset shape: (20,)
       INFO  2015-02-26 18:05:03,216      reader  Read dset shape: (16,)
       INFO  2015-02-26 18:05:03,217      writer  Sending event
       INFO  2015-02-26 18:05:03,217      reader  Read dset shape: (20,)
       INFO  2015-02-26 18:05:03,218      reader  Read dset shape: (20,)
       INFO  2015-02-26 18:05:03,219        root  Waiting for reader to finish


File: h5py.info,  Node: Virtual Datasets VDS,  Prev: Single Writer Multiple Reader SWMR,  Up: Advanced topics

5.7 Virtual Datasets (VDS)
==========================

Starting with version 2.9, h5py includes high-level support for HDF5
‘virtual datasets’.  The VDS feature is available in version 1.10 of the
HDF5 library; h5py must be built with a new enough version of HDF5 to
create or read virtual datasets.

* Menu:

* What are virtual datasets?::
* Creating virtual datasets in h5py::
* Examples: Examples<2>.
* Reference: Reference<5>.


File: h5py.info,  Node: What are virtual datasets?,  Next: Creating virtual datasets in h5py,  Up: Virtual Datasets VDS

5.7.1 What are virtual datasets?
--------------------------------

Virtual datasets allow a number of real datasets to be mapped together
into a single, sliceable dataset via an interface layer.  The mapping
can be made ahead of time, before the parent files are written, and is
transparent to the parent dataset characteristics (SWMR, chunking,
compression etc…).  The datasets can be meshed in arbitrary
combinations, and even the data type converted.

Once a virtual dataset has been created, it can be read just like any
other HDF5 dataset.

     Warning: Virtual dataset files cannot be opened with versions of
     the hdf5 library older than 1.10.

The HDF Group has documented the VDS features in detail on the website:
Virtual Datasets (VDS) Documentation(1).

   ---------- Footnotes ----------

   (1) 
https://support.hdfgroup.org/HDF5/docNewFeatures/NewFeaturesVirtualDatasetDocs.html


File: h5py.info,  Node: Creating virtual datasets in h5py,  Next: Examples<2>,  Prev: What are virtual datasets?,  Up: Virtual Datasets VDS

5.7.2 Creating virtual datasets in h5py
---------------------------------------

To make a virtual dataset using h5py, you need to:

  1. Create a *note VirtualLayout: 75. object representing the
     dimensions and data type of the virtual dataset.

  2. Create a number of *note VirtualSource: 119. objects, representing
     the datasets the array will be built from.  These objects can be
     created either from an h5py *note Dataset: 4e, or from a filename,
     dataset name and shape.  This can be done even before the source
     file exists.

  3. Map slices from the sources into the layout.

  4. Convert the *note VirtualLayout: 75. object into a virtual dataset
     in an HDF5 file.

The following snippet creates a virtual dataset to stack together four
1D datasets from separate files into a 2D dataset:

     layout = h5py.VirtualLayout(shape=(4, 100), dtype='i4')

     for n in range(1, 5):
         filename = "{}.h5".format(n)
         vsource = h5py.VirtualSource(filename, 'data', shape=(100,))
         layout[n - 1] = vsource

     # Add virtual dataset to output file
     with h5py.File("VDS.h5", 'w', libver='latest') as f:
         f.create_virtual_dataset('data', layout, fillvalue=-5)

This is an extract from the ‘vds_simple.py’ example in the examples
folder.

     Note: Slices up to ‘h5py.h5s.UNLIMITED’ can be used to create an
     unlimited selection along a single axis.  Resizing the source data
     along this axis will cause the virtual dataset to grow.  E.g.:

          layout[n - 1, :UNLIMITED] = vsource[:UNLIMITED]

     A normal slice with no defined end point (‘[:]’) is fixed based on
     the shape when you define it.

     New in version 3.0.


File: h5py.info,  Node: Examples<2>,  Next: Reference<5>,  Prev: Creating virtual datasets in h5py,  Up: Virtual Datasets VDS

5.7.3 Examples
--------------

In addition to the above example snippet, a few more complete examples
can be found in the examples folder:

   - vds_simple.py(1) is a self-contained, runnable example which
     creates four source files, and then maps them into a virtual
     dataset as shown above.

   - dataset_concatenation.py(2) illustrates virtually stacking datasets
     together along a new axis.

   - A number of examples are based on the sample use cases presented in
     the virtual datasets RFC(3):

        - excalibur_detector_modules.py(4)

        - dual_pco_edge.py(5)

        - eiger_use_case.py(6)

        - percival_use_case.py(7)

   ---------- Footnotes ----------

   (1) https://github.com/h5py/h5py/blob/master/examples/vds_simple.py

   (2) 
https://github.com/h5py/h5py/blob/master/examples/dataset_concatenation.py

   (3) 
https://support.hdfgroup.org/HDF5/docNewFeatures/VDS/HDF5-VDS-requirements-use-cases-2014-12-10.pdf

   (4) 
https://github.com/h5py/h5py/blob/master/examples/excalibur_detector_modules.py

   (5) 
https://github.com/h5py/h5py/blob/master/examples/dual_pco_edge.py

   (6) 
https://github.com/h5py/h5py/blob/master/examples/eiger_use_case.py

   (7) 
https://github.com/h5py/h5py/blob/master/examples/percival_use_case.py


File: h5py.info,  Node: Reference<5>,  Prev: Examples<2>,  Up: Virtual Datasets VDS

5.7.4 Reference
---------------

 -- Class: h5py.VirtualLayout (shape, dtype, maxshape=None)

     Object for building a virtual dataset.

     Instantiate this class to define a virtual dataset, assign *note
     VirtualSource: 119. objects to slices of it, and then pass it to
     *note Group.create_virtual_dataset(): 73. to add the virtual
     dataset to a file.

     This class does not allow access to the data; the virtual dataset
     must be created in a file before it can be used.


     Parameters:

        * ‘shape’ (‘tuple’) – The full shape of the virtual dataset.

        * ‘dtype’ – Numpy dtype or string.

        * ‘maxshape’ (‘tuple’) – The virtual dataset is resizable up to
          this shape.  Use None for axes you want to be unlimited.

 -- Class: h5py.VirtualSource (path_or_dataset, name=None, shape=None,
          dtype=None, maxshape=None)

     Source definition for virtual data sets.

     Instantiate this class to represent an entire source dataset, and
     then slice it to indicate which regions should be used in the
     virtual dataset.

     When *note creating a virtual dataset: 117, paths to sources
     present in the same file are changed to a “.”, refering to the
     current file (see H5Pset_virtual(1)).  This will keep such sources
     valid in case the file is renamed.


     Parameters:

        * ‘path_or_dataset’ – The path to a file, or a *note Dataset:
          4e. object.  If a dataset is given, no other parameters are
          allowed, as the relevant values are taken from the dataset
          instead.

        * ‘name’ (‘str’) – The name of the source dataset within the
          file.

        * ‘shape’ (‘tuple’) – The full shape of the source dataset.

        * ‘dtype’ – Numpy dtype or string.

        * ‘maxshape’ (‘tuple’) – The source dataset is resizable up to
          this shape.  Use None for axes you want to be unlimited.

   ---------- Footnotes ----------

   (1) https://portal.hdfgroup.org/display/HDF5/H5P_SET_VIRTUAL


File: h5py.info,  Node: Meta-info about the h5py project,  Next: Index,  Prev: Advanced topics,  Up: Top

6 Meta-info about the h5py project
**********************************

* Menu:

* “What’s new” documents::
* Bug Reports & Contributions::
* Release Guide::
* FAQ::
* Licenses and legal info::


File: h5py.info,  Node: “What’s new” documents,  Next: Bug Reports & Contributions,  Up: Meta-info about the h5py project

6.1 “What’s new” documents
==========================

These document the changes between minor (or major) versions of h5py.

* Menu:

* What’s new in h5py 3.1: What’s new in h5py 3 1.
* What’s new in h5py 3.0: What’s new in h5py 3 0.
* What’s new in h5py 2.10: What’s new in h5py 2 10.
* What’s new in h5py 2.9: What’s new in h5py 2 9.
* What’s new in h5py 2.8: What’s new in h5py 2 8.
* What’s new in h5py 2.7.1: What’s new in h5py 2 7 1.
* What’s new in h5py 2.7: What’s new in h5py 2 7.
* What’s new in h5py 2.6: What’s new in h5py 2 6.
* What’s new in h5py 2.5: What’s new in h5py 2 5.
* What’s new in h5py 2.4: What’s new in h5py 2 4.
* What’s new in h5py 2.3: What’s new in h5py 2 3.
* What’s new in h5py 2.2: What’s new in h5py 2 2.
* What’s new in h5py 2.1: What’s new in h5py 2 1.
* What’s new in h5py 2.0: What’s new in h5py 2 0.


File: h5py.info,  Node: What’s new in h5py 3 1,  Next: What’s new in h5py 3 0,  Up: “What’s new” documents

6.1.1 What’s new in h5py 3.1
----------------------------

* Menu:

* Bug fixes::
* Building h5py::
* Development::


File: h5py.info,  Node: Bug fixes,  Next: Building h5py,  Up: What’s new in h5py 3 1

6.1.1.1 Bug fixes
.................

   * Fix reading numeric data which is not in the native endianness,
     e.g.  big-endian data on a little-endian system (GH1729(1)).

   * Fix using bytes as names for *note Group.create_dataset(): 6c. and
     *note Group.create_virtual_dataset(): 73. (GH1732(2)).

   * Fix writing data as a list to a dataset with a sub-array data type
     (GH1735(3)).

   ---------- Footnotes ----------

   (1) https://github.com/h5py/h5py/issues/1729

   (2) https://github.com/h5py/h5py/issues/1732

   (3) https://github.com/h5py/h5py/issues/1735


File: h5py.info,  Node: Building h5py,  Next: Development,  Prev: Bug fixes,  Up: What’s new in h5py 3 1

6.1.1.2 Building h5py
.....................

   * Allow building against system lzf library by setting
     ‘H5PY_SYSTEM_LZF=1’.  See *note Custom installation: 1d.


File: h5py.info,  Node: Development,  Prev: Building h5py,  Up: What’s new in h5py 3 1

6.1.1.3 Development
...................

   * If pytest is missing pytest-mpi(1) it will now fail with a clear
     error.

   * Fix a test which was failing on big-endian systems.

   ---------- Footnotes ----------

   (1) https://pytest-mpi.readthedocs.io/en/latest/


File: h5py.info,  Node: What’s new in h5py 3 0,  Next: What’s new in h5py 2 10,  Prev: What’s new in h5py 3 1,  Up: “What’s new” documents

6.1.2 What’s new in h5py 3.0
----------------------------

* Menu:

* New features::
* Breaking changes & deprecations::
* Exposing HDF5 functions::
* Bug fixes: Bug fixes<2>.
* Building h5py: Building h5py<2>.
* Development: Development<2>.


File: h5py.info,  Node: New features,  Next: Breaking changes & deprecations,  Up: What’s new in h5py 3 0

6.1.2.1 New features
....................

   * The interface for storing & reading strings has changed - see *note
     Strings in HDF5: f4.  The new rules are hopefully more consistent,
     but may well require some changes in coding using h5py.

   * Reading & writing data now releases the GIL, so another Python
     thread can continue while HDF5 accesses data.  Where HDF5 can call
     back into Python, such as for data conversion, h5py re-acquires the
     GIL. However, HDF5 has its own global lock, so this won’t speed up
     parallel data access using multithreading.

   * Numpy datetime and timedelta arrays can now be stored and read as
     HDF5 opaque data (GH1339(1)), though other tools will not
     understand them.  See *note Storing other types as opaque data: ed.
     for more information.

   * New *note Dataset.iter_chunks(): a6. method, to iterate over chunks
     within the given selection.

   * Compatibility with HDF5 1.12.

   * Methods which accept a shape tuple, e.g.  to create a dataset, now
     also allow an integer for a 1D shape (PR 1340(2)).

   * Casting data to a specified type on reading (*note
     Dataset.astype(): a3.) can now be done without a with statement,
     like this:

          data = dset.astype(np.int32)[:]

   * A new *note Dataset.fields(): a5. method lets you read only
     selected fields from a dataset with a compound datatype.

   * Reading data has less overhead, as selection has been implemented
     in Cython.  Making many small reads from the same dataset can be as
     much as 10 times faster, but there are many factors that can affect
     performance.

   * A new NumPy-style *note Dataset.nbytes: ae. attribute to get the
     size of the dataset’s data in bytes.  This differs from the *note
     size: ad. attribute, which gives the number of elements.

   * The ‘external’ argument of *note Group.create_dataset(): 6c, which
     specifies any external storage for the dataset, accepts more types
     (GH1260(3)), as follows:

        * The top-level container may be any iterable, not only a list.

        * The names of external files may be not only ‘str’ but also
          ‘bytes’ or ‘os.PathLike’ objects.

        * The offsets and sizes may be NumPy integers as well as Python
          integers.

     See also the deprecation related to the ‘external’ argument.

   * Support for setting file space strategy at file creation.  Includes
     option to persist empty space tracking between sessions.  See *note
     File: d. for details.

   * More efficient writing when assiging a scalar to a chunked dataset,
     when the number of elements to write is no more than the size of
     one chunk.

   * Introduced support for the split *note file driver: 2c. (PR
     1468(4)).

   * Allow making virtual datasets which can grow as the source data is
     resized - see *note Virtual Datasets (VDS): 74.

   * New ‘allow_unknown_filter’ option to *note Group.create_dataset():
     6c.  This should only be used if you will compress the data before
     writing it with the low-level write_direct_chunk()(5) method.

   * The low-level chunk query API provides information about dataset
     chunks in an HDF5 file: get_num_chunks()(6), get_chunk_info()(7)
     and get_chunk_info_by_coord()(8).

   * The low-level h5py.h5f.FileID.get_vfd_handle()(9) method now works
     for any file driver that supports it, not only the sec2 driver.

   ---------- Footnotes ----------

   (1) https://github.com/h5py/h5py/issues/1339

   (2) https://github.com/h5py/h5py/pull/1340

   (3) https://github.com/h5py/h5py/issues/1260

   (4) https://github.com/h5py/h5py/pull/1468

   (5) 
https://api.h5py.org/h5d.html#h5py.h5d.DatasetID.write_direct_chunk

   (6) https://api.h5py.org/h5d.html#h5py.h5d.DatasetID.get_num_chunks

   (7) https://api.h5py.org/h5d.html#h5py.h5d.DatasetID.get_chunk_info

   (8) 
https://api.h5py.org/h5d.html#h5py.h5d.DatasetID.get_chunk_info_by_coord

   (9) https://api.h5py.org/h5f.html#h5py.h5f.FileID.get_vfd_handle


File: h5py.info,  Node: Breaking changes & deprecations,  Next: Exposing HDF5 functions,  Prev: New features,  Up: What’s new in h5py 3 0

6.1.2.2 Breaking changes & deprecations
.......................................

   * h5py now requires Python 3.6 or above; it is no longer compatible
     with Python 2.7.

   * The default mode for opening files is now ‘r’ (read-only).  See
     *note Opening & creating files: 2a. for other possible modes if you
     need to write to a file.

   * In previous versions, creating a dataset from a list of bytes
     objects would choose a fixed length string datatype to fit the
     biggest item.  It will now use a variable length string datatype.
     To store fixed length strings, use a suitable dtype from *note
     h5py.string_dtype(): e0.

   * Variable-length UTF-8 strings in datasets are now read as ‘bytes’
     objects instead of ‘str’ by default, for consistency with other
     kinds of strings.  See *note Strings in HDF5: f4. for more details.

   * When making a virtual dataset, a dtype must be specified in *note
     VirtualLayout: 75.  There is no longer a default dtype, as this was
     surprising in some cases.

   * The ‘external’ argument of ‘Group.create_dataset()’ no longer
     accepts the following forms (GH1260(1)):

        * a list containing `name', [`offset', [`size']];

        * a list containing `name1', `name2', …; and

        * a list containing tuples such as ‘(name,)’ and ‘(name,
          offset)’ that lack the offset or size.

     Furthermore, each `name'–`offset'–`size' triplet now must be a
     tuple rather than an arbitrary iterable.  See also the new feature
     related to the ‘external’ argument.

   * The MPI mode no longer supports mpi4py 1.x.

   * The deprecated ‘h5py.h5t.available_ftypes’ dictionary was removed.

   * The deprecated ‘Dataset.value’ property was removed.  Use ‘ds[()]’
     to read all data from any dataset.

   * The deprecated functions ‘new_vlen’, ‘new_enum’, ‘get_vlen’ and
     ‘get_enum’ have been removed.  See *note Special types: da. for the
     newer APIs.

   * Removed deprecated File.fid attribute.  Use *note File.id: 45.
     instead.

   * Remove the deprecated ‘h5py.highlevel’ module.  The high-level API
     is available directly in the ‘h5py’ module.

   * The third argument of ‘h5py._hl.selections.select()’ is now an
     optional high-level *note Dataset: 4e. object, rather than a
     ‘DatasetID’.  This is not really a public API - it has to be
     imported through the private ‘_hl’ module - but probably some
     people are using it anyway.

   ---------- Footnotes ----------

   (1) https://github.com/h5py/h5py/issues/1260


File: h5py.info,  Node: Exposing HDF5 functions,  Next: Bug fixes<2>,  Prev: Breaking changes & deprecations,  Up: What’s new in h5py 3 0

6.1.2.3 Exposing HDF5 functions
...............................

   * H5Dget_num_chunks

   * H5Dget_chunk_info

   * H5Dget_chunk_info_by_coord

   * H5Oget_info1

   * H5Oget_info_by_name1

   * H5Oget_info_by_idx1

   * H5Ovisit1

   * H5Ovisit_by_name1

   * H5Pset_attr_phase_change

   * H5Pset_fapl_split

   * H5Pget_file_space_strategy

   * H5Pset_file_space_strategy

   * H5Sencode1

   * H5Tget_create_plist


File: h5py.info,  Node: Bug fixes<2>,  Next: Building h5py<2>,  Prev: Exposing HDF5 functions,  Up: What’s new in h5py 3 0

6.1.2.4 Bug fixes
.................

   * Fix segmentation fault when accessing vlen of strings (GH1336(1)).

   * Fix the storage of non-contiguous arrays, such as numpy slices, as
     HDF5 vlen data (GH1649(2)).

   * Fix pathologically slow reading/writing in certain conditions with
     integer indexing (GH492(3)).

   * Fix bug when *note Group.copy(): 6a. source is a high-level object
     and destination is a Group (GH1005(4)).

   * Fix reading data for region references pointing to an empty
     selection.

   * Unregister converter functions at exit, preventing segfaults on
     exit in some situations with threads (PR 1440(5)).

   * As HDF5 1.10.6 and later support UTF-8 paths on Windows, h5py built
     against HDF5 1.10.6 will use UTF-8 for file names, allowing all
     filenames.

   * Fixed h5py.h5d.DatasetID.get_storage_size()(6) to report storage
     size of zero bytes without raising an exception (GH1475(7)).

   * Attribute Managers (‘obj.attrs’) can now work on HDF5 stored
     datatypes (GH1476(8)).

   * Remove broken inherited ‘ds.dims.values()’ and ‘ds.dims.items()’
     methods.  The dimensions interface behaves as a sequence, not a
     mapping (GH744(9)).

   * Fix creating attribute with ‘Empty’ by converting its dtype to a
     numpy dtype object.

   * Fix getting *note maxshape: a7. on empty/null datasets.

   * The *note File.swmr_mode: 47. property is always available
     (GH1580(10)).

   * The *note File.mode: 46. property handles SWMR access modes in
     addition to plain RDONLY/RDWR modes

   * Importing an MPI build of h5py no longer initialises MPI
     immediately, which will hopefully avoid various strange behaviours.

   * Avoid launching a subprocess by using ‘platform.machine()’ at
     import time.  This could trigger a warning in MPI.

   * Removed an equality comparison with an empty array, which will
     cause problems with future versions of numpy.

   * Better error message if you try to use the mpio driver and h5py was
     not built with MPI support.

   * Improved error messages when requesting chunked storage for an
     empty dataset.

   * Data conversion functions should fail more gracefully if no memory
     is available.

   * Fix some errors for internal functions that were raising
     “TypeError: expected bytes, str found” instead of the correct
     error.

   * Use relative path for virtual data sources if the source dataset is
     in the same file as the virtual dataset.

   * Generic exception types used in tests’ assertRaise (exception types
     changed in new HDF5 version)

   * Use ‘dtype=object’ in tests with ragged arrays

   ---------- Footnotes ----------

   (1) https://github.com/h5py/h5py/issues/1336

   (2) https://github.com/h5py/h5py/issues/1649

   (3) https://github.com/h5py/h5py/issues/492

   (4) https://github.com/h5py/h5py/issues/1005

   (5) https://github.com/h5py/h5py/pull/1440

   (6) https://api.h5py.org/h5d.html#h5py.h5d.DatasetID.get_storage_size

   (7) https://github.com/h5py/h5py/issues/1475

   (8) https://github.com/h5py/h5py/issues/1476

   (9) https://github.com/h5py/h5py/issues/744

   (10) https://github.com/h5py/h5py/issues/1580


File: h5py.info,  Node: Building h5py<2>,  Next: Development<2>,  Prev: Bug fixes<2>,  Up: What’s new in h5py 3 0

6.1.2.5 Building h5py
.....................

   * The ‘setup.py configure’ command was removed.  Configuration for
     the build can be specified with environment variables instead.  See
     *note Custom installation: 1d. for details.

   * It is now possible to specify separate include and library
     directories for HDF5 via environment variables.  See *note Custom
     installation: 1d. for more details.

   * The pkg-config name to use when looking up the HDF5 library can now
     be configured, this can assist with selecting the correct HDF5
     library when using MPI. See *note Custom installation: 1d. for more
     details.

   * Using bare ‘char*’ instead of ‘array.array’ in
     h5d.read_direct_chunk since ‘array.array’ is a private CPython
     C-API interface

   * Define ‘NPY_NO_DEPRECATED_API’ to silence a warning.

   * Make the lzf filter build with HDF5 1.10 (GH1219(1)).

   * If HDF5 is not loaded, an additional message is displayed to check
     HDF5 installation

   * Rely much more on the C-interface provided by Cython to call Python
     and NumPy.

   * Removed an old workaround which tried to run Cython in a subprocess
     if cythonize() didn’t work.  This shouldn’t be necessary for any
     recent version of setuptools.

   * Migrate all Cython code base to Cython3 syntax

             * The only noticeable change is in exception raising from
               cython which use bytes

             * Massively use local imports everywhere as expected from
               Python3

             * Explicitly mark several Cython functions as non-binding

   ---------- Footnotes ----------

   (1) https://github.com/h5py/h5py/issues/1219


File: h5py.info,  Node: Development<2>,  Prev: Building h5py<2>,  Up: What’s new in h5py 3 0

6.1.2.6 Development
...................

   * Unregistering converter functions on exit (PR 1440(1)) should allow
     profiling and code coverage tools to work on Cython code.

   ---------- Footnotes ----------

   (1) https://github.com/h5py/h5py/pull/1440


File: h5py.info,  Node: What’s new in h5py 2 10,  Next: What’s new in h5py 2 9,  Prev: What’s new in h5py 3 0,  Up: “What’s new” documents

6.1.3 What’s new in h5py 2.10
-----------------------------

* Menu:

* New features: New features<2>.
* Deprecations::
* Exposing HDF5 functions: Exposing HDF5 functions<2>.
* Bugfixes::
* Building h5py: Building h5py<3>.
* Development: Development<3>.


File: h5py.info,  Node: New features<2>,  Next: Deprecations,  Up: What’s new in h5py 2 10

6.1.3.1 New features
....................

   - HDF5 8-bit bitfield data can now be read either as uint8 or
     booleans (GH821(1)).  Pytables stores booleans as this type.  For
     now, you must pick which type to use explicitly:

          with dset.astype(numpy.uint8):   # or numpy.bool
              arr = dset[:]

   - Numpy arrays of integers can now be used for fancy indexing, where
     previously a Python list was required (GH963(2)).

   - Fancy indexing now allows an empty list or array (GH1174(3)).

   - IPython can now tab-complete names in h5py groups and attributes
     without any special user action (GH1228(4)).  This simple
     completion only matches the first level of keys in a group, not
     subkeys.  You can still call ‘h5py.enable_ipython_completion()’ for
     more complete results.

   - The ‘libver’ parameter for ‘File’ now accepts ‘'v108'’ and ‘'v110'’
     to specify compatibility with HDF5 1.8 or 1.10 (GH1155(5)).  See
     *note Version bounding: 32. for details.

   - New functions and constants for getting and identifying *note
     special data types: dc. - ‘string_dtype()’, ‘vlen_dtype()’,
     ‘enum_dtype()’, ‘ref_dtype’ and ‘regionref_dtype’ replace
     ‘special_dtype()’.  For identifying string, vlen and enum dtypes,
     ‘check_string_dtype()’, ‘check_vlen_dtype()’ and
     ‘check_enum_dtype()’ replace ‘check_dtype()’ (GH1132(6)).

   - A new method *note make_scale(): a8. to conveniently make a dataset
     into a *note dimension scale: a9. (GH830(7), GH1212(8)).

   - A new method ‘AttributeManager.get_id()’ to get a low-level
     AttrID(9) object referring to an attribute (GH1278(10)).

   - Several examples were updated to run on Python 3 (GH1149(11)).

   ---------- Footnotes ----------

   (1) https://github.com/h5py/h5py/issues/821

   (2) https://github.com/h5py/h5py/issues/963

   (3) https://github.com/h5py/h5py/issues/1174

   (4) https://github.com/h5py/h5py/issues/1228

   (5) https://github.com/h5py/h5py/issues/1155

   (6) https://github.com/h5py/h5py/issues/1132

   (7) https://github.com/h5py/h5py/issues/830

   (8) https://github.com/h5py/h5py/issues/1212

   (9) https://api.h5py.org/h5a.html#h5py.h5a.AttrID

   (10) https://github.com/h5py/h5py/issues/1278

   (11) https://github.com/h5py/h5py/issues/1149


File: h5py.info,  Node: Deprecations,  Next: Exposing HDF5 functions<2>,  Prev: New features<2>,  Up: What’s new in h5py 2 10

6.1.3.2 Deprecations
....................

   - The default behaviour of ‘h5py.File’ with no specified mode is
     deprecated (GH1143(1)).  It currently tries to create a file or
     open it for read/write access, silently falling back to read-only
     depending on permissions.  From h5py 3.0, the default will be
     read-only.

     Ideally, code should pass an explicit mode each time a file is
     opened:

          h5py.File("example.h5", "r")

     The possible modes are described in *note Opening & creating files:
     2a.  If you want to suppress the deprecation warnings from code you
     can’t modify, you can either:

             - set ‘h5.get_config().default_file_mode = 'r'’ (or another
               available mode)

             - or set the environment variable ‘H5PY_DEFAULT_READONLY’
               to any non-empty string, to adopt the future default.

   - This is expected to be the last h5py release to support Python 2.7
     and 3.4.

   ---------- Footnotes ----------

   (1) https://github.com/h5py/h5py/issues/1143


File: h5py.info,  Node: Exposing HDF5 functions<2>,  Next: Bugfixes,  Prev: Deprecations,  Up: What’s new in h5py 2 10

6.1.3.3 Exposing HDF5 functions
...............................

   - ‘H5Zunregister’ exposed as ‘h5z.unregister_filter()’ (GH746(1),
     GH1224(2)).

   - The new module h5py.h5pl(3) module exposes various ‘H5PL’ functions
     to inspect and modify the search path for plugins (GH1166(4),
     GH1256(5)).

   - ‘H5Dread_chunk’ exposed as ‘h5d.read_direct_chunk()’ (GH1190(6)).

   ---------- Footnotes ----------

   (1) https://github.com/h5py/h5py/issues/746

   (2) https://github.com/h5py/h5py/issues/1224

   (3) https://api.h5py.org/h5pl.html#module-h5py.h5pl

   (4) https://github.com/h5py/h5py/issues/1166

   (5) https://github.com/h5py/h5py/issues/1256

   (6) https://github.com/h5py/h5py/issues/1190


File: h5py.info,  Node: Bugfixes,  Next: Building h5py<3>,  Prev: Exposing HDF5 functions<2>,  Up: What’s new in h5py 2 10

6.1.3.4 Bugfixes
................

   - Fix crash with empty variable-length data (GH1248(1), GH1253(2)).

   - Fixed random selection of data type when reading 64-bit floats on
     Windows where Python uses random dictionary order (GH1051(3),
     GH1134(4)).

   - Pickling h5py objects now fails explicitly.  It previously failed
     on unpickling, and we can’t reliably serialise and restore handles
     to HDF5 objects anyway (GH531(5), GH1194(6)).  If you need to use
     these objects in other processes, you could explicitly serialise
     the filename and the name of the object inside the file.  Or
     consider h5pickle(7), which does the same implicitly.

   - Creating a dataset with external storage can no longer mutate the
     ‘external’ list parameter passed in (GH1205(8)).  It also has
     improved error messages (GH1204(9)).

   - Certain deprecation warnings will now show the relevant line of
     code which uses the deprecated feature (GH1146(10)).

   - Skipped a failing test for complex floating point numbers on 32-bit
     x86 systems (GH1235(11)).

   - Disabled the longdouble type on the ‘ppc64le’ architecture, as it
     was causing segfaults with more commonly used float types
     (GH1243(12)).

   - Documented that nested compound types are not currently supported
     (GH1236(13)).

   - Fixed attribute ‘create’ method to be consistent with ‘__setattr__’
     (GH1265(14)).

   ---------- Footnotes ----------

   (1) https://github.com/h5py/h5py/issues/1248

   (2) https://github.com/h5py/h5py/issues/1253

   (3) https://github.com/h5py/h5py/issues/1051

   (4) https://github.com/h5py/h5py/issues/1134

   (5) https://github.com/h5py/h5py/issues/531

   (6) https://github.com/h5py/h5py/issues/1194

   (7) https://github.com/Exteris/h5pickle/

   (8) https://github.com/h5py/h5py/issues/1205

   (9) https://github.com/h5py/h5py/issues/1204

   (10) https://github.com/h5py/h5py/issues/1146

   (11) https://github.com/h5py/h5py/issues/1235

   (12) https://github.com/h5py/h5py/issues/1243

   (13) https://github.com/h5py/h5py/issues/1236

   (14) https://github.com/h5py/h5py/issues/1265


File: h5py.info,  Node: Building h5py<3>,  Next: Development<3>,  Prev: Bugfixes,  Up: What’s new in h5py 2 10

6.1.3.5 Building h5py
.....................

   - The version of HDF5 can now be automatically detected on Windows
     (GH1123(1)).

   - Fixed autodetecting the version from libhdf5 in default locations
     on Windows and Mac (GH1240(2)).

   - Fail to build if it can’t detect version from libhdf5, rather than
     assuming 1.8.4 as a default (GH1241(3)).

   - Building h5py from source on Unix platforms now requires either
     ‘pkg-config’ or an explicitly specified path to HDF5 (GH1231(4)).
     Previously it had a hardcoded default path, but when this was
     wrong, the failures were unnecessarily confusing.

   - The Cython ‘language level’ is now explicitly set to 2, to prepare
     h5py for changing defaults in Cython (GH1171(5)).

   - Avoid using ‘setup_requires’ when pip calls ‘setup.py egg_info’
     (GH1259(6)).

   ---------- Footnotes ----------

   (1) https://github.com/h5py/h5py/issues/1123

   (2) https://github.com/h5py/h5py/issues/1240

   (3) https://github.com/h5py/h5py/issues/1241

   (4) https://github.com/h5py/h5py/issues/1231

   (5) https://github.com/h5py/h5py/issues/1171

   (6) https://github.com/h5py/h5py/issues/1259


File: h5py.info,  Node: Development<3>,  Prev: Building h5py<3>,  Up: What’s new in h5py 2 10

6.1.3.6 Development
...................

   - h5py’s tests are now run by pytest (GH1003(1)), and coverage
     reports are automatically generated on Codecov(2).

   ---------- Footnotes ----------

   (1) https://github.com/h5py/h5py/issues/1003

   (2) https://codecov.io/gh/h5py/h5py


File: h5py.info,  Node: What’s new in h5py 2 9,  Next: What’s new in h5py 2 8,  Prev: What’s new in h5py 2 10,  Up: “What’s new” documents

6.1.4 What’s new in h5py 2.9
----------------------------

* Menu:

* New features: New features<3>.
* Exposing HDF5 functions: Exposing HDF5 functions<3>.
* Bugfixes: Bugfixes<2>.
* Support for old Python::


File: h5py.info,  Node: New features<3>,  Next: Exposing HDF5 functions<3>,  Up: What’s new in h5py 2 9

6.1.4.1 New features
....................

   * A convenient high-level API for creating virtual datasets, HDF5
     objects which act as a view over one or more real datasets
     (GH1060(1), GH1126(2)).  See *note Virtual Datasets (VDS): 114. for
     details.

   * ‘File’ can now be constructed with a Python file-like object,
     making it easy to create an HDF5 file in memory using ‘io.BytesIO’
     (GH1061(3), GH1105(4), GH1116(5)).  See *note Python file-like
     objects: 30. for details.

   * ‘File’ now accepts parameters to control the chunk cache
     (GH1008(6)).  See *note Chunk cache: 3f. for details.

   * New options to record the order of insertion for attributes and
     group entries.  Iterating over these collections now follows
     insertion order if it was recorded, or name order if not
     (GH1098(7)).

   * A new method ‘Group.create_dataset_like()’ to create a new dataset
     with similar properties to an existing one (GH1085(8)).

   * Datasets can now be created with storage backed by external
     non-HDF5 files (GH1000(9)).

   * Lists or tuples of unicode strings can now be stored as HDF5
     attributes (GH1032(10)).

   * Inspecting the view returned by ‘.keys()’ now shows the key names,
     for convenient interactive use (GH1049(11)).

   ---------- Footnotes ----------

   (1) https://github.com/h5py/h5py/issues/1060

   (2) https://github.com/h5py/h5py/issues/1126

   (3) https://github.com/h5py/h5py/issues/1061

   (4) https://github.com/h5py/h5py/issues/1105

   (5) https://github.com/h5py/h5py/issues/1116

   (6) https://github.com/h5py/h5py/issues/1008

   (7) https://github.com/h5py/h5py/issues/1098

   (8) https://github.com/h5py/h5py/issues/1085

   (9) https://github.com/h5py/h5py/issues/1000

   (10) https://github.com/h5py/h5py/issues/1032

   (11) https://github.com/h5py/h5py/issues/1049


File: h5py.info,  Node: Exposing HDF5 functions<3>,  Next: Bugfixes<2>,  Prev: New features<3>,  Up: What’s new in h5py 2 9

6.1.4.2 Exposing HDF5 functions
...............................

   * ‘H5LTopen_file_image’ as h5py.h5f.open_file_image()(1) (GH1075(2)).

   * External dataset storage functions ‘H5Pset_external’,
     ‘H5Pget_external’ and ‘H5Pget_external_count’ as methods on
     h5py.h5p.PropDCID(3) (GH1000(4)).

   ---------- Footnotes ----------

   (1) https://api.h5py.org/h5f.html#h5py.h5f.open_file_image

   (2) https://github.com/h5py/h5py/issues/1075

   (3) https://api.h5py.org/h5p.html#h5py.h5p.PropDCID

   (4) https://github.com/h5py/h5py/issues/1000


File: h5py.info,  Node: Bugfixes<2>,  Next: Support for old Python,  Prev: Exposing HDF5 functions<3>,  Up: What’s new in h5py 2 9

6.1.4.3 Bugfixes
................

   * Fix reading/writing of float128 data (GH1114(1)).

   * Converting data to float16 when creating a dataset (GH1115(2)).

   ---------- Footnotes ----------

   (1) https://github.com/h5py/h5py/issues/1114

   (2) https://github.com/h5py/h5py/issues/1115


File: h5py.info,  Node: Support for old Python,  Prev: Bugfixes<2>,  Up: What’s new in h5py 2 9

6.1.4.4 Support for old Python
..............................

Support for Python 3.3 has been dropped.

Support for Python 2.6 has been dropped.


File: h5py.info,  Node: What’s new in h5py 2 8,  Next: What’s new in h5py 2 7 1,  Prev: What’s new in h5py 2 9,  Up: “What’s new” documents

6.1.5 What’s new in h5py 2.8
----------------------------

This is the first release of the h5py 2.8 series.  Note that the 2.8
series is the last series of h5py to support Python 2.6 and 3.3.  Users
should look to moving to Python 2.7 or (preferably) Python 3.4 or
higher, as earlier releases are now outside of security support.

* Menu:

* API changes::
* Features::
* Bug fixes: Bug fixes<3>.
* Wheels HDF5 Version::
* CI/Testing improvements and fixes::
* Other changes::
* Acknowledgements and Thanks::


File: h5py.info,  Node: API changes,  Next: Features,  Up: What’s new in h5py 2 8

6.1.5.1 API changes
...................

   * Deprecation of ‘h5t.available_ftypes’.  This is no longer used
     internally and will be removed in the future.  There is no
     replacement public API. See GH926(1) for how to add addition
     floating point types to h5py.

   * Do not sort fields in compound types (GH970(2) by James Tocknell).
     This is to account for changes in numpy 1.14.

   * Minimum required version of Cython is now 0.23.

   ---------- Footnotes ----------

   (1) https://github.com/h5py/h5py/issues/926

   (2) https://github.com/h5py/h5py/issues/970


File: h5py.info,  Node: Features,  Next: Bug fixes<3>,  Prev: API changes,  Up: What’s new in h5py 2 8

6.1.5.2 Features
................

   * Allow registration of new file drivers (GH956(1) by Joe Jevnik).

   * Add option to track object creation order to ‘Group.create_group’
     (GH968(2) by Chen Yufei)

   ---------- Footnotes ----------

   (1) https://github.com/h5py/h5py/issues/956

   (2) https://github.com/h5py/h5py/issues/968


File: h5py.info,  Node: Bug fixes<3>,  Next: Wheels HDF5 Version,  Prev: Features,  Up: What’s new in h5py 2 8

6.1.5.3 Bug fixes
.................

   * Support slices with stop < start as empty slices (GH924(1) by Joe
     Jevnik)

   * Avoid crashing the IPython auto-completer when missing key
     (GH885(2), GH958(3) by James Tocknell).  The auto-completer
     currently only works on older versions of IPython, see GH1022(4)
     for what’s needed to support newer versions of IPython/jupyter (PRs
     welcome!)

   * Set libver default to ‘earliest’ (a.k.a LIBVER_EARLIEST) as
     previously documented (GH933(5), GH936(6) by James Tocknell)

   * Fix conflict between fletcher32 and szip compression when using the
     float64 dtype (GH953(7), GH989(8), by Paul Müller).

   * Update floating point type handling as flagged by numpy deprecation
     warning (GH985(9), by Eric Larson)

   * Allow ExternalLinks to use non-ASCII hdf5 paths (GH333(10),
     GH952(11) by James Tocknell)

   * Prefer custom HDF5 over pkg-config/fallback paths when
     building/installing (GH946(12), GH947(13) by Lars Viklund)

   * Fix compatibility with Python 3 in document generation (GH921(14)
     by Ghislain Antony Vaillant)

   * Fix spelling and grammar in documentation (GH931(15) by Michael V.
     DePalatis, GH950(16) by Christian Sachs, GH1015(17) by Mikhail)

   * Add minor changes to documentation in order to improve clarity and
     warn about potential problems (GH528(18), GH783(19), GH829(20),
     GH849(21), GH911(22), GH959(23), by James Tocknell)

   * Add license field to ‘setup.py’ metadata (GH999(24) by Nils
     Werner).

   * Use system encoding for errors, not utf-8 (GH1016(25), GH1025(26)
     by James Tocknell)

   * Add ‘write_direct’ to the documentation (GH1028(27) by Sajid Ali
     and Thomas A Caswell)

   ---------- Footnotes ----------

   (1) https://github.com/h5py/h5py/issues/924

   (2) https://github.com/h5py/h5py/issues/885

   (3) https://github.com/h5py/h5py/issues/958

   (4) https://github.com/h5py/h5py/issues/1022

   (5) https://github.com/h5py/h5py/issues/933

   (6) https://github.com/h5py/h5py/issues/936

   (7) https://github.com/h5py/h5py/issues/953

   (8) https://github.com/h5py/h5py/issues/989

   (9) https://github.com/h5py/h5py/issues/985

   (10) https://github.com/h5py/h5py/issues/333

   (11) https://github.com/h5py/h5py/issues/952

   (12) https://github.com/h5py/h5py/issues/946

   (13) https://github.com/h5py/h5py/issues/947

   (14) https://github.com/h5py/h5py/issues/921

   (15) https://github.com/h5py/h5py/issues/931

   (16) https://github.com/h5py/h5py/issues/950

   (17) https://github.com/h5py/h5py/issues/1015

   (18) https://github.com/h5py/h5py/issues/528

   (19) https://github.com/h5py/h5py/issues/783

   (20) https://github.com/h5py/h5py/issues/829

   (21) https://github.com/h5py/h5py/issues/849

   (22) https://github.com/h5py/h5py/issues/911

   (23) https://github.com/h5py/h5py/issues/959

   (24) https://github.com/h5py/h5py/issues/999

   (25) https://github.com/h5py/h5py/issues/1016

   (26) https://github.com/h5py/h5py/issues/1025

   (27) https://github.com/h5py/h5py/issues/1028


File: h5py.info,  Node: Wheels HDF5 Version,  Next: CI/Testing improvements and fixes,  Prev: Bug fixes<3>,  Up: What’s new in h5py 2 8

6.1.5.4 Wheels HDF5 Version
...........................

   * Wheels uploaded to PyPI will now be built against the HDF5 1.10
     series as opposed to the 1.8 series (h5py 2.8 is built against HDF5
     1.10.2).


File: h5py.info,  Node: CI/Testing improvements and fixes,  Next: Other changes,  Prev: Wheels HDF5 Version,  Up: What’s new in h5py 2 8

6.1.5.5 CI/Testing improvements and fixes
.........................................

There were a number of improvements to testing and CI systems of h5py,
including running the CI against multiple versions of HDF5, improving
reliability and speed of the CIs, and simplifying the tox file.  See
GH857(1), GH894(2), GH922(3), GH954(4) and GH962(5) by Thomas A Caswell
and James Tocknell for more details.

   ---------- Footnotes ----------

   (1) https://github.com/h5py/h5py/issues/857

   (2) https://github.com/h5py/h5py/issues/894

   (3) https://github.com/h5py/h5py/issues/922

   (4) https://github.com/h5py/h5py/issues/954

   (5) https://github.com/h5py/h5py/issues/962


File: h5py.info,  Node: Other changes,  Next: Acknowledgements and Thanks,  Prev: CI/Testing improvements and fixes,  Up: What’s new in h5py 2 8

6.1.5.6 Other changes
.....................

   * Emphasise reading from HDF5 files rather than writing to files in
     Quickguide (GH609(1), GH610(2) by Yu Feng).  Note these changes
     were in the 2.5 branch, but never got merged into master.  The h5py
     2.8 release now actually includes these changes.

   * Use lazy-loading of run_tests to avoid strong dependency on
     unittest2 (GH1013(3), GH1014(4) by Thomas VINCENT)

   * Correctly handle with multiple float types of the same size
     (GH926(5) by James Tocknell)

   ---------- Footnotes ----------

   (1) https://github.com/h5py/h5py/issues/609

   (2) https://github.com/h5py/h5py/issues/610

   (3) https://github.com/h5py/h5py/issues/1013

   (4) https://github.com/h5py/h5py/issues/1014

   (5) https://github.com/h5py/h5py/issues/926


File: h5py.info,  Node: Acknowledgements and Thanks,  Prev: Other changes,  Up: What’s new in h5py 2 8

6.1.5.7 Acknowledgements and Thanks
...................................

The h5py developers thank Nathan Goldbaum, Matthew Brett, and Christoph
Gohlke for building the wheels that appear on PyPI.


File: h5py.info,  Node: What’s new in h5py 2 7 1,  Next: What’s new in h5py 2 7,  Prev: What’s new in h5py 2 8,  Up: “What’s new” documents

6.1.6 What’s new in h5py 2.7.1
------------------------------

2.7.1 is the first bug-fix release in the 2.7.x series.

* Menu:

* Bug fixes: Bug fixes<4>.


File: h5py.info,  Node: Bug fixes<4>,  Up: What’s new in h5py 2 7 1

6.1.6.1 Bug fixes
.................

        - GH903(1) Fixed critical issue with cyclic gc which resulted in
          segfaults

        - GH904(2) Avoid unaligned access fixing h5py on sparc64

        - GH883(3) Fixed compilation issues for some library locations

        - GH868(4) Fix deadlock between phil and the import lock in py2

        - GH841(5) Improve windows handling if filenames

        - GH874(6) Allow close to be called on file multiple times

        - GH867(7), GH872(8) Warn on loaded vs complied hdf5 version
          issues

        - GH902(9) Fix overflow computing size of dataset on windows

        - GH912(10) Do not mangle capitalization of filenames in error
          messages

        - GH842(11) Fix longdouble on ppc64le

        - GH862(12), GH916(13) Fix compounds structs with variable-size
          members

* Menu:

* Fix h5py segfaulting on some Python 3 versions::
* Avoid unaligned memory access in conversion functions::

   ---------- Footnotes ----------

   (1) https://github.com/h5py/h5py/issues/903

   (2) https://github.com/h5py/h5py/issues/904

   (3) https://github.com/h5py/h5py/issues/883

   (4) https://github.com/h5py/h5py/issues/868

   (5) https://github.com/h5py/h5py/issues/841

   (6) https://github.com/h5py/h5py/issues/874

   (7) https://github.com/h5py/h5py/issues/867

   (8) https://github.com/h5py/h5py/issues/872

   (9) https://github.com/h5py/h5py/issues/902

   (10) https://github.com/h5py/h5py/issues/912

   (11) https://github.com/h5py/h5py/issues/842

   (12) https://github.com/h5py/h5py/issues/862

   (13) https://github.com/h5py/h5py/issues/916


File: h5py.info,  Node: Fix h5py segfaulting on some Python 3 versions,  Next: Avoid unaligned memory access in conversion functions,  Up: Bug fixes<4>

6.1.6.2 Fix h5py segfaulting on some Python 3 versions
......................................................

Through an intersection of Python Issue 30484(1) and GH888(2), it was
possible for the Python Garbage Collector to activate when closing
‘h5py’ objects, which due to how dictionaries were iterated over in
Python could cause a segfault.  GH903(3) fixes the Garbage Collector
activating whilst closing, whilst Python Issue 30484(4) had been fixed
upstream (and backported to Python 3.3 onwards).

   ---------- Footnotes ----------

   (1) https://bugs.python.org/issue30484

   (2) https://github.com/h5py/h5py/issues/888

   (3) https://github.com/h5py/h5py/issues/903

   (4) https://bugs.python.org/issue30484


File: h5py.info,  Node: Avoid unaligned memory access in conversion functions,  Prev: Fix h5py segfaulting on some Python 3 versions,  Up: Bug fixes<4>

6.1.6.3 Avoid unaligned memory access in conversion functions
.............................................................

Some architectures (e.g.  SPARC64) do not allow unaligned memory access,
which can come up when copying packed structs.  GH904(1) (by James
Clarke) uses ‘memcpy’ to avoid said unaligned memory access.

   ---------- Footnotes ----------

   (1) https://github.com/h5py/h5py/issues/904


File: h5py.info,  Node: What’s new in h5py 2 7,  Next: What’s new in h5py 2 6,  Prev: What’s new in h5py 2 7 1,  Up: “What’s new” documents

6.1.7 What’s new in h5py 2.7
----------------------------

* Menu:

* Python 3.2 is no longer supported: Python 3 2 is no longer supported.
* Improved testing support::
* Improved python compatibility::
* Documentation improvements::
* setup.py improvements: setup py improvements.
* Support for additional HDF5 features added::
* Improvements to type system::
* Other changes: Other changes<2>.
* Acknowledgements::


File: h5py.info,  Node: Python 3 2 is no longer supported,  Next: Improved testing support,  Up: What’s new in h5py 2 7

6.1.7.1 Python 3.2 is no longer supported
.........................................

‘h5py’ 2.7 drops Python 3.2 support, and testing is not longer performed
on Python 3.2.  The latest versions of ‘pip’, ‘virtualenv’, ‘setuptools’
and ‘numpy’ do not support Python 3.2, and dropping 3.2 allows both ‘u’
and ‘b’ prefixes to be used for strings.  A clean up of some of the
legacy code was done in #675(1) by Andrew Collette.

Additionally, support for Python 2.6 is soon to be dropped for ‘pip’
(See ‘https://github.com/pypa/pip/issues/3955’) and ‘setuptools’ (See
‘https://github.com/pypa/setuptools/issues/878’), and ‘numpy’ has
dropped Python 2.6 also in the latest release.  While ‘h5py’ has not
dropped Python 2.6 this release, users are strongly encouraged to move
to Python 2.7 where possible.

   ---------- Footnotes ----------

   (1) https://github.com/h5py/h5py/pull/675


File: h5py.info,  Node: Improved testing support,  Next: Improved python compatibility,  Prev: Python 3 2 is no longer supported,  Up: What’s new in h5py 2 7

6.1.7.2 Improved testing support
................................

There has been a major increase in the number of configurations ‘h5py’
is automatically tested in, with Windows CI support added via Appveyor
(#795(1), #798(2), #799(3) and #801(4) by James Tocknell) and testing of
minimum requirements to ensure we still satisfy them (#703(5) by James
Tocknell).  Additionally, ‘tox’ was used to ensure that we don’t run
tests on Python versions which our dependencies have dropped or do not
support (#662(6), #700(7) and #733(8)).  Thanks to to the Appveyor
support, unicode tests were made more robust (#788(9), #800(10) and
#804(11) by James Tocknell).  Finally, other tests were improved or
added where needed (#724(12) by Matthew Brett, #789(13), #794(14) and
#802(15) by James Tocknell).

   ---------- Footnotes ----------

   (1) https://github.com/h5py/h5py/pull/795

   (2) https://github.com/h5py/h5py/pull/798

   (3) https://github.com/h5py/h5py/pull/799

   (4) https://github.com/h5py/h5py/pull/801

   (5) https://github.com/h5py/h5py/pull/703

   (6) https://github.com/h5py/h5py/pull/662

   (7) https://github.com/h5py/h5py/pull/700

   (8) https://github.com/h5py/h5py/pull/733

   (9) https://github.com/h5py/h5py/pull/788

   (10) https://github.com/h5py/h5py/pull/800

   (11) https://github.com/h5py/h5py/pull/804

   (12) https://github.com/h5py/h5py/pull/724

   (13) https://github.com/h5py/h5py/pull/789

   (14) https://github.com/h5py/h5py/pull/794

   (15) https://github.com/h5py/h5py/pull/802


File: h5py.info,  Node: Improved python compatibility,  Next: Documentation improvements,  Prev: Improved testing support,  Up: What’s new in h5py 2 7

6.1.7.3 Improved python compatibility
.....................................

The ‘ipython’/‘jupyter’ completion support now has Python 3 support
(#715(1) by Joseph Kleinhenz).  ‘h5py’ now supports ‘pathlib’ filenames
(#716(2) by James Tocknell).

   ---------- Footnotes ----------

   (1) https://github.com/h5py/h5py/pull/715

   (2) https://github.com/h5py/h5py/pull/716


File: h5py.info,  Node: Documentation improvements,  Next: setup py improvements,  Prev: Improved python compatibility,  Up: What’s new in h5py 2 7

6.1.7.4 Documentation improvements
..................................

An update to the installation instructions and some whitespace cleanup
was done in #808(1) by Thomas A Caswell, and mistake in the quickstart
was fixed by Joydeep Bhattacharjee in #708(2).

   ---------- Footnotes ----------

   (1) https://github.com/h5py/h5py/pull/808

   (2) https://github.com/h5py/h5py/pull/708


File: h5py.info,  Node: setup py improvements,  Next: Support for additional HDF5 features added,  Prev: Documentation improvements,  Up: What’s new in h5py 2 7

6.1.7.5 setup.py improvements
.............................

Support for detecting the version of HDF5 via ‘pkgconfig’ was added by
Axel Huebl in #734(1), and support for specifying the path to
MPI-supported HDF5 was added by Axel Huebl in #721(2).  ‘h5py's’
classifiers were updated to include supported python version and
interpreters in #811(3) by James Tocknell.

   ---------- Footnotes ----------

   (1) https://github.com/h5py/h5py/pull/734

   (2) https://github.com/h5py/h5py/pull/721

   (3) https://github.com/h5py/h5py/pull/811


File: h5py.info,  Node: Support for additional HDF5 features added,  Next: Improvements to type system,  Prev: setup py improvements,  Up: What’s new in h5py 2 7

6.1.7.6 Support for additional HDF5 features added
..................................................

Low-level support for HDF5 Direct Chunk Write(1) was added in #691(2) by
Simon Gregor Ebner.  Minimal support for HDF5 File Image Operations(3)
was added by Andrea Bedini in #680(4).  Ideas and opinions for further
support for both HDF5 Direct Chunk Write(5) and HDF5 File Image
Operations(6) are welcome.  High-level support for reading and writing
null dataspaces was added in #664(7) by James Tocknell.

   ---------- Footnotes ----------

   (1) https://support.hdfgroup.org/HDF5/doc/Advanced/DirectChunkWrite/

   (2) https://github.com/h5py/h5py/pull/691

   (3) 
https://support.hdfgroup.org/HDF5/doc/Advanced/FileImageOperations/HDF5FileImageOperations.pdf

   (4) https://github.com/h5py/h5py/pull/680

   (5) https://support.hdfgroup.org/HDF5/doc/Advanced/DirectChunkWrite/

   (6) 
https://support.hdfgroup.org/HDF5/doc/Advanced/FileImageOperations/HDF5FileImageOperations.pdf

   (7) https://github.com/h5py/h5py/pull/664


File: h5py.info,  Node: Improvements to type system,  Next: Other changes<2>,  Prev: Support for additional HDF5 features added,  Up: What’s new in h5py 2 7

6.1.7.7 Improvements to type system
...................................

Reading and writing of compound datatypes has improved, with support for
different orderings and alignments (#701(1) by Jonah Bernhard, #702(2)
by Caleb Morse #738(3) by @smutch, #765(4) by Nathan Goldbaum and
#793(5) by James Tocknell).  Support for reading extended precision and
non-standard floating point numbers has also been added (#749(6),
#812(7) by Thomas A Caswell, #787(8) by James Tocknell and #781(9) by
Martin Raspaud).  Finally, compatibility improvements to ‘Cython’
annotations of HDF5 types were added in #692(10) and #693(11) by
Aleksandar Jelenak.

   ---------- Footnotes ----------

   (1) https://github.com/h5py/h5py/pull/701

   (2) https://github.com/h5py/h5py/pull/702

   (3) https://github.com/h5py/h5py/pull/738

   (4) https://github.com/h5py/h5py/pull/765

   (5) https://github.com/h5py/h5py/pull/793

   (6) https://github.com/h5py/h5py/pull/749

   (7) https://github.com/h5py/h5py/pull/812

   (8) https://github.com/h5py/h5py/pull/787

   (9) https://github.com/h5py/h5py/pull/781

   (10) https://github.com/h5py/h5py/pull/692

   (11) https://github.com/h5py/h5py/pull/693


File: h5py.info,  Node: Other changes<2>,  Next: Acknowledgements,  Prev: Improvements to type system,  Up: What’s new in h5py 2 7

6.1.7.8 Other changes
.....................

   * Fix deprecation of ‘-’ for ‘numpy’ boolean arrays (#683(1) by James
     Tocknell)

   * Check for duplicates in fancy index validation (#739(2) by Sam
     Toyer)

   * Avoid potential race condition (#754(3) by James Tocknell)

   * Fix inconsistency when slicing with ‘numpy.array’ of shape ‘(1,)’
     (#772(4) by Artsiom)

   * Use ‘size_t’ to store Python object id (#773(5) by Christoph
     Gohlke)

   * Avoid errors when the Python GC runs during ‘nonlocal_close()’
     (#776(6) by Antoine Pitrou)

   * Move from ‘six.PY3’ to ‘six.PY2’ (#686(7) by James Tocknell)

   ---------- Footnotes ----------

   (1) https://github.com/h5py/h5py/pull/683

   (2) https://github.com/h5py/h5py/pull/739

   (3) https://github.com/h5py/h5py/pull/754

   (4) https://github.com/h5py/h5py/pull/772

   (5) https://github.com/h5py/h5py/pull/773

   (6) https://github.com/h5py/h5py/pull/776

   (7) https://github.com/h5py/h5py/pull/686


File: h5py.info,  Node: Acknowledgements,  Prev: Other changes<2>,  Up: What’s new in h5py 2 7

6.1.7.9 Acknowledgements
........................


File: h5py.info,  Node: What’s new in h5py 2 6,  Next: What’s new in h5py 2 5,  Prev: What’s new in h5py 2 7,  Up: “What’s new” documents

6.1.8 What’s new in h5py 2.6
----------------------------

* Menu:

* Support for HDF5 Virtual Dataset API::
* Add MPI Collective I/O Support::
* Numerous build/testing/CI improvements::
* Cleanup of codebase based on pylint::
* Fixes to low-level API::
* Documentation improvements: Documentation improvements<2>.
* Other changes: Other changes<3>.
* Acknowledgements: Acknowledgements<2>.


File: h5py.info,  Node: Support for HDF5 Virtual Dataset API,  Next: Add MPI Collective I/O Support,  Up: What’s new in h5py 2 6

6.1.8.1 Support for HDF5 Virtual Dataset API
............................................

Initial support for the HDF5 Virtual Dataset API, which was introduced
in HDF5 1.10, was added to the low-level API. Ideas and input for how
this should work as part of the high-level interface are welcome.

This work was added in #663(1) by Aleksandar Jelenak.

   ---------- Footnotes ----------

   (1) https://github.com/h5py/h5py/pull/663


File: h5py.info,  Node: Add MPI Collective I/O Support,  Next: Numerous build/testing/CI improvements,  Prev: Support for HDF5 Virtual Dataset API,  Up: What’s new in h5py 2 6

6.1.8.2 Add MPI Collective I/O Support
......................................

Support for using MPI Collective I/O in both low-level and high-level
code has been added.  See the collective_io.py example for a simple
demonstration of how to use MPI Collective I/O with the high level API.

This work was added in #648(1) by Jialin Liu.

   ---------- Footnotes ----------

   (1) https://github.com/h5py/h5py/pull/648


File: h5py.info,  Node: Numerous build/testing/CI improvements,  Next: Cleanup of codebase based on pylint,  Prev: Add MPI Collective I/O Support,  Up: What’s new in h5py 2 6

6.1.8.3 Numerous build/testing/CI improvements
..............................................

There were a number of improvements to the setup.py file, which should
mean that ‘pip install h5py’ should work in most places.  Work was also
done to clean up the current testing system, using tox is the
recommended way of testing h5py across different Python versions.  See
#576(1) by Jakob Lombacher, #640(2) by Lawrence Mitchell, and #650(3),
#651(4) and #658(5) by James Tocknell.

   ---------- Footnotes ----------

   (1) https://github.com/h5py/h5py/pull/576

   (2) https://github.com/h5py/h5py/pull/640

   (3) https://github.com/h5py/h5py/pull/650

   (4) https://github.com/h5py/h5py/pull/651

   (5) https://github.com/h5py/h5py/pull/658


File: h5py.info,  Node: Cleanup of codebase based on pylint,  Next: Fixes to low-level API,  Prev: Numerous build/testing/CI improvements,  Up: What’s new in h5py 2 6

6.1.8.4 Cleanup of codebase based on pylint
...........................................

There was a large cleanup of pylint-identified problems by Andrew
Collette (#578(1), #579(2)).

   ---------- Footnotes ----------

   (1) https://github.com/h5py/h5py/pull/578

   (2) https://github.com/h5py/h5py/pull/579


File: h5py.info,  Node: Fixes to low-level API,  Next: Documentation improvements<2>,  Prev: Cleanup of codebase based on pylint,  Up: What’s new in h5py 2 6

6.1.8.5 Fixes to low-level API
..............................

Fixes to the typing of functions were added in #597(1) by Ulrik Kofoed
Pedersen, #589(2) by Peter Chang, and #625(3) by Spaghetti Sort.  A fix
for variable-length arrays was added in #621(4) by Sam Mason.  Fixes to
compound types were added in #639(5) by @nevion and #606(6) by Yu Feng.
Finally, a fix to type conversion was added in #614(7) by Andrew
Collette.

   ---------- Footnotes ----------

   (1) https://github.com/h5py/h5py/pull/597

   (2) https://github.com/h5py/h5py/pull/589

   (3) https://github.com/h5py/h5py/pull/625

   (4) https://github.com/h5py/h5py/pull/621

   (5) https://github.com/h5py/h5py/pull/639

   (6) https://github.com/h5py/h5py/pull/606

   (7) https://github.com/h5py/h5py/pull/614


File: h5py.info,  Node: Documentation improvements<2>,  Next: Other changes<3>,  Prev: Fixes to low-level API,  Up: What’s new in h5py 2 6

6.1.8.6 Documentation improvements
..................................

   * Updates to FAQ by Dan Guest (#608(1)) and Peter Hill (#607(2)).

   * Updates MPI-related documentation by Jens Timmerman (#604(3)) and
     Matthias König (#572(4)).

   * Fixes to documentation building by Ghislain Antony Vaillant
     (#562(5), #561(6)).

   * Update PyTables link (#574(7) by Dominik Kriegner)

   * Add File opening modes to docstring (#563(8) by Antony Lee)

   ---------- Footnotes ----------

   (1) https://github.com/h5py/h5py/pull/608

   (2) https://github.com/h5py/h5py/pull/607

   (3) https://github.com/h5py/h5py/pull/604

   (4) https://github.com/h5py/h5py/pull/572

   (5) https://github.com/h5py/h5py/pull/562

   (6) https://github.com/h5py/h5py/pull/561

   (7) https://github.com/h5py/h5py/pull/574

   (8) https://github.com/h5py/h5py/pull/563


File: h5py.info,  Node: Other changes<3>,  Next: Acknowledgements<2>,  Prev: Documentation improvements<2>,  Up: What’s new in h5py 2 6

6.1.8.7 Other changes
.....................

   * Add ‘Dataset.ndim’ (#649(1), #660(2) by @jakirkham, #661(3) by
     James Tocknell)

   * Fix import errors in IPython completer (#605(4) by Niru
     Maheswaranathan)

   * Turn off error printing in new threads (#583(5) by Andrew Collette)

   * Use item value in ‘KeyError’ instead of error message (#642(6) by
     Matthias Geier)

   ---------- Footnotes ----------

   (1) https://github.com/h5py/h5py/pull/649

   (2) https://github.com/h5py/h5py/pull/660

   (3) https://github.com/h5py/h5py/pull/661

   (4) https://github.com/h5py/h5py/pull/605

   (5) https://github.com/h5py/h5py/pull/583

   (6) https://github.com/h5py/h5py/pull/642


File: h5py.info,  Node: Acknowledgements<2>,  Prev: Other changes<3>,  Up: What’s new in h5py 2 6

6.1.8.8 Acknowledgements
........................


File: h5py.info,  Node: What’s new in h5py 2 5,  Next: What’s new in h5py 2 4,  Prev: What’s new in h5py 2 6,  Up: “What’s new” documents

6.1.9 What’s new in h5py 2.5
----------------------------

* Menu:

* Experimental support for Single Writer Multiple Reader (SWMR): Experimental support for Single Writer Multiple Reader SWMR.
* Other changes: Other changes<4>.
* Acknowledgements: Acknowledgements<3>.


File: h5py.info,  Node: Experimental support for Single Writer Multiple Reader SWMR,  Next: Other changes<4>,  Up: What’s new in h5py 2 5

6.1.9.1 Experimental support for Single Writer Multiple Reader (SWMR)
.....................................................................

This release introduces experimental support for the highly-anticipated
“Single Writer Multiple Reader” (SWMR) feature in the upcoming HDF5 1.10
release.  SWMR allows sharing of a single HDF5 file between multiple
processes without the complexity of MPI or multiprocessing-based
solutions.

This is an experimental feature that should NOT be used in production
code.  We are interested in getting feedback from the broader community
with respect to performance and the API design.

For more details, check out the h5py user guide:
‘https://docs.h5py.org/en/latest/swmr.html’

SWMR support was contributed by Ulrik Pedersen (#551(1)).

   ---------- Footnotes ----------

   (1) https://github.com/h5py/h5py/pull/551


File: h5py.info,  Node: Other changes<4>,  Next: Acknowledgements<3>,  Prev: Experimental support for Single Writer Multiple Reader SWMR,  Up: What’s new in h5py 2 5

6.1.9.2 Other changes
.....................

   * Use system Cython as a fallback if ‘cythonize()’ fails (#541(1) by
     Ulrik Pedersen).

   * Use pkg-config for building/linking against hdf5 (#505(2) by James
     Tocknell).

   * Disable building Cython on Travis (#513(3) by Andrew Collette).

   * Improvements to release tarball (#555(4), #560(5) by Ghislain
     Antony Vaillant).

   * h5py now has one codebase for both Python 2 and 3; 2to3 removed
     from setup.py (#508(6) by James Tocknell).

   * Add python 3.4 to tox (#507(7) by James Tocknell).

   * Warn when importing from inside install dir (#558(8) by Andrew
     Collette).

   * Tweak installation docs with reference to Anaconda and other Python
     package managers (#546(9) by Andrew Collette).

   * Fix incompatible function pointer types (#526(10), #524(11) by
     Peter H. Li).

   * Add explicit ‘vlen is not None’ check to work around
     ‘https://github.com/numpy/numpy/issues/2190’ (‘#538’ by Will
     Parkin).

   * Group and AttributeManager classes now inherit from the appropriate
     ABCs (#527(12) by James Tocknell).

   * Don’t strip metadata from special dtypes on read (#512(13) by
     Antony Lee).

   * Add ‘x’ mode as an alias for ‘w-‘ (#510(14) by Antony Lee).

   * Support dynamical loading of LZF filter plugin (#506(15) by Peter
     Colberg).

   * Fix accessing attributes with array type (#501(16) by Andrew
     Collette).

   * Don’t leak types in enum converter (#503(17) by Andrew Collette).

   * Cython warning cleanups related to “const”

   ---------- Footnotes ----------

   (1) https://github.com/h5py/h5py/pull/541

   (2) https://github.com/h5py/h5py/pull/505

   (3) https://github.com/h5py/h5py/pull/513

   (4) https://github.com/h5py/h5py/pull/555

   (5) https://github.com/h5py/h5py/pull/560

   (6) https://github.com/h5py/h5py/pull/508

   (7) https://github.com/h5py/h5py/pull/507

   (8) https://github.com/h5py/h5py/pull/558

   (9) https://github.com/h5py/h5py/pull/546

   (10) https://github.com/h5py/h5py/pull/526

   (11) https://github.com/h5py/h5py/pull/524

   (12) https://github.com/h5py/h5py/pull/527

   (13) https://github.com/h5py/h5py/pull/512

   (14) https://github.com/h5py/h5py/pull/510

   (15) https://github.com/h5py/h5py/pull/506

   (16) https://github.com/h5py/h5py/pull/501

   (17) https://github.com/h5py/h5py/pull/503


File: h5py.info,  Node: Acknowledgements<3>,  Prev: Other changes<4>,  Up: What’s new in h5py 2 5

6.1.9.3 Acknowledgements
........................

This release incorporates changes from, among others:

   * Ulrik Pedersen

   * James Tocknell

   * Will Parkin

   * Antony Lee

   * Peter H. Li

   * Peter Colberg

   * Ghislain Antony Vaillant


File: h5py.info,  Node: What’s new in h5py 2 4,  Next: What’s new in h5py 2 3,  Prev: What’s new in h5py 2 5,  Up: “What’s new” documents

6.1.10 What’s new in h5py 2.4
-----------------------------

* Menu:

* Build system changes::
* Files will now auto-close::
* Thread safety improvements::
* External link improvements::
* Thanks to::


File: h5py.info,  Node: Build system changes,  Next: Files will now auto-close,  Up: What’s new in h5py 2 4

6.1.10.1 Build system changes
.............................

The setup.py-based build system has been reworked to be more
maintainable, and to fix certain long-standing bugs.  As a consequence,
the options to setup.py have changed; a new top-level “configure”
command handles options like ‘--hdf5=/path/to/hdf5’ and ‘--mpi’.
Setup.py now works correctly under Python 3 when these options are used.

Cython (0.17+) is now required when building from source on all
platforms; the .c files are no longer shipped in the UNIX release.  The
minimum NumPy version is now 1.6.1.


File: h5py.info,  Node: Files will now auto-close,  Next: Thread safety improvements,  Prev: Build system changes,  Up: What’s new in h5py 2 4

6.1.10.2 Files will now auto-close
..................................

Files are now automatically closed when all objects within them are
unreachable.  Previously, if File.close() was not explicitly called,
files would remain open and “leaks” were possible if the File object was
lost.


File: h5py.info,  Node: Thread safety improvements,  Next: External link improvements,  Prev: Files will now auto-close,  Up: What’s new in h5py 2 4

6.1.10.3 Thread safety improvements
...................................

Access to all APIs, high- and low-level, are now protected by a global
lock.  The entire API is now believed to be thread-safe.  Feedback and
real-world testing is welcome.


File: h5py.info,  Node: External link improvements,  Next: Thanks to,  Prev: Thread safety improvements,  Up: What’s new in h5py 2 4

6.1.10.4 External link improvements
...................................

External links now work if the target file is already open.  Previously
this was not possible because of a mismatch in the file close strengths.


File: h5py.info,  Node: Thanks to,  Prev: External link improvements,  Up: What’s new in h5py 2 4

6.1.10.5 Thanks to
..................

Many people, but especially:

   * Matthieu Brucher

   * Laurence Hole

   * John Tyree

   * Pierre de Buyl

   * Matthew Brett


File: h5py.info,  Node: What’s new in h5py 2 3,  Next: What’s new in h5py 2 2,  Prev: What’s new in h5py 2 4,  Up: “What’s new” documents

6.1.11 What’s new in h5py 2.3
-----------------------------

* Menu:

* Support for arbitrary vlen data::
* Improved exception messages::
* Improved setuptools support::
* Multiple low-level additions::
* Improved support for MPI features::
* Readonly files can now be opened in default mode::
* Single-step build for HDF5 on Windows::
* Thanks to: Thanks to<2>.


File: h5py.info,  Node: Support for arbitrary vlen data,  Next: Improved exception messages,  Up: What’s new in h5py 2 3

6.1.11.1 Support for arbitrary vlen data
........................................

Variable-length data is *note no longer restricted to strings: e6.  You
can use this feature to produce “ragged” arrays, whose members are 1D
arrays of variable length.

The implementation of special types was changed to use the NumPy dtype
“metadata” field.  This change should be transparent, as access to
special types is handled through ‘h5py.special_dtype’ and
‘h5py.check_dtype’.


File: h5py.info,  Node: Improved exception messages,  Next: Improved setuptools support,  Prev: Support for arbitrary vlen data,  Up: What’s new in h5py 2 3

6.1.11.2 Improved exception messages
....................................

H5py has historically suffered from low-detail exception messages
generated automatically by HDF5.  While the exception types in 2.3
remain identical to those in 2.2, the messages have been substantially
improved to provide more information as to the source of the error.

Examples:

     ValueError: Unable to set extend dataset (Dimension cannot exceed the existing maximal size (new: 100 max: 1))

     IOError: Unable to open file (Unable to open file: name = 'x3', errno = 2, error message = 'no such file or directory', flags = 0, o_flags = 0)

     KeyError: "Unable to open object (Object 'foo' doesn't exist)"


File: h5py.info,  Node: Improved setuptools support,  Next: Multiple low-level additions,  Prev: Improved exception messages,  Up: What’s new in h5py 2 3

6.1.11.3 Improved setuptools support
....................................

‘setup.py’ now uses ‘setup_requires’ to make installation via pip
friendlier.


File: h5py.info,  Node: Multiple low-level additions,  Next: Improved support for MPI features,  Prev: Improved setuptools support,  Up: What’s new in h5py 2 3

6.1.11.4 Multiple low-level additions
.....................................

Improved support for opening datasets via the low-level interface, by
adding ‘H5Dopen2’ and many new property-list functions.


File: h5py.info,  Node: Improved support for MPI features,  Next: Readonly files can now be opened in default mode,  Prev: Multiple low-level additions,  Up: What’s new in h5py 2 3

6.1.11.5 Improved support for MPI features
..........................................

Added support for retrieving the MPI communicator and info objects from
an open file.  Added boilerplate code to allow compiling cleanly against
newer versions of mpi4py.


File: h5py.info,  Node: Readonly files can now be opened in default mode,  Next: Single-step build for HDF5 on Windows,  Prev: Improved support for MPI features,  Up: What’s new in h5py 2 3

6.1.11.6 Readonly files can now be opened in default mode
.........................................................

When opening a read-only file with no mode flags, now defaults to
opening the file on RO mode rather than raising an exception.


File: h5py.info,  Node: Single-step build for HDF5 on Windows,  Next: Thanks to<2>,  Prev: Readonly files can now be opened in default mode,  Up: What’s new in h5py 2 3

6.1.11.7 Single-step build for HDF5 on Windows
..............................................

Building h5py on windows has typically been hamstrung by the need to
build a compatible version of HDF5 first.  A new Paver-based system
located in the “windows” distribution directory allows single-step
compilation of HDF5 with settings that are known to work with h5py.

For more, see:

‘https://github.com/h5py/h5py/tree/master/windows’


File: h5py.info,  Node: Thanks to<2>,  Prev: Single-step build for HDF5 on Windows,  Up: What’s new in h5py 2 3

6.1.11.8 Thanks to
..................

   * Martin Teichmann

   * Florian Rathgerber

   * Pierre de Buyl

   * Thomas Caswell

   * Andy Salnikov

   * Darren Dale

   * Robert David Grant

   * Toon Verstraelen

   * Many others who contributed bug reports


File: h5py.info,  Node: What’s new in h5py 2 2,  Next: What’s new in h5py 2 1,  Prev: What’s new in h5py 2 3,  Up: “What’s new” documents

6.1.12 What’s new in h5py 2.2
-----------------------------

* Menu:

* Support for Parallel HDF5::
* Support for Python 3.3: Support for Python 3 3.
* Mini float support (issue #141): Mini float support issue #141.
* HDF5 scale/offset filter::
* Field indexing is now allowed when writing to a dataset (issue #42): Field indexing is now allowed when writing to a dataset issue #42.
* Region references preserve shape (issue #295): Region references preserve shape issue #295.
* Committed types can be linked to datasets and attributes::
* move method on Group objects::


File: h5py.info,  Node: Support for Parallel HDF5,  Next: Support for Python 3 3,  Up: What’s new in h5py 2 2

6.1.12.1 Support for Parallel HDF5
..................................

On UNIX platforms, you can now take advantage of MPI and Parallel HDF5.
Cython, ‘mpi4py’ and an MPI-enabled build of HDF5 are required..  See
*note Parallel HDF5: 26. in the documentation for details.


File: h5py.info,  Node: Support for Python 3 3,  Next: Mini float support issue #141,  Prev: Support for Parallel HDF5,  Up: What’s new in h5py 2 2

6.1.12.2 Support for Python 3.3
...............................

Python 3.3 is now officially supported.


File: h5py.info,  Node: Mini float support issue #141,  Next: HDF5 scale/offset filter,  Prev: Support for Python 3 3,  Up: What’s new in h5py 2 2

6.1.12.3 Mini float support (issue #141)
........................................

Two-byte floats (NumPy ‘float16’) are supported.


File: h5py.info,  Node: HDF5 scale/offset filter,  Next: Field indexing is now allowed when writing to a dataset issue #42,  Prev: Mini float support issue #141,  Up: What’s new in h5py 2 2

6.1.12.4 HDF5 scale/offset filter
.................................

The Scale/Offset filter added in HDF5 1.8 is now available.


File: h5py.info,  Node: Field indexing is now allowed when writing to a dataset issue #42,  Next: Region references preserve shape issue #295,  Prev: HDF5 scale/offset filter,  Up: What’s new in h5py 2 2

6.1.12.5 Field indexing is now allowed when writing to a dataset (issue #42)
............................................................................

H5py has long supported reading only certain fields from a dataset:

     >>> dset = f.create_dataset('x', (100,), dtype=np.dtype([('a', 'f'), ('b', 'i')]))
     >>> out = dset['a', 0:100:10]
     >>> out.dtype
     dtype('float32')

Now, field names are also allowed when writing to a dataset:

     >>> dset['a', 20:50] = 1.0


File: h5py.info,  Node: Region references preserve shape issue #295,  Next: Committed types can be linked to datasets and attributes,  Prev: Field indexing is now allowed when writing to a dataset issue #42,  Up: What’s new in h5py 2 2

6.1.12.6 Region references preserve shape (issue #295)
......................................................

Previously, region references always resulted in a 1D selection, even
when 2D slicing was used:

     >>> dset = f.create_dataset('x', (10, 10))
     >>> ref = dset.regionref[0:5,0:5]
     >>> out = dset[ref]
     >>> out.shape
     (25,)

Shape is now preserved:

     >>> out = dset[ref]
     >>> out.shape
     (5, 5)

Additionally, the shape of both the target dataspace and the selection
shape can be determined via new methods on the ‘regionref’ proxy (now
available on both datasets and groups):

     >>> f.regionref.shape(ref)
     (10, 10)
     >>> f.regionref.selection(ref)
     (5, 5)


File: h5py.info,  Node: Committed types can be linked to datasets and attributes,  Next: move method on Group objects,  Prev: Region references preserve shape issue #295,  Up: What’s new in h5py 2 2

6.1.12.7 Committed types can be linked to datasets and attributes
.................................................................

HDF5 supports “shared” named types stored in the file:

     >>> f['name'] = np.dtype("int64")

You can now use these types when creating a new dataset or attribute,
and HDF5 will “link” the dataset type to the named type:

     >>> dset = f.create_dataset('int dataset', (10,), dtype=f['name'])
     >>> f.attrs.create('int scalar attribute', shape=(), dtype=f['name'])


File: h5py.info,  Node: move method on Group objects,  Prev: Committed types can be linked to datasets and attributes,  Up: What’s new in h5py 2 2

6.1.12.8 ‘move’ method on Group objects
.......................................

It’s no longer necessary to move objects in a file by manually
re-linking them:

     >>> f.create_group('a')
     >>> f['b'] = f['a']
     >>> del f['a']

The method ‘Group.move’ allows this to be performed in one step:

     >>> f.move('a', 'b')

Both the source and destination must be in the same file.


File: h5py.info,  Node: What’s new in h5py 2 1,  Next: What’s new in h5py 2 0,  Prev: What’s new in h5py 2 2,  Up: “What’s new” documents

6.1.13 What’s new in h5py 2.1
-----------------------------

* Menu:

* Dimension scales::
* Unicode strings allowed in attributes::
* Dataset size property::
* Dataset.value property is now deprecated.: Dataset value property is now deprecated.
* Bug fixes: Bug fixes<5>.


File: h5py.info,  Node: Dimension scales,  Next: Unicode strings allowed in attributes,  Up: What’s new in h5py 2 1

6.1.13.1 Dimension scales
.........................

H5py now supports the Dimension Scales feature of HDF5!  Thanks to
Darren Dale for implementing this.  You can find more information on
using scales in the *note Dimension Scales: a9. section of the docs.


File: h5py.info,  Node: Unicode strings allowed in attributes,  Next: Dataset size property,  Prev: Dimension scales,  Up: What’s new in h5py 2 1

6.1.13.2 Unicode strings allowed in attributes
..............................................

Group, dataset and attribute names in h5py 2.X can all be given as
unicode.  Now, you can also store (scalar) unicode data in attribute
values as well:

     >>> myfile.attrs['x'] = u"I'm a Unicode string!"

Storing Unicode strings in datasets or as members of compound types is
not yet implemented.


File: h5py.info,  Node: Dataset size property,  Next: Dataset value property is now deprecated,  Prev: Unicode strings allowed in attributes,  Up: What’s new in h5py 2 1

6.1.13.3 Dataset size property
..............................

Dataset objects now expose a ‘.size’ property which provides the total
number of elements in the dataspace.


File: h5py.info,  Node: Dataset value property is now deprecated,  Next: Bug fixes<5>,  Prev: Dataset size property,  Up: What’s new in h5py 2 1

6.1.13.4 ‘Dataset.value’ property is now deprecated.
....................................................

The property ‘Dataset.value’, which dates back to h5py 1.0, is
deprecated and will be removed in a later release.  This property dumps
the entire dataset into a NumPy array.  Code using ‘.value’ should be
updated to use NumPy indexing, using ‘mydataset[...]’ or ‘mydataset[()]’
as appropriate.


File: h5py.info,  Node: Bug fixes<5>,  Prev: Dataset value property is now deprecated,  Up: What’s new in h5py 2 1

6.1.13.5 Bug fixes
..................

        * Object and region references were sometimes incorrectly
          wrapped wrapped in a ‘numpy.object_’ instance (issue 202)

        * H5py now ignores old versions of Cython (<0.13) when building
          (issue 221)

        * Link access property lists weren’t being properly tracked in
          the high level interface (issue 212)

        * Race condition fixed in identifier tracking which led to
          Python crashes (issue 151)

        * Highlevel objects will now complain if you try to bind them to
          the wrong HDF5 object types (issue 191)

        * Unit tests can now be run after installation (issue 201)


File: h5py.info,  Node: What’s new in h5py 2 0,  Prev: What’s new in h5py 2 1,  Up: “What’s new” documents

6.1.14 What’s new in h5py 2.0
-----------------------------

HDF5 for Python (h5py) 2.0 represents the first major refactoring of the
h5py codebase since the project’s launch in 2008.  Many of the most
important changes are behind the scenes, and include changes to the way
h5py interacts with the HDF5 library and Python.  These changes have
substantially improved h5py’s stability, and make it possible to use
more modern versions of HDF5 without compatibility concerns.  It is now
also possible to use h5py with Python 3.

* Menu:

* Enhancements unlikely to affect compatibility::
* Changes which may break existing code::
* Known issues::


File: h5py.info,  Node: Enhancements unlikely to affect compatibility,  Next: Changes which may break existing code,  Up: What’s new in h5py 2 0

6.1.14.1 Enhancements unlikely to affect compatibility
......................................................

   * HDF5 1.8.3 through 1.8.7 now work correctly and are officially
     supported.

   * Python 3.2 is officially supported by h5py!  Thanks especially to
     Darren Dale for getting this working.

   * Fill values can now be specified when creating a dataset.  The fill
     time is H5D_FILL_TIME_IFSET for contiguous datasets, and
     H5D_FILL_TIME_ALLOC for chunked datasets.

   * On Python 3, dictionary-style methods like Group.keys() and
     Group.values() return view-like objects instead of lists.

   * Object and region references now work correctly in compound types.

   * Zero-length dimensions for extendable axes are now allowed.

   * H5py no longer attempts to auto-import ipython on startup.

   * File format bounds can now be given when opening a high-level File
     object (keyword “libver”).


File: h5py.info,  Node: Changes which may break existing code,  Next: Known issues,  Prev: Enhancements unlikely to affect compatibility,  Up: What’s new in h5py 2 0

6.1.14.2 Changes which may break existing code
..............................................

* Menu:

* Supported HDF5/Python versions::
* Group, Dataset and Datatype constructors have changed: Group Dataset and Datatype constructors have changed.
* Unicode is now used for object names::
* File objects must be manually closed::
* Changes to scalar slicing code::
* Array scalars now always returned when indexing a dataset::
* Reading object-like data strips special type information::
* The selections module has been removed::
* The H5Error exception class has been removed (along with h5py.h5e): The H5Error exception class has been removed along with h5py h5e.
* File .mode property is now either ‘r’ or ‘r+: File mode property is now either ‘r’ or ‘r+.
* Long-deprecated dict methods have been removed::


File: h5py.info,  Node: Supported HDF5/Python versions,  Next: Group Dataset and Datatype constructors have changed,  Up: Changes which may break existing code

6.1.14.3 Supported HDF5/Python versions
.......................................

   * HDF5 1.6.X is no longer supported on any platform; following the
     release of 1.6.10 some time ago, this branch is no longer
     maintained by The HDF Group.

   * Python 2.6 or later is now required to run h5py.  This is a
     consequence of the numerous changes made to h5py for Python 3
     compatibility.

   * On Python 2.6, unittest2 is now required to run the test suite.


File: h5py.info,  Node: Group Dataset and Datatype constructors have changed,  Next: Unicode is now used for object names,  Prev: Supported HDF5/Python versions,  Up: Changes which may break existing code

6.1.14.4 Group, Dataset and Datatype constructors have changed
..............................................................

In h5py 2.0, it is no longer possible to create new groups, datasets or
named datatypes by passing names and settings to the constructors
directly.  Instead, you should use the standard Group methods
create_group and create_dataset.

The File constructor remains unchanged and is still the correct
mechanism for opening and creating files.

Code which manually creates Group, Dataset or Datatype objects will have
to be modified to use create_group or create_dataset.  File-resident
datatypes can be created by assigning a NumPy dtype to a name (e.g.
mygroup[“name”] = numpy.dtype(‘S10’)).


File: h5py.info,  Node: Unicode is now used for object names,  Next: File objects must be manually closed,  Prev: Group Dataset and Datatype constructors have changed,  Up: Changes which may break existing code

6.1.14.5 Unicode is now used for object names
.............................................

Older versions of h5py used byte strings to represent names in the file.
Starting with version 2.0, you may use either byte or unicode strings to
create objects, but object names (obj.name, etc) will generally be
returned as Unicode.

Code which may be affected:

   * Anything which uses “isinstance” or explicit type checks on names,
     expecting “str” objects.  Such checks should be removed, or changed
     to compare to “basestring” instead.

   * In Python 2.X, other parts of your application may complain if they
     are handed Unicode data which can’t be encoded down to ascii.  This
     is a general problem in Python 2.


File: h5py.info,  Node: File objects must be manually closed,  Next: Changes to scalar slicing code,  Prev: Unicode is now used for object names,  Up: Changes which may break existing code

6.1.14.6 File objects must be manually closed
.............................................

With h5py 1.3, when File objects (or low-level FileID) objects went out
of scope, the corresponding HDF5 file was closed.  This led to
surprising behavior, especially when files were opened with the
H5F_CLOSE_STRONG flag; “losing” the original File object meant that all
open groups and datasets suddenly became invalid.

Beginning with h5py 2.0, files must be manually closed, by calling the
“close” method or by using the file object as a context manager.  If you
forget to close a file, the HDF5 library will try to close it for you
when the application exits.

Please note that opening the same file multiple times (i.e.  without
closing it first) continues to result in undefined behavior.


File: h5py.info,  Node: Changes to scalar slicing code,  Next: Array scalars now always returned when indexing a dataset,  Prev: File objects must be manually closed,  Up: Changes which may break existing code

6.1.14.7 Changes to scalar slicing code
.......................................

When a scalar dataset was accessed with the syntax ‘dataset[()]’, h5py
incorrectly returned an ndarray.  H5py now correctly returns an array
scalar.  Using ‘dataset[...]’ on a scalar dataset still returns an
ndarray.


File: h5py.info,  Node: Array scalars now always returned when indexing a dataset,  Next: Reading object-like data strips special type information,  Prev: Changes to scalar slicing code,  Up: Changes which may break existing code

6.1.14.8 Array scalars now always returned when indexing a dataset
..................................................................

When using datasets of compound type, retrieving a single element
incorrectly returned a tuple of values, rather than an instance of
‘numpy.void_’ with the proper fields populated.  Among other things,
this meant you couldn’t do things like ‘dataset[index][field]’.  H5py
now always returns an array scalar, except in the case of object dtypes
(references, vlen strings).


File: h5py.info,  Node: Reading object-like data strips special type information,  Next: The selections module has been removed,  Prev: Array scalars now always returned when indexing a dataset,  Up: Changes which may break existing code

6.1.14.9 Reading object-like data strips special type information
.................................................................

In the past, reading multiple data points from dataset with vlen or
reference type returned a Numpy array with a “special dtype” (such as
those created by ‘h5py.special_dtype()’).  In h5py 2.0, all such arrays
now have a generic Numpy object dtype (‘numpy.dtype('O')’).  To get a
copy of the dataset’s dtype, always use the dataset’s dtype property
directly (‘mydataset.dtype’).


File: h5py.info,  Node: The selections module has been removed,  Next: The H5Error exception class has been removed along with h5py h5e,  Prev: Reading object-like data strips special type information,  Up: Changes which may break existing code

6.1.14.10 The selections module has been removed
................................................

Only numpy-style slicing arguments remain supported in the high level
interface.  Existing code which uses the selections module should be
refactored to use numpy slicing (and ‘numpy.s_’ as appropriate), or the
standard C-style HDF5 dataspace machinery.


File: h5py.info,  Node: The H5Error exception class has been removed along with h5py h5e,  Next: File mode property is now either ‘r’ or ‘r+,  Prev: The selections module has been removed,  Up: Changes which may break existing code

6.1.14.11 The H5Error exception class has been removed (along with h5py.h5e)
............................................................................

All h5py exceptions are now native Python exceptions, no longer
inheriting from H5Error.  RuntimeError is raised if h5py can’t figure
out what exception is appropriate… every instance of this behavior is
considered a bug.  If you see h5py raising RuntimeError please report it
so we can add the correct mapping!

The old errors module (h5py.h5e) has also been removed.  There is no
public error-management API.


File: h5py.info,  Node: File mode property is now either ‘r’ or ‘r+,  Next: Long-deprecated dict methods have been removed,  Prev: The H5Error exception class has been removed along with h5py h5e,  Up: Changes which may break existing code

6.1.14.12 File .mode property is now either ‘r’ or ‘r+
......................................................

Files can be opened using the same mode arguments as before, but now the
property File.mode will always return ‘r’ (read-only) or ‘r+’
(read-write).


File: h5py.info,  Node: Long-deprecated dict methods have been removed,  Prev: File mode property is now either ‘r’ or ‘r+,  Up: Changes which may break existing code

6.1.14.13 Long-deprecated dict methods have been removed
........................................................

Certain ancient aliases for Group/AttributeManager methods (e.g.
‘listnames’) have been removed.  Please use the standard Python dict
interface (Python 2 or Python 3 as appropriate) to interact with these
objects.


File: h5py.info,  Node: Known issues,  Prev: Changes which may break existing code,  Up: What’s new in h5py 2 0

6.1.14.14 Known issues
......................

   * Thread support has been improved in h5py 2.0.  However, we still
     recommend that for your own sanity you use locking to serialize
     access to files.

   * There are reports of crashes related to storing object and region
     references.  If this happens to you, please post on the mailing
     list or contact the h5py author directly.


File: h5py.info,  Node: Bug Reports & Contributions,  Next: Release Guide,  Prev: “What’s new” documents,  Up: Meta-info about the h5py project

6.2 Bug Reports & Contributions
===============================

Contributions and bug reports are welcome from anyone!  Some of the best
features in h5py, including thread support, dimension scales, and the
scale-offset filter, came from user code contributions.

Since we use GitHub, the workflow will be familiar to many people.  If
you have questions about the process or about the details of
implementing your feature, always feel free to ask on the Google Groups
list, either by emailing:

     <h5py@googlegroups.com>

or via the web interface at:

     ‘https://groups.google.com/forum/#!forum/h5py’

Anyone can post to this list.  Your first message will be approved by a
moderator, so don’t worry if there’s a brief delay.

This guide is divided into three sections.  The first describes how to
file a bug report.

The second describes the mechanics of how to submit a contribution to
the h5py project; for example, how to create a pull request, which
branch to base your work on, etc.  We assume you’re are familiar with
Git, the version control system used by h5py.  If not, here’s a great
place to start(1).

Finally, we describe the various subsystems inside h5py, and give
technical guidance as to how to implement your changes.

* Menu:

* How to File a Bug Report::
* How to Get Your Code into h5py::
* How to Modify h5py::

   ---------- Footnotes ----------

   (1) https://git-scm.com/book


File: h5py.info,  Node: How to File a Bug Report,  Next: How to Get Your Code into h5py,  Up: Bug Reports & Contributions

6.2.1 How to File a Bug Report
------------------------------

Bug reports are always welcome!  The issue tracker is at:

     ‘https://github.com/h5py/h5py/issues’

* Menu:

* If you’re unsure whether you’ve found a bug::
* What to include::


File: h5py.info,  Node: If you’re unsure whether you’ve found a bug,  Next: What to include,  Up: How to File a Bug Report

6.2.1.1 If you’re unsure whether you’ve found a bug
...................................................

Always feel free to ask on the mailing list (h5py at Google Groups).
Discussions there are seen by lots of people and are archived by Google.
Even if the issue you’re having turns out not to be a bug in the end,
other people can benefit from a record of the conversation.

By the way, nobody will get mad if you file a bug and it turns out to be
something else.  That’s just how software development goes.


File: h5py.info,  Node: What to include,  Prev: If you’re unsure whether you’ve found a bug,  Up: How to File a Bug Report

6.2.1.2 What to include
.......................

When filing a bug, there are two things you should include.  The first
is the output of ‘h5py.version.info’:

     >>> import h5py
     >>> print(h5py.version.info)

The second is a detailed explanation of what went wrong.  Unless the bug
is really trivial, `include code if you can', either via GitHub’s inline
markup:

     ```
         import h5py
         h5py.explode()    # Destroyed my computer!
     ```

or by uploading a code sample to Github Gist(1).

   ---------- Footnotes ----------

   (1) http://gist.github.com


File: h5py.info,  Node: How to Get Your Code into h5py,  Next: How to Modify h5py,  Prev: How to File a Bug Report,  Up: Bug Reports & Contributions

6.2.2 How to Get Your Code into h5py
------------------------------------

This section describes how to contribute changes to the h5py code base.
Before you start, be sure to read the h5py license and contributor
agreement in “license.txt”.  You can find this in the source
distribution, or view it online at the main h5py repository at GitHub.

The basic workflow is to clone h5py with git, make your changes in a
topic branch, and then create a pull request at GitHub asking to merge
the changes into the main h5py project.

Here are some tips to getting your pull requests accepted:

  1. Let people know you’re working on something.  This could mean
     posting a comment in an open issue, or sending an email to the
     mailing list.  There’s nothing wrong with just opening a pull
     request, but it might save you time if you ask for advice first.

  2. Keep your changes focused.  If you’re fixing multiple issues, file
     multiple pull requests.  Try to keep the amount of reformatting
     clutter small so the maintainers can easily see what you’ve changed
     in a diff.

  3. Unit tests are mandatory for new features.  This doesn’t mean
     hundreds (or even dozens) of tests!  Just enough to make sure the
     feature works as advertised.  The maintainers will let you know if
     more are needed.

* Menu:

* Clone the h5py repository::
* Create a topic branch for your feature::
* Implement the feature!::
* Run the tests::
* Write a release note::
* Add yourself to the author list::
* Push your changes back and open a pull request::
* Work with the maintainers::


File: h5py.info,  Node: Clone the h5py repository,  Next: Create a topic branch for your feature,  Up: How to Get Your Code into h5py

6.2.2.1 Clone the h5py repository
.................................

The best way to do this is by signing in to GitHub and cloning the h5py
project directly.  You’ll end up with a new repository under your
account; for example, if your username is ‘yourname’, the repository
would be at ‘http://github.com/yourname/h5py’.

Then, clone your new copy of h5py to your local machine:

     $ git clone http://github.com/yourname/h5py


File: h5py.info,  Node: Create a topic branch for your feature,  Next: Implement the feature!,  Prev: Clone the h5py repository,  Up: How to Get Your Code into h5py

6.2.2.2 Create a topic branch for your feature
..............................................

Check out a new branch for the bugfix or feature you’re writing:

     $ git checkout -b newfeature master

The exact name of the branch can be anything you want.  For bug fixes,
one approach is to put the issue number in the branch name.

We develop all changes against the `master' branch.  If we’re making a
bugfix release, a bot will backport merged pull requests.


File: h5py.info,  Node: Implement the feature!,  Next: Run the tests,  Prev: Create a topic branch for your feature,  Up: How to Get Your Code into h5py

6.2.2.3 Implement the feature!
..............................

You can implement the feature as a number of small changes, or as one
big commit; there’s no project policy.  Double-check to make sure you’ve
included all your files; run ‘git status’ and check the output.


File: h5py.info,  Node: Run the tests,  Next: Write a release note,  Prev: Implement the feature!,  Up: How to Get Your Code into h5py

6.2.2.4 Run the tests
.....................

The easiest way to run the tests is with tox(1):

     pip install tox  # Get tox

     tox -e py37-test-deps  # Run tests in one environment
     tox                    # Run tests in all possible environments
     tox -a                 # List defined environments

   ---------- Footnotes ----------

   (1) https://tox.readthedocs.io/en/latest/


File: h5py.info,  Node: Write a release note,  Next: Add yourself to the author list,  Prev: Run the tests,  Up: How to Get Your Code into h5py

6.2.2.5 Write a release note
............................

Changes which could affect people building and using h5py after the next
release should have a news entry.  You don’t need to do this if your
changes don’t affect usage, e.g.  adding tests or correcting comments.

In the ‘news/’ folder, make a copy of ‘TEMPLATE.rst’ named after your
branch.  Edit the new file, adding a sentence or two about what you’ve
added or fixed.  Commit this to git too.

News entries are merged into the *note what’s new documents: 11d. for
each release.  They should allow someone to quickly understand what a
new feature is, or whether a bug they care about has been fixed.  E.g.:

     Bug fixes
     ---------

     * Fix reading data for region references pointing to an empty selection.

The `Building h5py' section is for changes which affect how people build
h5py from source.  It’s not about how we make prebuilt wheels; changes
to that which make a visible difference can go in `New features' or `Bug
fixes'.


File: h5py.info,  Node: Add yourself to the author list,  Next: Push your changes back and open a pull request,  Prev: Write a release note,  Up: How to Get Your Code into h5py

6.2.2.6 Add yourself to the author list
.......................................

If it’s your first time to contribute to this project, you should add
yourself into author list(1) and follow the format below:

     - name: xxx                   # your name in Github, which can be found in your public profile if you set it up
       aliases: (optional)
         - xxx                     # your username alias in Github commits
       email: xxx                  # your email address
       alternate_emails: (optional)
         - xxx                     # if you committed your code with another email address, you can put it there
       num_commit: xxx             # the number of commits you have submitted to this project
       first_commit: xxx           # your first commit time, run `git log` to see it
       github: xxx                 # your username or account name in Github

   ---------- Footnotes ----------

   (1) https://github.com/h5py/h5py/blob/master/.authors.yml


File: h5py.info,  Node: Push your changes back and open a pull request,  Next: Work with the maintainers,  Prev: Add yourself to the author list,  Up: How to Get Your Code into h5py

6.2.2.7 Push your changes back and open a pull request
......................................................

Push your topic branch back up to your GitHub clone:

     $ git push origin newfeature

Then, create a pull request(1) based on your topic branch.

   ---------- Footnotes ----------

   (1) https://help.github.com/articles/creating-a-pull-request


File: h5py.info,  Node: Work with the maintainers,  Prev: Push your changes back and open a pull request,  Up: How to Get Your Code into h5py

6.2.2.8 Work with the maintainers
.................................

Your pull request might be accepted right away.  More commonly, the
maintainers will post comments asking you to fix minor things, like add
a few tests, clean up the style to be PEP-8 compliant, etc.

The pull request page also shows the results of building and testing the
modified code on Travis and Appveyor CI and Azure Pipelines.  Check back
after about 30 minutes to see if the build succeeded, and if not, try to
modify your changes to make it work.

When making changes after creating your pull request, just add commits
to your topic branch and push them to your GitHub repository.  Don’t try
to rebase or open a new pull request!  We don’t mind having a few extra
commits in the history, and it’s helpful to keep all the history
together in one place.


File: h5py.info,  Node: How to Modify h5py,  Prev: How to Get Your Code into h5py,  Up: Bug Reports & Contributions

6.2.3 How to Modify h5py
------------------------

This section is a little more involved, and provides tips on how to
modify h5py.  The h5py package is built in layers.  Starting from the
bottom, they are:

  1. The HDF5 C API (provided by libhdf5)

  2. Auto-generated Cython wrappers for the C API (‘api_gen.py’)

  3. Low-level interface, written in Cython, using the wrappers from (2)

  4. High-level interface, written in Python, with things like
     ‘h5py.File’.

  5. Unit test code

Rather than talk about the layers in an abstract way, the parts below
are guides to adding specific functionality to various parts of h5py.
Most sections span at least two or three of these layers.

* Menu:

* Adding a function from the HDF5 C API::
* Adding a function only available in certain versions of HDF5::
* Testing MPI-only features/code::


File: h5py.info,  Node: Adding a function from the HDF5 C API,  Next: Adding a function only available in certain versions of HDF5,  Up: How to Modify h5py

6.2.3.1 Adding a function from the HDF5 C API
.............................................

This is one of the most common contributed changes.  The example below
shows how one would add the function ‘H5Dget_storage_size’, which
determines the space on disk used by an HDF5 dataset.  This function is
already partially wrapped in h5py, so you can see how it works.

It’s recommended that you follow along, if not by actually adding the
feature then by at least opening the various files as we work through
the example.

First, get ahold of the function signature; the easiest place for this
is at the online HDF5 Reference Manual(1).  Then, add the function’s C
signature to the file ‘api_functions.txt’:

     hsize_t   H5Dget_storage_size(hid_t dset_id)

This particular signature uses types (‘hsize_t’, ‘hid_t’) which are
already defined elsewhere.  But if the function you’re adding needs a
struct or enum definition, you can add it using Cython code to the file
‘api_types_hdf5.pxd’.

The next step is to add a Cython function or method which calls the
function you added.  The h5py modules follow the naming convention of
the C API; functions starting with ‘H5D’ are wrapped in ‘h5d.pyx’.

Opening ‘h5d.pyx’, we notice that since this function takes a dataset
identifier as the first argument, it belongs as a method on the
DatasetID object.  We write a wrapper method:

     def get_storage_size(self):
         """ () => LONG storage_size

             Determine the amount of file space required for a dataset.  Note
             this only counts the space which has actually been allocated; it
             may even be zero.
         """
         return H5Dget_storage_size(self.id)

The first line of the docstring gives the method signature.  This is
necessary because Cython will use a “generic” signature like
‘method(*args, **kwds)’ when the file is compiled.  The h5py
documentation system will extract the first line and use it as the
signature.

Next, we decide whether we want to add access to this function to the
high-level interface.  That means users of the top-level ‘h5py.Dataset’
object will be able to see how much space on disk their files use.  The
high-level interface is implemented in the subpackage ‘h5py._hl’, and
the Dataset object is in module ‘dataset.py’.  Opening it up, we add a
property on the ‘Dataset’ object:

     @property
     def storagesize(self):
         """ Size (in bytes) of this dataset on disk. """
         return self.id.get_storage_size()

You’ll see that the low-level ‘DatasetID’ object is available on the
high-level ‘Dataset’ object as ‘obj.id’.  This is true of all the
high-level objects, like ‘File’ and ‘Group’ as well.

Finally (and don’t skip this step), we write `unit tests' for this
feature.  Since the feature is ultimately exposed at the high-level
interface, it’s OK to write tests for the ‘Dataset.storagesize’ property
only.  Unit tests for the high-level interface are located in the
“tests” subfolder, right near ‘dataset.py’.

It looks like the right file is ‘test_dataset.py’.  Unit tests are
implemented as methods on custom ‘unittest.UnitTest’ subclasses; each
new feature should be tested by its own new class.  In the
‘test_dataset’ module, we see there’s already a subclass called
‘BaseDataset’, which implements some simple set-up and cleanup methods
and provides a ‘h5py.File’ object as ‘obj.f’.  We’ll base our test class
on that:

     class TestStorageSize(BaseDataset):

         """
             Feature: Dataset.storagesize indicates how much space is used.
         """

         def test_empty(self):
             """ Empty datasets take no space on disk """
             dset = self.f.create_dataset("x", (100,100))
             self.assertEqual(dset.storagesize, 0)

         def test_data(self):
             """ Storage size is correct for non-empty datasets """
             dset = self.f.create_dataset("x", (100,), dtype='uint8')
             dset[...] = 42
             self.assertEqual(dset.storagesize, 100)

This set of tests would be adequate to get a pull request approved.  We
don’t test every combination under the sun (different ranks, datasets
with more than 2**32 elements, datasets with the string “kumquat” in the
name…), but the basic, commonly encountered set of conditions.

To build and test our changes, we have to do a few things.  First of
all, run the file ‘api_gen.py’ to re-generate the Cython wrappers from
‘api_functions.txt’:

     $ python api_gen.py

Then build the project, which recompiles ‘h5d.pyx’:

     $ python setup.py build

Finally, run the test suite, which includes the two methods we just
wrote:

     $ python setup.py test

If the tests pass, the feature is ready for a pull request.

   ---------- Footnotes ----------

   (1) https://support.hdfgroup.org/HDF5/doc/RM/RM_H5Front.html


File: h5py.info,  Node: Adding a function only available in certain versions of HDF5,  Next: Testing MPI-only features/code,  Prev: Adding a function from the HDF5 C API,  Up: How to Modify h5py

6.2.3.2 Adding a function only available in certain versions of HDF5
....................................................................

At the moment, h5py must be backwards-compatible all the way back to
HDF5 1.8.4.  Starting with h5py 2.2.0, it’s possible to conditionally
include functions which only appear in newer versions of HDF5.  It’s
also possible to mark functions which require Parallel HDF5.  For
example, the function ‘H5Fset_mpi_atomicity’ was introduced in HDF5
1.8.9 and requires Parallel HDF5.  Specifiers before the signature in
‘api_functions.txt’ communicate this:

     MPI 1.8.9 herr_t H5Fset_mpi_atomicity(hid_t file_id, hbool_t flag)

You can specify either, both or none of “MPI” or a version number in
“X.Y.Z” format.

In the Cython code, these show up as “preprocessor” defines ‘MPI’ and
‘HDF5_VERSION’.  So the low-level implementation (as a method on
‘h5py.h5f.FileID’) looks like this:

     IF MPI and HDF5_VERSION >= (1, 8, 9):

         def set_mpi_atomicity(self, bint atomicity):
             """ (BOOL atomicity)

             For MPI-IO driver, set to atomic (True), which guarantees sequential
             I/O semantics, or non-atomic (False), which improves  performance.

             Default is False.

             Feature requires: 1.8.9 and Parallel HDF5
             """
             H5Fset_mpi_atomicity(self.id, <hbool_t>atomicity)

High-level code can check the version of the HDF5 library, or check to
see if the method is present on ‘FileID’ objects.


File: h5py.info,  Node: Testing MPI-only features/code,  Prev: Adding a function only available in certain versions of HDF5,  Up: How to Modify h5py

6.2.3.3 Testing MPI-only features/code
......................................

Typically to run code under MPI, ‘mpirun’ must be used to start the MPI
processes.  Similarly, tests using MPI features (such as collective IO),
must also be run under ‘mpirun’.  h5py uses pytest markers (specifically
‘pytest.mark.mpi’ and other markers from pytest-mpi(1)) to specify which
tests require usage of ‘mpirun’, and will handle skipping the tests as
needed.  A simple example of how to do this is:

     @pytest.mark.mpi
     def test_mpi_feature():
        import mpi4py
        # test the MPI feature

To run these tests, you’ll need to:

  1. Have ‘tox’ installed (e.g.  via ‘pip install tox’)

  2. Have HDF5 built with MPI as per *note Building against Parallel
     HDF5: 23.

Then running:

     $ CC='mpicc' HDF5_MPI=ON tox -e py37-test-deps-mpi4py

should run the tests.  You may need to pass ‘HDF5_DIR’ depending on the
location of the HDF5 with MPI support.  You can choose which python
version to build against by changing py37 (e.g.  py36 runs python 3.6,
this is a tox feature), and test with the minimum version requirements
by using ‘mindeps’ rather than ‘deps’.

If you get an error similar to:

     There are not enough slots available in the system to satisfy the 4 slots
     that were requested by the application:
       python

     Either request fewer slots for your application, or make more slots available
     for use.

then you need to reduce the number of MPI processes you are asking MPI
to use.  If you have already reduced the number of processes requested
(or are running the default number which is 2), you will need to look up
the documentation for your MPI implementation for handling this error.
On OpenMPI (which is usually the default MPI implementation on most
systems), running:

     $ export OMPI_MCA_rmaps_base_oversubscribe=1

will instruct OpenMPI to allow more MPI processes than available cores
on your system.

If you need to pass additional environment variables to your MPI
implementation, add these variables to the ‘passenv’ setting in the
‘tox.ini’, and send us a PR with that change noting the MPI
implementation.

   ---------- Footnotes ----------

   (1) https://pytest-mpi.readthedocs.io


File: h5py.info,  Node: Release Guide,  Next: FAQ,  Prev: Bug Reports & Contributions,  Up: Meta-info about the h5py project

6.3 Release Guide
=================

h5py uses rever(1) for release management.  To install rever, use either
pip or conda:

     # pip
     $ pip install re-ver

     # conda
     $ conda install -c conda-forge rever

* Menu:

* Performing releases::

   ---------- Footnotes ----------

   (1) https://regro.github.io/rever-docs/


File: h5py.info,  Node: Performing releases,  Up: Release Guide

6.3.1 Performing releases
-------------------------

Once rever is installed, always run the ‘check’ command to make sure
that everything you need to perform the release is correctly installed
and that you have the correct permissions.  All rever commands should be
run in the root level of the repository.

`Step 1 (repeat until successful)'

     $ rever check

Resolve any issues that may have come up, and keep running ‘rever check’
until it passes.  After it is successful, simply pass the version number
you want to release (e.g.  ‘X.Y.Z’) into the rever command.

`Step 2'

     $ rever X.Y.Z

You probably want to make sure (with ‘git tag’) that the new version
number is available.  If any release activities fail while running this
command, you may safely re-run this command.  You can also safely undo
previously run activities.  Please see the rever docs for more details.


File: h5py.info,  Node: FAQ,  Next: Licenses and legal info,  Prev: Release Guide,  Up: Meta-info about the h5py project

6.4 FAQ
=======

* Menu:

* What datatypes are supported?::
* What compression/processing filters are supported?::
* What file drivers are available?::
* What’s the difference between h5py and PyTables?::
* Does h5py support Parallel HDF5?::
* Variable-length (VLEN) data: Variable-length VLEN data.
* Enumerated types: Enumerated types<2>.
* NumPy object types::
* Appending data to a dataset::
* Unicode::
* Exceptions::
* Development: Development<4>.


File: h5py.info,  Node: What datatypes are supported?,  Next: What compression/processing filters are supported?,  Up: FAQ

6.4.1 What datatypes are supported?
-----------------------------------

Below is a complete list of types for which h5py supports reading,
writing and creating datasets.  Each type is mapped to a native NumPy
type.

Fully supported types:

Type                          Precisions                                       Notes
                                                                               
--------------------------------------------------------------------------------------------------------------------
                                                                               
Integer                       1, 2, 4 or 8 byte, BE/LE, signed/unsigned
                              
                                                                               
Float                         2, 4, 8, 12, 16 byte, BE/LE
                              
                                                                               
Complex                       8 or 16 byte, BE/LE                              Stored as HDF5 struct
                                                                               
                                                                               
Compound                      Arbitrary names and offsets
                              
                                                                               
Strings (fixed-length)        Any length
                              
                                                                               
Strings (variable-length)     Any length, ASCII or Unicode
                              
                                                                               
Opaque (kind ‘V’)             Any length
                              
                                                                               
Boolean                       NumPy 1-byte bool                                Stored as HDF5 enum
                                                                               
                                                                               
Array                         Any supported type
                              
                                                                               
Enumeration                   Any NumPy integer type                           Read/write as integers
                                                                               
                                                                               
References                    Region and object
                              
                                                                               
Variable length array         Any supported type                               See *note Special Types: e6.
                                                                               

Other numpy dtypes, such as datetime64 and timedelta64, can optionally
be stored in HDF5 opaque data using ‘opaque_dtype()’.  h5py will read
this data back with the same dtype, but other software probably will not
understand it.

Unsupported types:

Type                          Status
                              
-------------------------------------------------------------------------------
                              
HDF5 “time” type

NumPy “U” strings             No HDF5 equivalent
                              
                              
NumPy generic “O”             Not planned
                              


File: h5py.info,  Node: What compression/processing filters are supported?,  Next: What file drivers are available?,  Prev: What datatypes are supported?,  Up: FAQ

6.4.2 What compression/processing filters are supported?
--------------------------------------------------------

Filter                                  Function                                        Availability
                                                                                        
---------------------------------------------------------------------------------------------------------------------------
                                                                                        
DEFLATE/GZIP                            Standard HDF5 compression                       All platforms
                                                                                        
                                                                                        
SHUFFLE                                 Increase compression ratio                      All platforms
                                                                                        
                                                                                        
FLETCHER32                              Error detection                                 All platforms
                                                                                        
                                                                                        
Scale-offset                            Integer/float scaling and truncation            All platforms
                                                                                        
                                                                                        
SZIP                                    Fast, patented compression for int/float           * UNIX: if supplied with
                                                                                             HDF5.
                                                                                        
                                                                                           * Windows: read-only
                                                                                        
                                                                                        
LZF(1)                                  Very fast compression, all types                Ships with h5py, C source
                                                                                        available
                                                                                        

   ---------- Footnotes ----------

   (1) http://h5py.org/lzf


File: h5py.info,  Node: What file drivers are available?,  Next: What’s the difference between h5py and PyTables?,  Prev: What compression/processing filters are supported?,  Up: FAQ

6.4.3 What file drivers are available?
--------------------------------------

A number of different HDF5 “drivers”, which provide different modes of
access to the filesystem, are accessible in h5py via the high-level
interface.  The currently supported drivers are:

Driver                                  Purpose                                         Notes
                                                                                        
-------------------------------------------------------------------------------------------------------------------------
                                                                                        
sec2                                    Standard optimized driver                       Default on UNIX/Windows
                                                                                        
                                                                                        
stdio                                   Buffered I/O using stdio.h
                                        
                                                                                        
core                                    In-memory file (optionally backed to disk)
                                        
                                                                                        
family                                  Multi-file driver
                                        
                                                                                        
mpio                                    Parallel HDF5 file access
                                        


File: h5py.info,  Node: What’s the difference between h5py and PyTables?,  Next: Does h5py support Parallel HDF5?,  Prev: What file drivers are available?,  Up: FAQ

6.4.4 What’s the difference between h5py and PyTables?
------------------------------------------------------

The two projects have different design goals.  PyTables presents a
database-like approach to data storage, providing features like indexing
and fast “in-kernel” queries on dataset contents.  It also has a custom
system to represent data types.

In contrast, h5py is an attempt to map the HDF5 feature set to NumPy as
closely as possible.  For example, the high-level type system uses NumPy
dtype objects exclusively, and method and attribute naming follows
Python and NumPy conventions for dictionary and array access (i.e.
“.dtype” and “.shape” attributes for datasets, ‘group[name]’ indexing
syntax for groups, etc).

Underneath the “high-level” interface to h5py (i.e.  NumPy-array-like
objects; what you’ll typically be using) is a large Cython layer which
calls into C. This “low-level” interface provides access to nearly all
of the HDF5 C API. This layer is object-oriented with respect to HDF5
identifiers, supports reference counting, automatic translation between
NumPy and HDF5 type objects, translation between the HDF5 error stack
and Python exceptions, and more.

This greatly simplifies the design of the complicated high-level
interface, by relying on the “Pythonicity” of the C API wrapping.

There’s also a PyTables perspective on this question at the PyTables
FAQ(1).

   ---------- Footnotes ----------

   (1) 
http://www.pytables.org/FAQ.html#how-does-pytables-compare-with-the-h5py-project


File: h5py.info,  Node: Does h5py support Parallel HDF5?,  Next: Variable-length VLEN data,  Prev: What’s the difference between h5py and PyTables?,  Up: FAQ

6.4.5 Does h5py support Parallel HDF5?
--------------------------------------

Starting with version 2.2, h5py supports Parallel HDF5 on UNIX
platforms.  ‘mpi4py’ is required, as well as an MPIO-enabled build of
HDF5.  Check out *note Parallel HDF5: 26. for details.


File: h5py.info,  Node: Variable-length VLEN data,  Next: Enumerated types<2>,  Prev: Does h5py support Parallel HDF5?,  Up: FAQ

6.4.6 Variable-length (VLEN) data
---------------------------------

Starting with version 2.3, all supported types can be stored in
variable-length arrays (previously only variable-length byte and unicode
strings were supported) See *note Special Types: e6. for use details.
Please note that since strings in HDF5 are encoded as ASCII or UTF-8,
NUL bytes are not allowed in strings.


File: h5py.info,  Node: Enumerated types<2>,  Next: NumPy object types,  Prev: Variable-length VLEN data,  Up: FAQ

6.4.7 Enumerated types
----------------------

HDF5 enumerated types are supported.  As NumPy has no native enum type,
they are treated on the Python side as integers with a small amount of
metadata attached to the dtype.


File: h5py.info,  Node: NumPy object types,  Next: Appending data to a dataset,  Prev: Enumerated types<2>,  Up: FAQ

6.4.8 NumPy object types
------------------------

Storage of generic objects (NumPy dtype “O”) is not implemented and not
planned to be implemented, as the design goal for h5py is to expose the
HDF5 feature set, not add to it.  However, objects picked to the
“plain-text” protocol (protocol 0) can be stored in HDF5 as strings.


File: h5py.info,  Node: Appending data to a dataset,  Next: Unicode,  Prev: NumPy object types,  Up: FAQ

6.4.9 Appending data to a dataset
---------------------------------

The short response is that h5py is NumPy-like, not database-like.
Unlike the HDF5 packet-table interface (and PyTables), there is no
concept of appending rows.  Rather, you can expand the shape of the
dataset to fit your needs.  For example, if I have a series of time
traces 1024 points long, I can create an extendable dataset to store
them:

     >>> dset = myfile.create_dataset("MyDataset", (10, 1024), maxshape=(None, 1024))
     >>> dset.shape
     (10,1024)

The keyword argument “maxshape” tells HDF5 that the first dimension of
the dataset can be expanded to any size, while the second dimension is
limited to a maximum size of 1024.  We create the dataset with room for
an initial ensemble of 10 time traces.  If we later want to store 10
more time traces, the dataset can be expanded along the first axis:

     >>> dset.resize(20, axis=0)   # or dset.resize((20,1024))
     >>> dset.shape
     (20, 1024)

Each axis can be resized up to the maximum values in “maxshape”.  Things
to note:

   * Unlike NumPy arrays, when you resize a dataset the indices of
     existing data do not change; each axis grows or shrinks
     independently

   * The dataset rank (number of dimensions) is fixed when it is created


File: h5py.info,  Node: Unicode,  Next: Exceptions,  Prev: Appending data to a dataset,  Up: FAQ

6.4.10 Unicode
--------------

As of h5py 2.0.0, Unicode is supported for file names as well as for
objects in the file.  When object names are read, they are returned as
Unicode by default.

However, HDF5 has no predefined datatype to represent fixed-width UTF-16
or UTF-32 (NumPy format) strings.  Therefore, the NumPy ‘U’ datatype is
not supported.


File: h5py.info,  Node: Exceptions,  Next: Development<4>,  Prev: Unicode,  Up: FAQ

6.4.11 Exceptions
-----------------

h5py tries to map the error codes from hdf5 to the corresponding
‘Exception’ class on the Python side.  However the HDF5 group does not
consider the error codes to be public API so we can not guarantee type
stability of the exceptions raised.


File: h5py.info,  Node: Development<4>,  Prev: Exceptions,  Up: FAQ

6.4.12 Development
------------------

* Menu:

* Building from Git::
* To build from a Git checkout;: To build from a Git checkout.


File: h5py.info,  Node: Building from Git,  Next: To build from a Git checkout,  Up: Development<4>

6.4.12.1 Building from Git
..........................

We moved to GitHub in December of 2012 (‘http://github.com/h5py/h5py’).

We use the following conventions for branches and tags:

   * master: integration branch for the next minor (or major) version

   * 2.0, 2.1, 2.2, etc: bugfix branches for released versions

   * tags 2.0.0, 2.0.1, etc: Released bugfix versions


File: h5py.info,  Node: To build from a Git checkout,  Prev: Building from Git,  Up: Development<4>

6.4.12.2 To build from a Git checkout:
......................................

Clone the project:

     $ git clone https://github.com/h5py/h5py.git
     $ cd h5py

(Optional) Choose which branch to build from (e.g.  a stable branch):

     $ git checkout 2.1

Build the project.  If given, /path/to/hdf5 should point to a directory
containing a compiled, shared-library build of HDF5 (containing things
like “include” and “lib”):

     $ python setup.py build [--hdf5=/path/to/hdf5]

(Optional) Run the unit tests:

     $ python setup.py test

Report any failing tests to the mailing list (h5py at googlegroups), or
by filing a bug report at GitHub.


File: h5py.info,  Node: Licenses and legal info,  Prev: FAQ,  Up: Meta-info about the h5py project

6.5 Licenses and legal info
===========================

* Menu:

* Copyright Notice and Statement for the h5py Project::
* HDF5 Copyright Statement::
* PyTables Copyright Statement::
* stdint.h (Windows version) License: stdint h Windows version License.
* Python license::


File: h5py.info,  Node: Copyright Notice and Statement for the h5py Project,  Next: HDF5 Copyright Statement,  Up: Licenses and legal info

6.5.1 Copyright Notice and Statement for the h5py Project
---------------------------------------------------------

     Copyright (c) 2008 Andrew Collette and contributors
     All rights reserved.

     Redistribution and use in source and binary forms, with or without
     modification, are permitted provided that the following conditions are
     met:

     1. Redistributions of source code must retain the above copyright
        notice, this list of conditions and the following disclaimer.

     2. Redistributions in binary form must reproduce the above copyright
        notice, this list of conditions and the following disclaimer in the
        documentation and/or other materials provided with the
        distribution.

     3. Neither the name of the copyright holder nor the names of its
        contributors may be used to endorse or promote products derived from
        this software without specific prior written permission.

     THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
     "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
     LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
     A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
     HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
     SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
     LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
     DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
     THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
     (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
     OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.


File: h5py.info,  Node: HDF5 Copyright Statement,  Next: PyTables Copyright Statement,  Prev: Copyright Notice and Statement for the h5py Project,  Up: Licenses and legal info

6.5.2 HDF5 Copyright Statement
------------------------------

     HDF5 (Hierarchical Data Format 5) Software Library and Utilities
     Copyright 2006-2007 by The HDF Group (THG).

     NCSA HDF5 (Hierarchical Data Format 5) Software Library and Utilities
     Copyright 1998-2006 by the Board of Trustees of the University of Illinois.

     All rights reserved.

     Contributors: National Center for Supercomputing Applications (NCSA)
     at the University of Illinois, Fortner Software, Unidata Program
     Center (netCDF), The Independent JPEG Group (JPEG), Jean-loup Gailly
     and Mark Adler (gzip), and Digital Equipment Corporation (DEC).

     Redistribution and use in source and binary forms, with or without
     modification, are permitted for any purpose (including commercial
     purposes) provided that the following conditions are met:

        1. Redistributions of source code must retain the above copyright
     notice, this list of conditions, and the following disclaimer.
        2. Redistributions in binary form must reproduce the above
     copyright notice, this list of conditions, and the following
     disclaimer in the documentation and/or materials provided with the
     distribution.
        3. In addition, redistributions of modified forms of the source or
     binary code must carry prominent notices stating that the original
     code was changed and the date of the change.
        4. All publications or advertising materials mentioning features or
     use of this software are asked, but not required, to acknowledge that
     it was developed by The HDF Group and by the National Center for
     Supercomputing Applications at the University of Illinois at
     Urbana-Champaign and credit the contributors.
        5. Neither the name of The HDF Group, the name of the University,
     nor the name of any Contributor may be used to endorse or promote
     products derived from this software without specific prior written
     permission from THG, the University, or the Contributor, respectively.

     DISCLAIMER: THIS SOFTWARE IS PROVIDED BY THE HDF GROUP (THG) AND THE
     CONTRIBUTORS "AS IS" WITH NO WARRANTY OF ANY KIND, EITHER EXPRESSED OR
     IMPLIED. In no event shall THG or the Contributors be liable for any
     damages suffered by the users arising out of the use of this software,
     even if advised of the possibility of such damage.

     Portions of HDF5 were developed with support from the University of
     California, Lawrence Livermore National Laboratory (UC LLNL). The
     following statement applies to those portions of the product and must
     be retained in any redistribution of source code, binaries,
     documentation, and/or accompanying materials:

     This work was partially produced at the University of California,
     Lawrence Livermore National Laboratory (UC LLNL) under contract
     no. W-7405-ENG-48 (Contract 48) between the U.S. Department of Energy
     (DOE) and The Regents of the University of California (University) for
     the operation of UC LLNL.

     DISCLAIMER: This work was prepared as an account of work sponsored by
     an agency of the United States Government. Neither the United States
     Government nor the University of California nor any of their
     employees, makes any warranty, express or implied, or assumes any
     liability or responsibility for the accuracy, completeness, or
     usefulness of any information, apparatus, product, or process
     disclosed, or represents that its use would not infringe privately-
     owned rights. Reference herein to any specific commercial products,
     process, or service by trade name, trademark, manufacturer, or
     otherwise, does not necessarily constitute or imply its endorsement,
     recommendation, or favoring by the United States Government or the
     University of California. The views and opinions of authors expressed
     herein do not necessarily state or reflect those of the United States
     Government or the University of California, and shall not be used for
     advertising or product endorsement purposes.


File: h5py.info,  Node: PyTables Copyright Statement,  Next: stdint h Windows version License,  Prev: HDF5 Copyright Statement,  Up: Licenses and legal info

6.5.3 PyTables Copyright Statement
----------------------------------

     Copyright Notice and Statement for PyTables Software Library and Utilities:

     Copyright (c) 2002, 2003, 2004  Francesc Altet
     Copyright (c) 2005, 2006, 2007  Carabos Coop. V.
     All rights reserved.

     Redistribution and use in source and binary forms, with or without
     modification, are permitted provided that the following conditions are
     met:

     a. Redistributions of source code must retain the above copyright
        notice, this list of conditions and the following disclaimer.

     b. Redistributions in binary form must reproduce the above copyright
        notice, this list of conditions and the following disclaimer in the
        documentation and/or other materials provided with the
        distribution.

     c. Neither the name of the Carabos Coop. V. nor the names of its
        contributors may be used to endorse or promote products derived
        from this software without specific prior written permission.

     THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
     "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
     LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
     A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
     OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
     SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
     LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
     DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
     THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
     (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
     OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.


File: h5py.info,  Node: stdint h Windows version License,  Next: Python license,  Prev: PyTables Copyright Statement,  Up: Licenses and legal info

6.5.4 stdint.h (Windows version) License
----------------------------------------

     Copyright (c) 2006-2008 Alexander Chemeris

     Redistribution and use in source and binary forms, with or without
     modification, are permitted provided that the following conditions are met:

       1. Redistributions of source code must retain the above copyright notice,
          this list of conditions and the following disclaimer.

       2. Redistributions in binary form must reproduce the above copyright
          notice, this list of conditions and the following disclaimer in the
          documentation and/or other materials provided with the distribution.

       3. The name of the author may be used to endorse or promote products
          derived from this software without specific prior written permission.

     THIS SOFTWARE IS PROVIDED BY THE AUTHOR ``AS IS'' AND ANY EXPRESS OR IMPLIED
     WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF
     MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO
     EVENT SHALL THE AUTHOR BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
     SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
     PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;
     OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,
     WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR
     OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF
     ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.


File: h5py.info,  Node: Python license,  Prev: stdint h Windows version License,  Up: Licenses and legal info

6.5.5 Python license
--------------------

  1. This LICENSE AGREEMENT is between the Python Software Foundation
     (“PSF”), and the Individual or Organization (“Licensee”) accessing
     and otherwise using Python Python 2.7.5 software in source or
     binary form and its associated documentation.

  2. Subject to the terms and conditions of this License Agreement, PSF
     hereby grants Licensee a nonexclusive, royalty-free, world-wide
     license to reproduce, analyze, test, perform and/or display
     publicly, prepare derivative works, distribute, and otherwise use
     Python Python 2.7.5 alone or in any derivative version, provided,
     however, that PSF’s License Agreement and PSF’s notice of
     copyright, i.e., “Copyright 2001-2013 Python Software Foundation;
     All Rights Reserved” are retained in Python Python 2.7.5 alone or
     in any derivative version prepared by Licensee.

  3. In the event Licensee prepares a derivative work that is based on
     or incorporates Python Python 2.7.5 or any part thereof, and wants
     to make the derivative work available to others as provided herein,
     then Licensee hereby agrees to include in any such work a brief
     summary of the changes made to Python Python 2.7.5.

  4. PSF is making Python Python 2.7.5 available to Licensee on an “AS
     IS” basis.  PSF MAKES NO REPRESENTATIONS OR WARRANTIES, EXPRESS OR
     IMPLIED. BY WAY OF EXAMPLE, BUT NOT LIMITATION, PSF MAKES NO AND
     DISCLAIMS ANY REPRESENTATION OR WARRANTY OF MERCHANTABILITY OR
     FITNESS FOR ANY PARTICULAR PURPOSE OR THAT THE USE OF PYTHON Python
     2.7.5 WILL NOT INFRINGE ANY THIRD PARTY RIGHTS.

  5. PSF SHALL NOT BE LIABLE TO LICENSEE OR ANY OTHER USERS OF PYTHON
     Python 2.7.5 FOR ANY INCIDENTAL, SPECIAL, OR CONSEQUENTIAL DAMAGES
     OR LOSS AS A RESULT OF MODIFYING, DISTRIBUTING, OR OTHERWISE USING
     PYTHON Python 2.7.5, OR ANY DERIVATIVE THEREOF, EVEN IF ADVISED OF
     THE POSSIBILITY THEREOF.

  6. This License Agreement will automatically terminate upon a material
     breach of its terms and conditions.

  7. Nothing in this License Agreement shall be deemed to create any
     relationship of agency, partnership, or joint venture between PSF
     and Licensee.  This License Agreement does not grant permission to
     use PSF trademarks or trade name in a trademark sense to endorse or
     promote products or services of Licensee, or any third party.

  8. By copying, installing or otherwise using Python Python 2.7.5,
     Licensee agrees to be bound by the terms and conditions of this
     License Agreement.


File: h5py.info,  Node: Index,  Prev: Meta-info about the h5py project,  Up: Top

Index
*****

 [index ]
* Menu:

* __bool__() (h5py.Dataset method):      Reference<3>.        (line  23)
* __bool__() (h5py.File method):         Reference.           (line  70)
* __bool__() (h5py.Group method):        Reference<2>.        (line  35)
* __contains__() (h5py.AttributeManager method): Reference<4>.
                                                              (line  16)
* __contains__() (h5py.Group method):    Reference<2>.        (line  19)
* __delitem__() (h5py.AttributeManager method): Reference<4>. (line  30)
* __getitem__() (h5py.AttributeManager method): Reference<4>. (line  20)
* __getitem__() (h5py.Dataset method):   Reference<3>.        (line  13)
* __getitem__() (h5py.Group method):     Reference<2>.        (line  24)
* __iter__() (h5py.AttributeManager method): Reference<4>.    (line  12)
* __iter__() (h5py.Group method):        Reference<2>.        (line  13)
* __setitem__() (h5py.AttributeManager method): Reference<4>. (line  24)
* __setitem__() (h5py.Dataset method):   Reference<3>.        (line  18)
* __setitem__() (h5py.Group method):     Reference<2>.        (line  30)
* asstr() (h5py.Dataset method):         Reference<3>.        (line  78)
* astype() (h5py.Dataset method):        Reference<3>.        (line  61)
* AttributeManager (class in h5py):      Reference<4>.        (line   6)
* attrs (h5py.Dataset attribute):        Reference<3>.        (line 243)
* attrs (h5py.Group attribute):          Reference<2>.        (line 318)
* check_dtype() (in module h5py):        Older API.           (line  29)
* check_enum_dtype() (in module h5py):   Enumerated types.    (line  37)
* check_opaque_dtype() (in module h5py): Storing other types as opaque data.
                                                              (line  25)
* check_string_dtype() (in module h5py): Variable-length strings.
                                                              (line  49)
* check_vlen_dtype() (in module h5py):   Arbitrary vlen data. (line  32)
* chunks (h5py.Dataset attribute):       Reference<3>.        (line 189)
* close() (h5py.File method):            Reference.           (line  82)
* compression (h5py.Dataset attribute):  Reference<3>.        (line 194)
* compression_opts (h5py.Dataset attribute): Reference<3>.    (line 200)
* copy() (h5py.Group method):            Reference<2>.        (line 141)
* create() (h5py.AttributeManager method): Reference<4>.      (line  64)
* create_dataset() (h5py.Group method):  Reference<2>.        (line 200)
* create_dataset_like() (h5py.Group method): Reference<2>.    (line 282)
* create_group() (h5py.Group method):    Reference<2>.        (line 175)
* create_virtual_dataset() (h5py.Group method): Reference<2>. (line 302)
* Dataset (class in h5py):               Reference<3>.        (line   6)
* dims (h5py.Dataset attribute):         Reference<3>.        (line 239)
* driver (h5py.File attribute):          Reference.           (line 112)
* dtype (h5py.Dataset attribute):        Reference<3>.        (line 158)
* encoding (h5py.string_info attribute): Variable-length strings.
                                                              (line  58)
* enum_dtype() (in module h5py):         Enumerated types.    (line  25)
* external (h5py.Dataset attribute):     Reference<3>.        (line 228)
* ExternalLink (class in h5py):          Link classes.        (line  24)
* fields() (h5py.Dataset method):        Reference<3>.        (line  91)
* File (class in h5py):                  Reference.           (line  10)
* file (h5py.Dataset attribute):         Reference<3>.        (line 266)
* file (h5py.Group attribute):           Reference<2>.        (line 340)
* filename (h5py.ExternalLink attribute): Link classes.       (line  37)
* filename (h5py.File attribute):        Reference.           (line  94)
* fillvalue (h5py.Dataset attribute):    Reference<3>.        (line 221)
* fletcher32 (h5py.Dataset attribute):   Reference<3>.        (line 216)
* flush() (h5py.File method):            Reference.           (line  86)
* get() (h5py.AttributeManager method):  Reference<4>.        (line  56)
* get() (h5py.Group method):             Reference<2>.        (line  76)
* get_id() (h5py.AttributeManager method): Reference<4>.      (line  60)
* Group (class in h5py):                 Reference<2>.        (line   6)
* HardLink (class in h5py):              Link classes.        (line   6)
* id (h5py.Dataset attribute):           Reference<3>.        (line 247)
* id (h5py.File attribute):              Reference.           (line  90)
* id (h5py.Group attribute):             Reference<2>.        (line 322)
* is_virtual (h5py.Dataset attribute):   Reference<3>.        (line 234)
* items() (h5py.AttributeManager method): Reference<4>.       (line  48)
* items() (h5py.Group method):           Reference<2>.        (line  67)
* iter_chunks() (h5py.Dataset method):   Reference<3>.        (line 104)
* keys() (h5py.AttributeManager method): Reference<4>.        (line  34)
* keys() (h5py.Group method):            Reference<2>.        (line  50)
* len() (h5py.Dataset method):           Reference<3>.        (line 131)
* length (h5py.string_info attribute):   Variable-length strings.
                                                              (line  63)
* libver (h5py.File attribute):          Reference.           (line 117)
* make_scale() (h5py.Dataset method):    Reference<3>.        (line 135)
* maxshape (h5py.Dataset attribute):     Reference<3>.        (line 183)
* mode (h5py.File attribute):            Reference.           (line 100)
* modify() (h5py.AttributeManager method): Reference<4>.      (line  84)
* move() (h5py.Group method):            Reference<2>.        (line 128)
* name (h5py.Dataset attribute):         Reference<3>.        (line 262)
* name (h5py.Group attribute):           Reference<2>.        (line 336)
* nbytes (h5py.Dataset attribute):       Reference<3>.        (line 166)
* ndim (h5py.Dataset attribute):         Reference<3>.        (line 179)
* opaque_dtype() (in module h5py):       Storing other types as opaque data.
                                                              (line  20)
* parent (h5py.Dataset attribute):       Reference<3>.        (line 270)
* parent (h5py.Group attribute):         Reference<2>.        (line 344)
* path (h5py.ExternalLink attribute):    Link classes.        (line  41)
* path (h5py.SoftLink attribute):        Link classes.        (line  20)
* read_direct() (h5py.Dataset method):   Reference<3>.        (line  38)
* ref (h5py.Dataset attribute):          Reference<3>.        (line 252)
* ref (h5py.Group attribute):            Reference<2>.        (line 326)
* regionref (h5py.Dataset attribute):    Reference<3>.        (line 257)
* regionref (h5py.Group attribute):      Reference<2>.        (line 331)
* require_dataset() (h5py.Group method): Reference<2>.        (line 262)
* require_group() (h5py.Group method):   Reference<2>.        (line 194)
* resize() (h5py.Dataset method):        Reference<3>.        (line 123)
* scaleoffset (h5py.Dataset attribute):  Reference<3>.        (line 205)
* shape (h5py.Dataset attribute):        Reference<3>.        (line 154)
* shuffle (h5py.Dataset attribute):      Reference<3>.        (line 211)
* size (h5py.Dataset attribute):         Reference<3>.        (line 162)
* SoftLink (class in h5py):              Link classes.        (line  11)
* special_dtype() (in module h5py):      Older API.           (line  11)
* string_dtype() (in module h5py):       Variable-length strings.
                                                              (line  37)
* string_info (class in h5py):           Variable-length strings.
                                                              (line  54)
* swmr_mode (h5py.File attribute):       Reference.           (line 106)
* userblock_size (h5py.File attribute):  Reference.           (line 122)
* values() (h5py.AttributeManager method): Reference<4>.      (line  41)
* values() (h5py.Group method):          Reference<2>.        (line  59)
* VirtualLayout (class in h5py):         Reference<5>.        (line   6)
* VirtualSource (class in h5py):         Reference<5>.        (line  28)
* virtual_sources() (h5py.Dataset method): Reference<3>.      (line 146)
* visit() (h5py.Group method):           Reference<2>.        (line  98)
* visititems() (h5py.Group method):      Reference<2>.        (line 117)
* vlen_dtype() (in module h5py):         Arbitrary vlen data. (line  25)
* write_direct() (h5py.Dataset method):  Reference<3>.        (line  54)



Tag Table:
Node: Top355
Ref: index doc588
Ref: 0588
Ref: Top-Footnote-11169
Node: Where to start1198
Ref: index hdf5-for-python1286
Ref: 11286
Ref: index where-to-start1286
Ref: 21286
Node: Other resources1384
Ref: index other-resources1486
Ref: 51486
Ref: Other resources-Footnote-11684
Ref: Other resources-Footnote-21742
Ref: Other resources-Footnote-31788
Node: Introductory info1825
Ref: index introductory-info1937
Ref: 61937
Node: Quick Start Guide2027
Ref: quick doc2117
Ref: 72117
Ref: quick quick2117
Ref: 32117
Ref: quick quick-start-guide2117
Ref: 82117
Node: Install2258
Ref: quick install2339
Ref: 92339
Ref: Install-Footnote-12742
Ref: Install-Footnote-22780
Ref: Install-Footnote-32827
Node: Core concepts2878
Ref: quick core-concepts3004
Ref: a3004
Node: Appendix Creating a file4648
Ref: quick appendix-creating-a-file4720
Ref: b4720
Node: Groups and hierarchical organization5728
Ref: quick groups-and-hierarchical-organization5857
Ref: f5857
Node: Attributes8155
Ref: quick attributes8262
Ref: 118262
Node: Installation8769
Ref: build doc8859
Ref: 138859
Ref: build install8859
Ref: 48859
Ref: build installation8859
Ref: 148859
Ref: build install-recommends8894
Ref: 158894
Node: Pre-built installation recommended9634
Ref: build pre-built-installation-recommended9743
Ref: 169743
Ref: build prebuilt-install9743
Ref: 179743
Node: Python Distributions10023
Ref: build python-distributions10127
Ref: 1810127
Ref: Python Distributions-Footnote-110600
Ref: Python Distributions-Footnote-210638
Ref: Python Distributions-Footnote-310685
Node: Wheels10736
Ref: build wheels10877
Ref: 1910877
Ref: Wheels-Footnote-111296
Node: OS-Specific Package Managers11347
Ref: build os-specific-package-managers11459
Ref: 1a11459
Ref: OS-Specific Package Managers-Footnote-112026
Ref: OS-Specific Package Managers-Footnote-212051
Ref: OS-Specific Package Managers-Footnote-312085
Ref: OS-Specific Package Managers-Footnote-412117
Ref: OS-Specific Package Managers-Footnote-512149
Node: Source installation12180
Ref: build source-install12317
Ref: 1b12317
Ref: build source-installation12317
Ref: 1c12317
Node: Source installation on OSX/MacOS13473
Ref: build source-installation-on-osx-macos13607
Ref: 1f13607
Ref: Source installation on OSX/MacOS-Footnote-114049
Ref: Source installation on OSX/MacOS-Footnote-214074
Ref: Source installation on OSX/MacOS-Footnote-314108
Node: Source installation on Linux/Other Unix14140
Ref: build source-installation-on-linux-other-unix14313
Ref: 2014313
Node: Source installation on Windows14846
Ref: build source-installation-on-windows14978
Ref: 2114978
Node: Custom installation15489
Ref: build custom-install15622
Ref: 1d15622
Ref: build custom-installation15622
Ref: 2215622
Node: Building against Parallel HDF517488
Ref: build build-mpi17593
Ref: 2317593
Ref: build building-against-parallel-hdf517593
Ref: 2517593
Node: High-level API reference18280
Ref: index high-level-api-reference18392
Ref: 2718392
Node: File Objects18566
Ref: high/file doc18652
Ref: 2818652
Ref: high/file file18652
Ref: c18652
Ref: high/file file-objects18652
Ref: 2918652
Node: Opening & creating files19115
Ref: high/file file-open19207
Ref: 2a19207
Ref: high/file opening-creating-files19207
Ref: 2b19207
Node: File drivers20218
Ref: high/file file-driver20343
Ref: 2c20343
Ref: high/file file-drivers20343
Ref: 2d20343
Node: Python file-like objects22738
Ref: high/file file-fileobj22855
Ref: 3022855
Ref: high/file python-file-like-objects22855
Ref: 3122855
Node: Version bounding25119
Ref: high/file file-version25237
Ref: 3225237
Ref: high/file version-bounding25237
Ref: 3325237
Node: Closing files26796
Ref: high/file closing-files26900
Ref: 3426900
Ref: high/file file-closing26900
Ref: 3526900
Node: User block27765
Ref: high/file file-userblock27883
Ref: 3627883
Ref: high/file user-block27883
Ref: 3727883
Node: Filenames on different systems28689
Ref: high/file file-filenames28805
Ref: 3828805
Ref: high/file filenames-on-different-systems28805
Ref: 3928805
Node: macOS OSX29383
Ref: high/file macos-osx29490
Ref: 3a29490
Node: Linux and non-macOS Unix29663
Ref: high/file linux-and-non-macos-unix29786
Ref: 3b29786
Node: Windows30532
Ref: high/file windows30637
Ref: 3c30637
Node: Chunk cache31457
Ref: high/file chunk-cache31572
Ref: 3e31572
Ref: high/file file-cache31572
Ref: 3f31572
Ref: Chunk cache-Footnote-135061
Node: Reference35127
Ref: high/file reference35203
Ref: 4135203
Ref: high/file h5py File35428
Ref: d35428
Ref: high/file h5py File __bool__37992
Ref: 4437992
Ref: high/file h5py File close38337
Ref: 2e38337
Ref: high/file h5py File flush38431
Ref: 2f38431
Ref: high/file h5py File id38526
Ref: 4538526
Ref: high/file h5py File filename38610
Ref: 4238610
Ref: high/file h5py File mode38798
Ref: 4638798
Ref: high/file h5py File swmr_mode39025
Ref: 4739025
Ref: high/file h5py File driver39226
Ref: 4939226
Ref: high/file h5py File libver39380
Ref: 4a39380
Ref: high/file h5py File userblock_size39501
Ref: 4b39501
Ref: Reference-Footnote-139666
Node: Groups39720
Ref: high/group doc39823
Ref: 4c39823
Ref: high/group group39823
Ref: 1039823
Ref: high/group groups39823
Ref: 4d39823
Node: Creating groups40798
Ref: high/group creating-groups40887
Ref: 4f40887
Ref: high/group group-create40887
Ref: 5040887
Node: Dict interface and links41335
Ref: high/group dict-interface-and-links41445
Ref: 5141445
Ref: high/group group-links41445
Ref: 5241445
Node: Hard links42681
Ref: high/group group-hardlinks42769
Ref: 5342769
Ref: high/group hard-links42769
Ref: 5442769
Node: Soft links43535
Ref: high/group group-softlinks43646
Ref: 5543646
Ref: high/group soft-links43646
Ref: 5643646
Node: External links44242
Ref: high/group external-links44334
Ref: 5744334
Ref: high/group group-extlinks44334
Ref: 3d44334
Node: Reference<2>45621
Ref: high/group reference45728
Ref: 5845728
Ref: high/group h5py Group45761
Ref: 4345761
Ref: high/group h5py Group __iter__46049
Ref: 5a46049
Ref: high/group h5py Group __contains__46278
Ref: 5d46278
Ref: high/group h5py Group __getitem__46415
Ref: 5e46415
Ref: high/group h5py Group __setitem__46636
Ref: 6046636
Ref: high/group h5py Group __bool__46796
Ref: 6146796
Ref: high/group h5py Group keys47340
Ref: 6247340
Ref: high/group h5py Group values47602
Ref: 6347602
Ref: high/group h5py Group items47823
Ref: 6447823
Ref: high/group h5py Group get48054
Ref: 6548054
Ref: high/group h5py Group visit48936
Ref: 5b48936
Ref: high/group h5py Group visititems49702
Ref: 5c49702
Ref: high/group h5py Group move50079
Ref: 6950079
Ref: high/group h5py Group copy50492
Ref: 6a50492
Ref: high/group h5py Group create_group51731
Ref: 5951731
Ref: high/group h5py Group require_group52362
Ref: 6b52362
Ref: high/group h5py Group create_dataset52602
Ref: 6c52602
Ref: high/group h5py Group require_dataset55312
Ref: 7155312
Ref: high/group h5py Group create_dataset_like56085
Ref: 7256085
Ref: high/group h5py Group create_virtual_dataset56846
Ref: 7356846
Ref: high/group h5py Group attrs57385
Ref: 7657385
Ref: high/group h5py Group id57461
Ref: 7757461
Ref: high/group h5py Group ref57560
Ref: 7857560
Ref: high/group h5py Group regionref57695
Ref: 7a57695
Ref: high/group h5py Group name57848
Ref: 7c57848
Ref: high/group h5py Group file57928
Ref: 7d57928
Ref: high/group h5py Group parent58018
Ref: 7e58018
Ref: Reference<2>-Footnote-158142
Ref: Reference<2>-Footnote-258197
Node: Link classes58252
Ref: high/group link-classes58326
Ref: 7f58326
Ref: high/group h5py HardLink58365
Ref: 6658365
Ref: high/group h5py SoftLink58501
Ref: 6758501
Ref: high/group h5py SoftLink path58800
Ref: 8058800
Ref: high/group h5py ExternalLink58860
Ref: 6858860
Ref: high/group h5py ExternalLink filename59224
Ref: 8159224
Ref: high/group h5py ExternalLink path59291
Ref: 8259291
Node: Datasets59368
Ref: high/dataset doc59472
Ref: 8359472
Ref: high/dataset dataset59472
Ref: e59472
Ref: high/dataset datasets59472
Ref: 8459472
Ref: Datasets-Footnote-160625
Node: Creating datasets60695
Ref: high/dataset creating-datasets60786
Ref: 8660786
Ref: high/dataset dataset-create60786
Ref: 6d60786
Node: Reading & writing data61893
Ref: high/dataset dataset-slicing62008
Ref: 8862008
Ref: high/dataset reading-writing-data62008
Ref: 8962008
Ref: Reading & writing data-Footnote-164084
Node: Multiple indexing64133
Ref: high/dataset multiple-indexing64236
Ref: 8b64236
Node: Length and iteration64964
Ref: high/dataset dataset-iter65067
Ref: 8c65067
Ref: high/dataset length-and-iteration65067
Ref: 8d65067
Node: Chunked storage65564
Ref: high/dataset chunked-storage65680
Ref: 8f65680
Ref: high/dataset dataset-chunks65680
Ref: 4065680
Node: Resizable datasets67329
Ref: high/dataset dataset-resize67438
Ref: 9067438
Ref: high/dataset resizable-datasets67438
Ref: 9167438
Node: Filter pipeline68263
Ref: high/dataset dataset-compression68378
Ref: 2468378
Ref: high/dataset filter-pipeline68378
Ref: 9368378
Node: Lossless compression filters69256
Ref: high/dataset lossless-compression-filters69369
Ref: 9469369
Node: Custom compression filters70089
Ref: high/dataset custom-compression-filters70230
Ref: 9570230
Node: Scale-Offset filter70941
Ref: high/dataset dataset-scaleoffset71068
Ref: 6e71068
Ref: high/dataset scale-offset-filter71068
Ref: 9671068
Node: Shuffle filter72024
Ref: high/dataset dataset-shuffle72142
Ref: 6f72142
Ref: high/dataset shuffle-filter72142
Ref: 9772142
Node: Fletcher32 filter72513
Ref: high/dataset dataset-fletcher3272603
Ref: 7072603
Ref: high/dataset fletcher32-filter72603
Ref: 9872603
Node: Multi-Block Selection72955
Ref: high/dataset dataset-multi-block73066
Ref: 9973066
Ref: high/dataset multi-block-selection73066
Ref: 9a73066
Ref: Multi-Block Selection-Footnote-174055
Node: Fancy indexing74121
Ref: high/dataset dataset-fancy74275
Ref: 8a74275
Ref: high/dataset fancy-indexing74275
Ref: 9b74275
Node: Creating and Reading Empty or Null datasets and attributes75569
Ref: high/dataset creating-and-reading-empty-or-null-datasets-and-attributes75714
Ref: 9c75714
Ref: high/dataset dataset-empty75714
Ref: 8775714
Node: Reference<3>77315
Ref: high/dataset reference77437
Ref: 9d77437
Ref: high/dataset h5py Dataset77470
Ref: 4e77470
Ref: high/dataset h5py Dataset __getitem__77746
Ref: 9e77746
Ref: high/dataset h5py Dataset __setitem__77881
Ref: 9f77881
Ref: high/dataset h5py Dataset __bool__78013
Ref: a078013
Ref: high/dataset h5py Dataset read_direct78554
Ref: a178554
Ref: high/dataset h5py Dataset write_direct79368
Ref: a279368
Ref: high/dataset h5py Dataset astype79669
Ref: a379669
Ref: high/dataset h5py Dataset asstr80289
Ref: a480289
Ref: high/dataset h5py Dataset fields80716
Ref: a580716
Ref: high/dataset h5py Dataset iter_chunks81144
Ref: a681144
Ref: high/dataset h5py Dataset resize81865
Ref: 9281865
Ref: high/dataset h5py Dataset len82166
Ref: 8e82166
Ref: high/dataset h5py Dataset make_scale82237
Ref: a882237
Ref: high/dataset h5py Dataset virtual_sources82549
Ref: aa82549
Ref: high/dataset h5py Dataset shape82896
Ref: ab82896
Ref: high/dataset h5py Dataset dtype82985
Ref: ac82985
Ref: high/dataset h5py Dataset size83071
Ref: ad83071
Ref: high/dataset h5py Dataset nbytes83168
Ref: ae83168
Ref: high/dataset h5py Dataset ndim83741
Ref: af83741
Ref: high/dataset h5py Dataset maxshape83840
Ref: a783840
Ref: high/dataset h5py Dataset chunks84036
Ref: b084036
Ref: high/dataset h5py Dataset compression84187
Ref: b184187
Ref: high/dataset h5py Dataset compression_opts84393
Ref: b284393
Ref: high/dataset h5py Dataset scaleoffset84520
Ref: b384520
Ref: high/dataset h5py Dataset shuffle84738
Ref: b484738
Ref: high/dataset h5py Dataset fletcher3284863
Ref: b584863
Ref: high/dataset h5py Dataset fillvalue84999
Ref: b684999
Ref: high/dataset h5py Dataset external85289
Ref: b785289
Ref: high/dataset h5py Dataset is_virtual85537
Ref: b885537
Ref: high/dataset h5py Dataset dims85660
Ref: b985660
Ref: high/dataset h5py Dataset attrs85735
Ref: ba85735
Ref: high/dataset h5py Dataset id85813
Ref: bb85813
Ref: high/dataset h5py Dataset ref85925
Ref: bc85925
Ref: high/dataset h5py Dataset regionref86062
Ref: bd86062
Ref: high/dataset h5py Dataset name86204
Ref: be86204
Ref: high/dataset h5py Dataset file86286
Ref: bf86286
Ref: high/dataset h5py Dataset parent86377
Ref: c086377
Ref: Reference<3>-Footnote-186503
Ref: Reference<3>-Footnote-286560
Ref: Reference<3>-Footnote-386615
Node: Attributes<2>86672
Ref: high/attr doc86786
Ref: c186786
Ref: high/attr attributes86786
Ref: 1286786
Ref: high/attr id186786
Ref: c286786
Node: Reference<4>87971
Ref: high/attr reference88031
Ref: c488031
Ref: high/attr h5py AttributeManager88064
Ref: c388064
Ref: high/attr h5py AttributeManager __iter__88283
Ref: c588283
Ref: high/attr h5py AttributeManager __contains__88362
Ref: c688362
Ref: high/attr h5py AttributeManager __getitem__88473
Ref: c788473
Ref: high/attr h5py AttributeManager __setitem__88544
Ref: c888544
Ref: high/attr h5py AttributeManager __delitem__88749
Ref: c988749
Ref: high/attr h5py AttributeManager keys88851
Ref: ca88851
Ref: high/attr h5py AttributeManager values88983
Ref: cb88983
Ref: high/attr h5py AttributeManager items89132
Ref: cc89132
Ref: high/attr h5py AttributeManager get89307
Ref: cd89307
Ref: high/attr h5py AttributeManager get_id89428
Ref: ce89428
Ref: high/attr h5py AttributeManager create89524
Ref: cf89524
Ref: high/attr h5py AttributeManager modify90273
Ref: d090273
Ref: Reference<4>-Footnote-191007
Node: Dimension Scales91061
Ref: high/dims doc91180
Ref: d191180
Ref: high/dims dimension-scales91180
Ref: a991180
Ref: high/dims id191180
Ref: d291180
Ref: Dimension Scales-Footnote-194936
Ref: Dimension Scales-Footnote-294992
Node: Low-Level API95060
Ref: high/lowlevel doc95157
Ref: d395157
Ref: high/lowlevel low-level-api95157
Ref: d495157
Ref: Low-Level API-Footnote-196393
Ref: Low-Level API-Footnote-296423
Ref: Low-Level API-Footnote-396489
Ref: Low-Level API-Footnote-496543
Ref: Low-Level API-Footnote-596598
Ref: Low-Level API-Footnote-696655
Ref: Low-Level API-Footnote-796709
Ref: Low-Level API-Footnote-896763
Ref: Low-Level API-Footnote-996818
Node: Advanced topics96875
Ref: index advanced-topics97002
Ref: d597002
Node: Configuring h5py97283
Ref: config doc97371
Ref: d697371
Ref: config configuring-h5py97371
Ref: d797371
Node: Library configuration97462
Ref: config library-configuration97550
Ref: d897550
Node: IPython98808
Ref: config ipython98896
Ref: d998896
Node: Special types100678
Ref: special doc100790
Ref: da100790
Ref: special id1100790
Ref: db100790
Ref: special special-types100790
Ref: dc100790
Node: How special types are represented101260
Ref: special how-special-types-are-represented101373
Ref: dd101373
Node: Variable-length strings101986
Ref: special variable-length-strings102127
Ref: de102127
Ref: special h5py string_dtype103361
Ref: e0103361
Ref: special h5py check_string_dtype103691
Ref: e1103691
Ref: special h5py string_info103847
Ref: e2103847
Ref: special h5py string_info encoding103937
Ref: e3103937
Ref: special h5py string_info length104090
Ref: e4104090
Node: Arbitrary vlen data104233
Ref: special arbitrary-vlen-data104357
Ref: e5104357
Ref: special vlen104357
Ref: e6104357
Ref: special h5py vlen_dtype105020
Ref: e7105020
Ref: special h5py check_vlen_dtype105203
Ref: e8105203
Node: Enumerated types105357
Ref: special enumerated-types105486
Ref: e9105486
Ref: special h5py enum_dtype106151
Ref: ea106151
Ref: special h5py check_enum_dtype106490
Ref: eb106490
Node: Object and region references106649
Ref: special object-and-region-references106793
Ref: ec106793
Node: Storing other types as opaque data106910
Ref: special opaque-dtypes107047
Ref: ed107047
Ref: special storing-other-types-as-opaque-data107047
Ref: ee107047
Ref: special h5py opaque_dtype107661
Ref: ef107661
Ref: special h5py check_opaque_dtype107782
Ref: f0107782
Node: Older API108260
Ref: special older-api108360
Ref: f1108360
Ref: special h5py special_dtype108617
Ref: f2108617
Ref: special h5py check_dtype109231
Ref: f3109231
Node: Strings in HDF5109867
Ref: strings doc109991
Ref: f4109991
Ref: strings strings109991
Ref: df109991
Ref: strings strings-in-hdf5109991
Ref: f5109991
Ref: Strings in HDF5-Footnote-1110357
Node: Reading strings110410
Ref: strings reading-strings110499
Ref: f6110499
Node: Storing strings110928
Ref: strings storing-strings111054
Ref: f7111054
Node: What about NumPy’s U type?112046
Ref: strings what-about-numpy-s-u-type112124
Ref: f8112124
Node: How to store raw binary data112465
Ref: strings how-to-store-raw-binary-data112588
Ref: f9112588
Ref: strings str-binary112588
Ref: fa112588
Node: Object names113217
Ref: strings object-names113334
Ref: fb113334
Node: Encodings113866
Ref: strings encodings113946
Ref: fc113946
Ref: strings str-encodings113946
Ref: fd113946
Ref: Encodings-Footnote-1116695
Ref: Encodings-Footnote-2116742
Ref: Encodings-Footnote-3116916
Node: Object and Region References116985
Ref: refs doc117109
Ref: fe117109
Ref: refs object-and-region-references117109
Ref: ff117109
Ref: refs refs117109
Ref: 5f117109
Node: Using object references118150
Ref: refs refs-object118268
Ref: 79118268
Ref: refs using-object-references118268
Ref: 100118268
Node: Using region references118871
Ref: refs refs-region119029
Ref: 7b119029
Ref: refs using-region-references119029
Ref: 101119029
Node: Storing references in a dataset119931
Ref: refs storing-references-in-a-dataset120100
Ref: 102120100
Node: Storing references in an attribute121173
Ref: refs storing-references-in-an-attribute121334
Ref: 103121334
Node: Null references121600
Ref: refs null-references121721
Ref: 104121721
Node: Parallel HDF5122074
Ref: mpi doc122217
Ref: 105122217
Ref: mpi parallel122217
Ref: 26122217
Ref: mpi parallel-hdf5122217
Ref: 106122217
Ref: Parallel HDF5-Footnote-1122910
Node: How does Parallel HDF5 work?122973
Ref: mpi how-does-parallel-hdf5-work123091
Ref: 107123091
Ref: How does Parallel HDF5 work?-Footnote-1124389
Ref: How does Parallel HDF5 work?-Footnote-2124422
Node: Building against Parallel HDF5<2>124455
Ref: mpi building-against-parallel-hdf5124611
Ref: 108124611
Node: Using Parallel HDF5 from h5py125406
Ref: mpi using-parallel-hdf5-from-h5py125574
Ref: 109125574
Node: Collective versus independent operations126566
Ref: mpi collective-versus-independent-operations126716
Ref: 10a126716
Node: MPI atomic mode127741
Ref: mpi mpi-atomic-mode127878
Ref: 10b127878
Node: More information128299
Ref: mpi more-information128387
Ref: 10c128387
Node: Single Writer Multiple Reader SWMR128626
Ref: swmr doc128761
Ref: 48128761
Ref: swmr single-writer-multiple-reader-swmr128761
Ref: 10d128761
Ref: swmr swmr128761
Ref: 10e128761
Node: What is SWMR?129002
Ref: swmr what-is-swmr129125
Ref: 10f129125
Ref: What is SWMR?-Footnote-1130928
Node: Using the SWMR feature from h5py131011
Ref: swmr using-the-swmr-feature-from-h5py131151
Ref: 110131151
Node: Examples132781
Ref: swmr examples132899
Ref: 111132899
Node: Dataset monitor with inotify133183
Ref: swmr dataset-monitor-with-inotify133301
Ref: 112133301
Ref: Dataset monitor with inotify-Footnote-1136691
Node: Multiprocess concurrent write and read136738
Ref: swmr multiprocess-concurrent-write-and-read136856
Ref: 113136856
Node: Virtual Datasets VDS144034
Ref: vds doc144147
Ref: 74144147
Ref: vds vds144147
Ref: 114144147
Ref: vds virtual-datasets-vds144147
Ref: 115144147
Node: What are virtual datasets?144583
Ref: vds what-are-virtual-datasets144706
Ref: 116144706
Ref: What are virtual datasets?-Footnote-1145514
Node: Creating virtual datasets in h5py145607
Ref: vds creating-vds145750
Ref: 117145750
Ref: vds creating-virtual-datasets-in-h5py145750
Ref: 118145750
Node: Examples<2>147462
Ref: vds examples147591
Ref: 11a147591
Ref: Examples<2>-Footnote-1148285
Ref: Examples<2>-Footnote-2148357
Ref: Examples<2>-Footnote-3148441
Ref: Examples<2>-Footnote-4148550
Ref: Examples<2>-Footnote-5148639
Ref: Examples<2>-Footnote-6148715
Ref: Examples<2>-Footnote-7148792
Node: Reference<5>148872
Ref: vds reference148959
Ref: 11b148959
Ref: vds h5py VirtualLayout148992
Ref: 75148992
Ref: vds h5py VirtualSource149754
Ref: 119149754
Ref: Reference<5>-Footnote-1150986
Node: Meta-info about the h5py project151051
Ref: index meta-info-about-the-h5py-project151159
Ref: 11c151159
Node: “What’s new” documents151359
Ref: whatsnew/index doc151490
Ref: 11d151490
Ref: whatsnew/index what-s-new-documents151490
Ref: 11e151490
Ref: whatsnew/index whatsnew151490
Ref: 11f151490
Node: What’s new in h5py 3 1152394
Ref: whatsnew/3 1 doc152514
Ref: 120152514
Ref: whatsnew/3 1 what-s-new-in-h5py-3-1152514
Ref: 121152514
Node: Bug fixes152633
Ref: whatsnew/3 1 bug-fixes152723
Ref: 122152723
Ref: Bug fixes-Footnote-1153156
Ref: Bug fixes-Footnote-2153205
Ref: Bug fixes-Footnote-3153254
Node: Building h5py153303
Ref: whatsnew/3 1 building-h5py153413
Ref: 123153413
Node: Development153583
Ref: whatsnew/3 1 development153675
Ref: 124153675
Ref: Development-Footnote-1153893
Node: What’s new in h5py 3 0153946
Ref: whatsnew/3 0 doc154100
Ref: 125154100
Ref: whatsnew/3 0 what-s-new-in-h5py-3-0154100
Ref: 126154100
Node: New features154345
Ref: whatsnew/3 0 new-features154456
Ref: 127154456
Ref: New features-Footnote-1157962
Ref: New features-Footnote-2158011
Ref: New features-Footnote-3158058
Ref: New features-Footnote-4158107
Ref: New features-Footnote-5158154
Ref: New features-Footnote-6158231
Ref: New features-Footnote-7158303
Ref: New features-Footnote-8158375
Ref: New features-Footnote-9158457
Node: Breaking changes & deprecations158526
Ref: whatsnew/3 0 breaking-changes-deprecations158669
Ref: 128158669
Ref: Breaking changes & deprecations-Footnote-1161260
Node: Exposing HDF5 functions161309
Ref: whatsnew/3 0 exposing-hdf5-functions161452
Ref: 129161452
Node: Bug fixes<2>161874
Ref: whatsnew/3 0 bug-fixes162002
Ref: 12a162002
Ref: Bug fixes<2>-Footnote-1164716
Ref: Bug fixes<2>-Footnote-2164765
Ref: Bug fixes<2>-Footnote-3164814
Ref: Bug fixes<2>-Footnote-4164862
Ref: Bug fixes<2>-Footnote-5164911
Ref: Bug fixes<2>-Footnote-6164958
Ref: Bug fixes<2>-Footnote-7165032
Ref: Bug fixes<2>-Footnote-8165081
Ref: Bug fixes<2>-Footnote-9165130
Ref: Bug fixes<2>-Footnote-10165178
Node: Building h5py<2>165228
Ref: whatsnew/3 0 building-h5py165347
Ref: 12b165347
Ref: Building h5py<2>-Footnote-1167007
Node: Development<2>167056
Ref: whatsnew/3 0 development167154
Ref: 12c167154
Ref: Development<2>-Footnote-1167368
Node: What’s new in h5py 2 10167415
Ref: whatsnew/2 10 doc167569
Ref: 12d167569
Ref: whatsnew/2 10 what-s-new-in-h5py-2-10167569
Ref: 12e167569
Node: New features<2>167826
Ref: whatsnew/2 10 new-features167922
Ref: 12f167922
Ref: New features<2>-Footnote-1169751
Ref: New features<2>-Footnote-2169799
Ref: New features<2>-Footnote-3169847
Ref: New features<2>-Footnote-4169896
Ref: New features<2>-Footnote-5169945
Ref: New features<2>-Footnote-6169994
Ref: New features<2>-Footnote-7170043
Ref: New features<2>-Footnote-8170091
Ref: New features<2>-Footnote-9170140
Ref: New features<2>-Footnote-10170194
Ref: New features<2>-Footnote-11170244
Node: Deprecations170294
Ref: whatsnew/2 10 deprecations170425
Ref: 130170425
Ref: Deprecations-Footnote-1171447
Node: Exposing HDF5 functions<2>171496
Ref: whatsnew/2 10 exposing-hdf5-functions171620
Ref: 131171620
Ref: Exposing HDF5 functions<2>-Footnote-1172058
Ref: Exposing HDF5 functions<2>-Footnote-2172106
Ref: Exposing HDF5 functions<2>-Footnote-3172155
Ref: Exposing HDF5 functions<2>-Footnote-4172211
Ref: Exposing HDF5 functions<2>-Footnote-5172260
Ref: Exposing HDF5 functions<2>-Footnote-6172309
Node: Bugfixes172358
Ref: whatsnew/2 10 bugfixes172486
Ref: 132172486
Ref: Bugfixes-Footnote-1173965
Ref: Bugfixes-Footnote-2174014
Ref: Bugfixes-Footnote-3174063
Ref: Bugfixes-Footnote-4174112
Ref: Bugfixes-Footnote-5174161
Ref: Bugfixes-Footnote-6174209
Ref: Bugfixes-Footnote-7174258
Ref: Bugfixes-Footnote-8174303
Ref: Bugfixes-Footnote-9174352
Ref: Bugfixes-Footnote-10174401
Ref: Bugfixes-Footnote-11174451
Ref: Bugfixes-Footnote-12174501
Ref: Bugfixes-Footnote-13174551
Ref: Bugfixes-Footnote-14174601
Node: Building h5py<3>174651
Ref: whatsnew/2 10 building-h5py174767
Ref: 133174767
Ref: Building h5py<3>-Footnote-1175662
Ref: Building h5py<3>-Footnote-2175711
Ref: Building h5py<3>-Footnote-3175760
Ref: Building h5py<3>-Footnote-4175809
Ref: Building h5py<3>-Footnote-5175858
Ref: Building h5py<3>-Footnote-6175907
Node: Development<3>175956
Ref: whatsnew/2 10 development176055
Ref: 134176055
Ref: Development<3>-Footnote-1176257
Ref: Development<3>-Footnote-2176306
Node: What’s new in h5py 2 9176346
Ref: whatsnew/2 9 doc176500
Ref: 135176500
Ref: whatsnew/2 9 what-s-new-in-h5py-2-9176500
Ref: 136176500
Node: New features<3>176711
Ref: whatsnew/2 9 new-features176820
Ref: 137176820
Ref: New features<3>-Footnote-1178170
Ref: New features<3>-Footnote-2178219
Ref: New features<3>-Footnote-3178268
Ref: New features<3>-Footnote-4178317
Ref: New features<3>-Footnote-5178366
Ref: New features<3>-Footnote-6178415
Ref: New features<3>-Footnote-7178464
Ref: New features<3>-Footnote-8178513
Ref: New features<3>-Footnote-9178562
Ref: New features<3>-Footnote-10178611
Ref: New features<3>-Footnote-11178661
Node: Exposing HDF5 functions<3>178711
Ref: whatsnew/2 9 exposing-hdf5-functions178840
Ref: 138178840
Ref: Exposing HDF5 functions<3>-Footnote-1179195
Ref: Exposing HDF5 functions<3>-Footnote-2179258
Ref: Exposing HDF5 functions<3>-Footnote-3179307
Ref: Exposing HDF5 functions<3>-Footnote-4179363
Node: Bugfixes<2>179412
Ref: whatsnew/2 9 bugfixes179548
Ref: 139179548
Ref: Bugfixes<2>-Footnote-1179745
Ref: Bugfixes<2>-Footnote-2179794
Node: Support for old Python179843
Ref: whatsnew/2 9 support-for-old-python179944
Ref: 13a179944
Node: What’s new in h5py 2 8180091
Ref: whatsnew/2 8 doc180246
Ref: 13b180246
Ref: whatsnew/2 8 what-s-new-in-h5py-2-8180246
Ref: 13c180246
Node: API changes180758
Ref: whatsnew/2 8 api-changes180845
Ref: 13d180845
Ref: API changes-Footnote-1181338
Ref: API changes-Footnote-2181386
Node: Features181434
Ref: whatsnew/2 8 features181542
Ref: 13e181542
Ref: Features-Footnote-1181790
Ref: Features-Footnote-2181838
Node: Bug fixes<3>181886
Ref: whatsnew/2 8 bug-fixes182002
Ref: 13f182002
Ref: Bug fixes<3>-Footnote-1183786
Ref: Bug fixes<3>-Footnote-2183834
Ref: Bug fixes<3>-Footnote-3183882
Ref: Bug fixes<3>-Footnote-4183930
Ref: Bug fixes<3>-Footnote-5183979
Ref: Bug fixes<3>-Footnote-6184027
Ref: Bug fixes<3>-Footnote-7184075
Ref: Bug fixes<3>-Footnote-8184123
Ref: Bug fixes<3>-Footnote-9184171
Ref: Bug fixes<3>-Footnote-10184219
Ref: Bug fixes<3>-Footnote-11184268
Ref: Bug fixes<3>-Footnote-12184317
Ref: Bug fixes<3>-Footnote-13184366
Ref: Bug fixes<3>-Footnote-14184415
Ref: Bug fixes<3>-Footnote-15184464
Ref: Bug fixes<3>-Footnote-16184513
Ref: Bug fixes<3>-Footnote-17184562
Ref: Bug fixes<3>-Footnote-18184612
Ref: Bug fixes<3>-Footnote-19184661
Ref: Bug fixes<3>-Footnote-20184710
Ref: Bug fixes<3>-Footnote-21184759
Ref: Bug fixes<3>-Footnote-22184808
Ref: Bug fixes<3>-Footnote-23184857
Ref: Bug fixes<3>-Footnote-24184906
Ref: Bug fixes<3>-Footnote-25184955
Ref: Bug fixes<3>-Footnote-26185005
Ref: Bug fixes<3>-Footnote-27185055
Node: Wheels HDF5 Version185105
Ref: whatsnew/2 8 wheels-hdf5-version185246
Ref: 140185246
Node: CI/Testing improvements and fixes185460
Ref: whatsnew/2 8 ci-testing-improvements-and-fixes185602
Ref: 141185602
Ref: CI/Testing improvements and fixes-Footnote-1186043
Ref: CI/Testing improvements and fixes-Footnote-2186091
Ref: CI/Testing improvements and fixes-Footnote-3186139
Ref: CI/Testing improvements and fixes-Footnote-4186187
Ref: CI/Testing improvements and fixes-Footnote-5186235
Node: Other changes186283
Ref: whatsnew/2 8 other-changes186433
Ref: 142186433
Ref: Other changes-Footnote-1187004
Ref: Other changes-Footnote-2187052
Ref: Other changes-Footnote-3187100
Ref: Other changes-Footnote-4187149
Ref: Other changes-Footnote-5187198
Node: Acknowledgements and Thanks187246
Ref: whatsnew/2 8 acknowledgements-and-thanks187354
Ref: 143187354
Node: What’s new in h5py 2 7 1187552
Ref: whatsnew/2 7 1 doc187707
Ref: 144187707
Ref: whatsnew/2 7 1 what-s-new-in-h5py-2-7-1187707
Ref: 145187707
Node: Bug fixes<4>187866
Ref: whatsnew/2 7 1 bug-fixes187939
Ref: 146187939
Ref: Bug fixes<4>-Footnote-1188948
Ref: Bug fixes<4>-Footnote-2188996
Ref: Bug fixes<4>-Footnote-3189044
Ref: Bug fixes<4>-Footnote-4189092
Ref: Bug fixes<4>-Footnote-5189140
Ref: Bug fixes<4>-Footnote-6189188
Ref: Bug fixes<4>-Footnote-7189236
Ref: Bug fixes<4>-Footnote-8189284
Ref: Bug fixes<4>-Footnote-9189332
Ref: Bug fixes<4>-Footnote-10189380
Ref: Bug fixes<4>-Footnote-11189429
Ref: Bug fixes<4>-Footnote-12189478
Ref: Bug fixes<4>-Footnote-13189527
Node: Fix h5py segfaulting on some Python 3 versions189576
Ref: whatsnew/2 7 1 fix-h5py-segfaulting-on-some-python-3-versions189731
Ref: 147189731
Ref: Fix h5py segfaulting on some Python 3 versions-Footnote-1190277
Ref: Fix h5py segfaulting on some Python 3 versions-Footnote-2190320
Ref: Fix h5py segfaulting on some Python 3 versions-Footnote-3190368
Ref: Fix h5py segfaulting on some Python 3 versions-Footnote-4190416
Node: Avoid unaligned memory access in conversion functions190459
Ref: whatsnew/2 7 1 avoid-unaligned-memory-access-in-conversion-functions190614
Ref: 148190614
Ref: Avoid unaligned memory access in conversion functions-Footnote-1190981
Node: What’s new in h5py 2 7191029
Ref: whatsnew/2 7 doc191184
Ref: 149191184
Ref: whatsnew/2 7 python-issue-30484191184
Ref: 14a191184
Ref: whatsnew/2 7 what-s-new-in-h5py-2-7191184
Ref: 14b191184
Node: Python 3 2 is no longer supported191604
Ref: whatsnew/2 7 python-3-2-is-no-longer-supported191729
Ref: 14c191729
Ref: Python 3 2 is no longer supported-Footnote-1192617
Node: Improved testing support192663
Ref: whatsnew/2 7 improved-testing-support192826
Ref: 14d192826
Ref: Improved testing support-Footnote-1193668
Ref: Improved testing support-Footnote-2193714
Ref: Improved testing support-Footnote-3193760
Ref: Improved testing support-Footnote-4193806
Ref: Improved testing support-Footnote-5193852
Ref: Improved testing support-Footnote-6193898
Ref: Improved testing support-Footnote-7193944
Ref: Improved testing support-Footnote-8193990
Ref: Improved testing support-Footnote-9194036
Ref: Improved testing support-Footnote-10194082
Ref: Improved testing support-Footnote-11194129
Ref: Improved testing support-Footnote-12194176
Ref: Improved testing support-Footnote-13194223
Ref: Improved testing support-Footnote-14194270
Ref: Improved testing support-Footnote-15194317
Node: Improved python compatibility194364
Ref: whatsnew/2 7 improved-python-compatibility194520
Ref: 14e194520
Ref: Improved python compatibility-Footnote-1194819
Ref: Improved python compatibility-Footnote-2194865
Node: Documentation improvements194911
Ref: whatsnew/2 7 documentation-improvements195064
Ref: 14f195064
Ref: Documentation improvements-Footnote-1195361
Ref: Documentation improvements-Footnote-2195407
Node: setup py improvements195453
Ref: whatsnew/2 7 setup-py-improvements195619
Ref: 150195619
Ref: setup py improvements-Footnote-1196031
Ref: setup py improvements-Footnote-2196077
Ref: setup py improvements-Footnote-3196123
Node: Support for additional HDF5 features added196169
Ref: whatsnew/2 7 support-for-additional-hdf5-features-added196336
Ref: 151196336
Ref: Support for additional HDF5 features added-Footnote-1196882
Ref: Support for additional HDF5 features added-Footnote-2196955
Ref: Support for additional HDF5 features added-Footnote-3197001
Ref: Support for additional HDF5 features added-Footnote-4197105
Ref: Support for additional HDF5 features added-Footnote-5197151
Ref: Support for additional HDF5 features added-Footnote-6197224
Ref: Support for additional HDF5 features added-Footnote-7197328
Node: Improvements to type system197374
Ref: whatsnew/2 7 improvements-to-type-system197536
Ref: 152197536
Ref: Improvements to type system-Footnote-1198219
Ref: Improvements to type system-Footnote-2198265
Ref: Improvements to type system-Footnote-3198311
Ref: Improvements to type system-Footnote-4198357
Ref: Improvements to type system-Footnote-5198403
Ref: Improvements to type system-Footnote-6198449
Ref: Improvements to type system-Footnote-7198495
Ref: Improvements to type system-Footnote-8198541
Ref: Improvements to type system-Footnote-9198587
Ref: Improvements to type system-Footnote-10198633
Ref: Improvements to type system-Footnote-11198680
Node: Other changes<2>198727
Ref: whatsnew/2 7 other-changes198863
Ref: 153198863
Ref: Other changes<2>-Footnote-1199558
Ref: Other changes<2>-Footnote-2199604
Ref: Other changes<2>-Footnote-3199650
Ref: Other changes<2>-Footnote-4199696
Ref: Other changes<2>-Footnote-5199742
Ref: Other changes<2>-Footnote-6199788
Ref: Other changes<2>-Footnote-7199834
Node: Acknowledgements199880
Ref: whatsnew/2 7 acknowledgements199980
Ref: 154199980
Ref: whatsnew/2 7 hdf5-file-image-operations199980
Ref: 155199980
Node: What’s new in h5py 2 6200031
Ref: whatsnew/2 6 doc200184
Ref: 156200184
Ref: whatsnew/2 6 what-s-new-in-h5py-2-6200184
Ref: 157200184
Node: Support for HDF5 Virtual Dataset API200578
Ref: whatsnew/2 6 support-for-hdf5-virtual-dataset-api200712
Ref: 158200712
Ref: Support for HDF5 Virtual Dataset API-Footnote-1201102
Node: Add MPI Collective I/O Support201148
Ref: whatsnew/2 6 add-mpi-collective-i-o-support201329
Ref: 159201329
Ref: Add MPI Collective I/O Support-Footnote-1201702
Node: Numerous build/testing/CI improvements201748
Ref: whatsnew/2 6 numerous-build-testing-ci-improvements201928
Ref: 15a201928
Ref: Numerous build/testing/CI improvements-Footnote-1202450
Ref: Numerous build/testing/CI improvements-Footnote-2202496
Ref: Numerous build/testing/CI improvements-Footnote-3202542
Ref: Numerous build/testing/CI improvements-Footnote-4202588
Ref: Numerous build/testing/CI improvements-Footnote-5202634
Node: Cleanup of codebase based on pylint202680
Ref: whatsnew/2 6 cleanup-of-codebase-based-on-pylint202852
Ref: 15b202852
Ref: Cleanup of codebase based on pylint-Footnote-1203073
Ref: Cleanup of codebase based on pylint-Footnote-2203119
Node: Fixes to low-level API203165
Ref: whatsnew/2 6 fixes-to-low-level-api203328
Ref: 15c203328
Ref: Fixes to low-level API-Footnote-1203790
Ref: Fixes to low-level API-Footnote-2203836
Ref: Fixes to low-level API-Footnote-3203882
Ref: Fixes to low-level API-Footnote-4203928
Ref: Fixes to low-level API-Footnote-5203974
Ref: Fixes to low-level API-Footnote-6204020
Ref: Fixes to low-level API-Footnote-7204066
Node: Documentation improvements<2>204112
Ref: whatsnew/2 6 documentation-improvements204256
Ref: 15d204256
Ref: Documentation improvements<2>-Footnote-1204751
Ref: Documentation improvements<2>-Footnote-2204797
Ref: Documentation improvements<2>-Footnote-3204843
Ref: Documentation improvements<2>-Footnote-4204889
Ref: Documentation improvements<2>-Footnote-5204935
Ref: Documentation improvements<2>-Footnote-6204981
Ref: Documentation improvements<2>-Footnote-7205027
Ref: Documentation improvements<2>-Footnote-8205073
Node: Other changes<3>205119
Ref: whatsnew/2 6 other-changes205260
Ref: 15e205260
Ref: Other changes<3>-Footnote-1205690
Ref: Other changes<3>-Footnote-2205736
Ref: Other changes<3>-Footnote-3205782
Ref: Other changes<3>-Footnote-4205828
Ref: Other changes<3>-Footnote-5205874
Ref: Other changes<3>-Footnote-6205920
Node: Acknowledgements<2>205966
Ref: whatsnew/2 6 acknowledgements206069
Ref: 15f206069
Ref: whatsnew/2 6 id30206069
Ref: 160206069
Node: What’s new in h5py 2 5206120
Ref: whatsnew/2 5 doc206273
Ref: 161206273
Ref: whatsnew/2 5 what-s-new-in-h5py-2-5206273
Ref: 162206273
Node: Experimental support for Single Writer Multiple Reader SWMR206546
Ref: whatsnew/2 5 experimental-support-for-single-writer-multiple-reader-swmr206689
Ref: 163206689
Ref: Experimental support for Single Writer Multiple Reader SWMR-Footnote-1207509
Node: Other changes<4>207555
Ref: whatsnew/2 5 other-changes207726
Ref: 164207726
Ref: Other changes<4>-Footnote-1209354
Ref: Other changes<4>-Footnote-2209400
Ref: Other changes<4>-Footnote-3209446
Ref: Other changes<4>-Footnote-4209492
Ref: Other changes<4>-Footnote-5209538
Ref: Other changes<4>-Footnote-6209584
Ref: Other changes<4>-Footnote-7209630
Ref: Other changes<4>-Footnote-8209676
Ref: Other changes<4>-Footnote-9209722
Ref: Other changes<4>-Footnote-10209768
Ref: Other changes<4>-Footnote-11209815
Ref: Other changes<4>-Footnote-12209862
Ref: Other changes<4>-Footnote-13209909
Ref: Other changes<4>-Footnote-14209956
Ref: Other changes<4>-Footnote-15210003
Ref: Other changes<4>-Footnote-16210050
Ref: Other changes<4>-Footnote-17210097
Node: Acknowledgements<3>210144
Ref: whatsnew/2 5 acknowledgements210247
Ref: 165210247
Ref: whatsnew/2 5 id19210247
Ref: 166210247
Node: What’s new in h5py 2 4210499
Ref: whatsnew/2 4 doc210652
Ref: 167210652
Ref: whatsnew/2 4 what-s-new-in-h5py-2-4210652
Ref: 168210652
Node: Build system changes210856
Ref: whatsnew/2 4 build-system-changes210969
Ref: 169210969
Node: Files will now auto-close211553
Ref: whatsnew/2 4 files-will-now-auto-close211701
Ref: 16a211701
Node: Thread safety improvements211993
Ref: whatsnew/2 4 thread-safety-improvements212147
Ref: 16b212147
Node: External link improvements212394
Ref: whatsnew/2 4 external-link-improvements212532
Ref: 16c212532
Node: Thanks to212751
Ref: whatsnew/2 4 thanks-to212854
Ref: 16d212854
Node: What’s new in h5py 2 3213024
Ref: whatsnew/2 3 doc213177
Ref: 16e213177
Ref: whatsnew/2 3 what-s-new-in-h5py-2-3213177
Ref: 16f213177
Node: Support for arbitrary vlen data213543
Ref: whatsnew/2 3 support-for-arbitrary-vlen-data213669
Ref: 170213669
Node: Improved exception messages214155
Ref: whatsnew/2 3 improved-exception-messages214317
Ref: 171214317
Node: Improved setuptools support215012
Ref: whatsnew/2 3 improved-setuptools-support215171
Ref: 172215171
Node: Multiple low-level additions215333
Ref: whatsnew/2 3 multiple-low-level-additions215498
Ref: 173215498
Node: Improved support for MPI features215706
Ref: whatsnew/2 3 improved-support-for-mpi-features215892
Ref: 174215892
Node: Readonly files can now be opened in default mode216151
Ref: whatsnew/2 3 readonly-files-can-now-be-opened-in-default-mode216346
Ref: 175216346
Node: Single-step build for HDF5 on Windows216592
Ref: whatsnew/2 3 single-step-build-for-hdf5-on-windows216766
Ref: 176216766
Node: Thanks to<2>217210
Ref: whatsnew/2 3 thanks-to217327
Ref: 177217327
Node: What’s new in h5py 2 2217588
Ref: whatsnew/2 2 doc217741
Ref: 178217741
Ref: whatsnew/2 2 what-s-new-in-h5py-2-2217741
Ref: 179217741
Node: Support for Parallel HDF5218315
Ref: whatsnew/2 2 support-for-parallel-hdf5218430
Ref: 17a218430
Node: Support for Python 3 3218707
Ref: whatsnew/2 2 support-for-python-3-3218860
Ref: 17b218860
Node: Mini float support issue #141218966
Ref: whatsnew/2 2 mini-float-support-issue-141219118
Ref: 17c219118
Node: HDF5 scale/offset filter219255
Ref: whatsnew/2 2 hdf5-scale-offset-filter219450
Ref: 17d219450
Node: Field indexing is now allowed when writing to a dataset issue #42219580
Ref: whatsnew/2 2 field-indexing-is-now-allowed-when-writing-to-a-dataset-issue-42219789
Ref: 17e219789
Node: Region references preserve shape issue #295220273
Ref: whatsnew/2 2 region-references-preserve-shape-issue-295220514
Ref: 17f220514
Node: Committed types can be linked to datasets and attributes221228
Ref: whatsnew/2 2 committed-types-can-be-linked-to-datasets-and-attributes221432
Ref: 180221432
Node: move method on Group objects221945
Ref: whatsnew/2 2 move-method-on-group-objects222097
Ref: 181222097
Node: What’s new in h5py 2 1222496
Ref: whatsnew/2 1 doc222649
Ref: 182222649
Ref: whatsnew/2 1 what-s-new-in-h5py-2-1222649
Ref: 183222649
Node: Dimension scales222925
Ref: whatsnew/2 1 dimension-scales223046
Ref: 184223046
Node: Unicode strings allowed in attributes223305
Ref: whatsnew/2 1 unicode-strings-allowed-in-attributes223456
Ref: 185223456
Node: Dataset size property223852
Ref: whatsnew/2 1 dataset-size-property224027
Ref: 186224027
Node: Dataset value property is now deprecated224203
Ref: whatsnew/2 1 dataset-value-property-is-now-deprecated224353
Ref: 187224353
Node: Bug fixes<5>224775
Ref: whatsnew/2 1 bug-fixes224895
Ref: 188224895
Node: What’s new in h5py 2 0225586
Ref: whatsnew/2 0 doc225706
Ref: 189225706
Ref: whatsnew/2 0 what-s-new-in-h5py-2-0225706
Ref: 18a225706
Node: Enhancements unlikely to affect compatibility226357
Ref: whatsnew/2 0 enhancements-unlikely-to-affect-compatibility226507
Ref: 18b226507
Node: Changes which may break existing code227443
Ref: whatsnew/2 0 changes-which-may-break-existing-code227614
Ref: 18c227614
Node: Supported HDF5/Python versions228440
Ref: whatsnew/2 0 supported-hdf5-python-versions228603
Ref: 18d228603
Node: Group Dataset and Datatype constructors have changed229075
Ref: whatsnew/2 0 group-dataset-and-datatype-constructors-have-changed229283
Ref: 18e229283
Node: Unicode is now used for object names230009
Ref: whatsnew/2 0 unicode-is-now-used-for-object-names230223
Ref: 18f230223
Node: File objects must be manually closed230967
Ref: whatsnew/2 0 file-objects-must-be-manually-closed231159
Ref: 190231159
Node: Changes to scalar slicing code231956
Ref: whatsnew/2 0 changes-to-scalar-slicing-code232169
Ref: 191232169
Node: Array scalars now always returned when indexing a dataset232476
Ref: whatsnew/2 0 array-scalars-now-always-returned-when-indexing-a-dataset232709
Ref: 192232709
Node: Reading object-like data strips special type information233227
Ref: whatsnew/2 0 reading-object-like-data-strips-special-type-information233468
Ref: 193233468
Node: The selections module has been removed234001
Ref: whatsnew/2 0 the-selections-module-has-been-removed234249
Ref: 194234249
Node: The H5Error exception class has been removed along with h5py h5e234607
Ref: whatsnew/2 0 the-h5error-exception-class-has-been-removed-along-with-h5py-h5e234848
Ref: 195234848
Node: File mode property is now either ‘r’ or ‘r+235419
Ref: whatsnew/2 0 file-mode-property-is-now-either-r-or-r235668
Ref: 196235668
Node: Long-deprecated dict methods have been removed235943
Ref: whatsnew/2 0 long-deprecated-dict-methods-have-been-removed236119
Ref: 197236119
Node: Known issues236453
Ref: whatsnew/2 0 known-issues236570
Ref: 198236570
Node: Bug Reports & Contributions236967
Ref: contributing doc237120
Ref: 199237120
Ref: contributing bug-reports-contributions237120
Ref: 19a237120
Ref: Bug Reports & Contributions-Footnote-1238510
Node: How to File a Bug Report238543
Ref: contributing how-to-file-a-bug-report238668
Ref: 19b238668
Node: If you’re unsure whether you’ve found a bug238920
Ref: contributing if-you-re-unsure-whether-you-ve-found-a-bug239050
Ref: 19c239050
Node: What to include239570
Ref: contributing what-to-include239700
Ref: 19d239700
Ref: What to include-Footnote-1240254
Node: How to Get Your Code into h5py240285
Ref: contributing how-to-get-your-code-into-h5py240437
Ref: 19e240437
Node: Clone the h5py repository242046
Ref: contributing clone-the-h5py-repository242183
Ref: 19f242183
Ref: contributing git-checkout242183
Ref: 1e242183
Node: Create a topic branch for your feature242625
Ref: contributing create-a-topic-branch-for-your-feature242793
Ref: 1a0242793
Node: Implement the feature!243262
Ref: contributing implement-the-feature243418
Ref: 1a1243418
Node: Run the tests243697
Ref: contributing run-the-tests243835
Ref: 1a2243835
Ref: Run the tests-Footnote-1244184
Node: Write a release note244230
Ref: contributing write-a-release-note244377
Ref: 1a3244377
Node: Add yourself to the author list245400
Ref: contributing add-yourself-to-the-author-list245580
Ref: 1a4245580
Ref: Add yourself to the author list-Footnote-1246509
Node: Push your changes back and open a pull request246571
Ref: contributing push-your-changes-back-and-open-a-pull-request246756
Ref: 1a5246756
Ref: Push your changes back and open a pull request-Footnote-1247052
Node: Work with the maintainers247117
Ref: contributing work-with-the-maintainers247262
Ref: 1a6247262
Node: How to Modify h5py248100
Ref: contributing how-to-modify-h5py248219
Ref: 1a7248219
Node: Adding a function from the HDF5 C API249072
Ref: contributing adding-a-function-from-the-hdf5-c-api249231
Ref: 1a8249231
Ref: Adding a function from the HDF5 C API-Footnote-1254147
Node: Adding a function only available in certain versions of HDF5254212
Ref: contributing adding-a-function-only-available-in-certain-versions-of-hdf5254410
Ref: 1a9254410
Node: Testing MPI-only features/code255957
Ref: contributing testing-mpi-only-features-code256109
Ref: 1aa256109
Ref: Testing MPI-only features/code-Footnote-1258355
Node: Release Guide258397
Ref: release_guide doc258525
Ref: 1ab258525
Ref: release_guide id1258525
Ref: 1ac258525
Ref: release_guide release-guide258525
Ref: 1ad258525
Ref: Release Guide-Footnote-1258814
Node: Performing releases258858
Ref: release_guide performing-releases258925
Ref: 1ae258925
Node: FAQ259827
Ref: faq doc259951
Ref: 1af259951
Ref: faq faq259951
Ref: 85259951
Ref: faq id1259951
Ref: 1b0259951
Node: What datatypes are supported?260408
Ref: faq what-datatypes-are-supported260534
Ref: 1b1260534
Node: What compression/processing filters are supported?264074
Ref: faq what-compression-processing-filters-are-supported264241
Ref: 1b2264241
Ref: What compression/processing filters are supported?-Footnote-1266811
Node: What file drivers are available?266839
Ref: faq what-file-drivers-are-available267027
Ref: 1b3267027
Node: What’s the difference between h5py and PyTables?268689
Ref: faq what-s-the-difference-between-h5py-and-pytables268859
Ref: 1b4268859
Ref: What’s the difference between h5py and PyTables?-Footnote-1270328
Node: Does h5py support Parallel HDF5?270418
Ref: faq does-h5py-support-parallel-hdf5270581
Ref: 1b5270581
Node: Variable-length VLEN data270853
Ref: faq variable-length-vlen-data270985
Ref: 1b6270985
Node: Enumerated types<2>271370
Ref: faq enumerated-types271488
Ref: 1b7271488
Node: NumPy object types271711
Ref: faq numpy-object-types271831
Ref: 1b8271831
Node: Appending data to a dataset272169
Ref: faq appending-data-to-a-dataset272277
Ref: 1b9272277
Node: Unicode273578
Ref: faq unicode273678
Ref: 1ba273678
Node: Exceptions274035
Ref: faq exceptions274122
Ref: 1bb274122
Node: Development<4>274407
Ref: faq development274478
Ref: 1bc274478
Node: Building from Git274612
Ref: faq building-from-git274715
Ref: 1bd274715
Node: To build from a Git checkout275094
Ref: faq to-build-from-a-git-checkout275197
Ref: 1be275197
Node: Licenses and legal info275858
Ref: licenses doc275960
Ref: 1bf275960
Ref: licenses licenses-and-legal-info275960
Ref: 1c0275960
Node: Copyright Notice and Statement for the h5py Project276236
Ref: licenses copyright-notice-and-statement-for-the-h5py-project276378
Ref: 1c1276378
Node: HDF5 Copyright Statement278141
Ref: licenses hdf5-copyright-statement278320
Ref: 1c2278320
Node: PyTables Copyright Statement282450
Ref: licenses pytables-copyright-statement282610
Ref: 1c3282610
Node: stdint h Windows version License284457
Ref: licenses stdint-h-windows-version-license284607
Ref: 1c4284607
Node: Python license286185
Ref: licenses python-license286298
Ref: 1c5286298
Node: Index288927

End Tag Table


Local Variables:
coding: utf-8
End:
